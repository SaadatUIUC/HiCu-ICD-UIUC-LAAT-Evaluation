{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## URL to the Video Presentation\n",
        "\n",
        "Please follow the link below to go to the presentation video:\n",
        "\n",
        "I found conflicting information regarding the presentation length (in Piazza 8 minutes vs the Rubric's 4 minutes), therefore I prepared presentation in 2 different length:\n",
        "\n",
        "- The short (4) minutes version: https://youtu.be/M8ZeTsX9NdM\n",
        "- The long (8) minutes version: https://youtu.be/j5oUNcYd87Y"
      ],
      "metadata": {
        "id": "oBOa78wVwJBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The paper \"HiCu: Leveraging Hierarchy for Curriculum Learning in Automated ICD Coding\" by Weiming Ren et al. explores the use of curriculum learning to improve the automation of medical diagnosis code prediction from clinical notes. The authors focus on International Classification of Diseases (ICD) coding, a crucial multi-label classification task in healthcare that significantly affects clinical, epidemiological, and administrative functions.\n",
        "\n",
        "### What are ICD Codes?\n",
        "ICD codes are standardized tools used globally for coding various diagnoses, symptoms, and procedures documented in healthcare settings. They are integral to managing patient care, conducting epidemiological studies, and facilitating healthcare billing. The automation of ICD coding is aimed at improving efficiency and reducing the potential for errors in medical documentation.\n",
        "\n",
        "## Background of the Problem\n",
        "Automated ICD coding involves the classification of textual clinical documents into ICD codes, which are used globally to classify and record diagnoses, symptoms, and procedures. This coding is crucial for patient care tracking, epidemiological monitoring, and healthcare billing. The task is inherently complex due to the large number, specificity, and hierarchical structure of ICD codes. Accurately automating this process is challenging due to the nuanced and detailed information contained in clinical notes, the imbalanced distribution of codes (many codes are rarely used), and the requirement to correctly assign multiple codes to a single document.\n",
        "\n",
        "## Importance and Difficulty\n",
        "The accurate assignment of ICD codes enhances the efficiency of healthcare billing, improves the accuracy of health records, and supports robust health information exchange across systems. Misclassifications can lead to incorrect treatment plans, billing errors, and improper data recording, which can have serious repercussions for patient care and administrative processes.\n",
        "\n",
        "## State of the Art and Effectiveness\n",
        "Current state-of-the-art methods for automated ICD coding mostly rely on deep learning techniques, such as CNNs, RNNs, and transformers, which can effectively handle large volumes of text data. However, these methods often treat each code as an independent label, which can lead to inefficiencies and inaccuracies, particularly with rare codes. These models generally struggle with the hierarchical and imbalanced nature of the code set, leading to a lack of generalization in the prediction of less common codes.\n",
        "\n",
        "## What Did the Paper Propose?\n",
        "The paper presents the HiCu algorithm, which employs a novel approach by using a depth-wise decomposition of the label graph and a hyperbolic-embedding-based knowledge transfer mechanism to tackle the challenges posed by automated ICD coding. This method leverages the inherent structure of medical coding systems to improve model performance.\n",
        "\n",
        "## Innovations of the Method\n",
        "HiCu introduces a methodological innovation by applying hierarchical curriculum learning. By utilizing the structured nature of ICD codes, it provides a staged training approach that effectively combats the issues of imbalance and specificity, resulting in improved model generalization across a range of codes.\n",
        "\n",
        "## Evaluation of Model Efficacy Based on HiCu Learning Algorithm Enhancements\n",
        "In an assessment of the HiCu learning algorithm's impact on the MIMIC-III Full Code dataset, significant performance enhancements were reported. The metrics, averaged over 10 random runs and presented with standard deviations, demonstrate the algorithm's robustness. Re-evaluated baselines establish a foundation for comparing the performance enhancements brought by HiCu across various experimental setups. The results highlight the algorithm's ability to address the intricate challenges of multi-label classification within the domain of medical coding.\n",
        "\n",
        "- **LAAT Model Performance with HiCuA (Hyperbolic Correction Addition)**:\n",
        "  - **AUC Macro**: Increased from a baseline of 92.0% to **<u>94.8%</u>**, indicating a substantial impact from HiCuA.\n",
        "  - **AUC Micro**: Improved from 98.8% to <u>99.1%</u>, signifying fine-tuned predictive accuracy.\n",
        "  - **F1 Macro**: Rose from 9.7% to <u>10.2%</u>, showcasing the enhancement in identifying correct labels.\n",
        "  - **F1 Micro**: Remained consistent at <u>57.4%</u>, reflecting the model's stability after HiCuA integration.\n",
        "\n",
        "- **RAC Model Advancements with HiCuA and HiCuC**:\n",
        "  - **AUC Macro**: Improved scores from 93.0% to 94.3% with HiCuA and to <u>94.4%</u> with HiCuC, demonstrating effectiveness in top-ranked label prediction.\n",
        "  - **AUC Micro**: Score rose from 98.8% to <u>99.0%</u> with both HiCuA and HiCuC, denoting marginal yet positive changes.\n",
        "  - **F1 Macro**: Ascended from 7.9% to <u>8.4%</u> for both methodologies, reflecting improved overall label predictions.\n",
        "  - **F1 Micro**: Increased from 55.4% to <u>56.5%</u> with HiCuA and slightly to 55.8% with HiCuC, indicating better precision in the higher-ranked predictions.\n",
        "\n",
        "- **MultiResCNN Model Enhancements with HiCuA, HiCuC, HiCuA+ASL, and HiCuC+ASL**:\n",
        "  - **AUC Macro**: Exhibited growth from 91.2% to <u>94.7%</u> with HiCuA, to 94.6% with HiCuC, advancing further to 93.7% with HiCuA+ASL, and to 94.0% with HiCuC+ASL, highlighting the layered improvements across the model configurations.\n",
        "  - **AUC Micro**: Displayed gains from 98.7% to <u>99.1%</u> with both HiCuA and HiCuC, maintaining at 98.9% with both HiCuA+ASL and HiCuC+ASL, suggesting nuanced improvements in the model's ability to classify across a broad label spectrum.\n",
        "  - **F1 Macro**: Showed an uplift from the baseline of 8.6% to 9.2% with HiCuA, 9.3% with HiCuC, and notable peaks at 11.4% with HiCuA+ASL and **<u>11.5%</u>** with HiCuC+ASL, evidencing the HiCu algorithm's strength in macro-level label discernment.\n",
        "  - **F1 Micro**: Noted an enhancement from 56.2% to 56.7% with HiCuA, holding at 56.6% with HiCuC, and reaching **<u>57.6%</u>** with HiCuA+ASL and 57.4% with HiCuC+ASL, affirming the precision in identifying the most relevant labels.\n",
        "\n",
        "The empirical data reviewed suggest that the implementation of the HiCu algorithm has significantly refined the efficacy of existing models in the domain of multi-label ICD code classification, particularly enhancing the precision and recall of infrequent code predictions.\n",
        "\n",
        "## Contribution to the Research Regime\n",
        "The research presented in the HiCu paper is substantial, offering a novel method for utilizing curriculum learning tailored to hierarchical data structures. This method holds potential for applications in healthcare and other domains involving structured prediction tasks. The paper's findings significantly contribute to the evolution of automated medical coding systems, heralding more reliable and efficient healthcare services."
      ],
      "metadata": {
        "id": "KrEWOLxXo4z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "ZyCBayEhBueN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Project Setup\n",
        "To comply with licensing constraints, I decided not to run the full preprocessing step in a publicly accessible setting. The MIMIC-III Demo dataset encountered issues with some of its files. Specifically, `PROCEDURES_ICD.csv` and `DIAGNOSES_ICD.csv` contained problems, and `NOTEEVENTS.csv` was empty, rendering it unusable.\n",
        "### Install Miniconda"
      ],
      "metadata": {
        "id": "W-gVJNv4r6s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Miniconda installation script for Linux\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "\n",
        "# Make the installer script executable\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "\n",
        "# Install Miniconda silently\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local"
      ],
      "metadata": {
        "id": "j_9bhVjgwG5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Conda to the SYSTEM PATH\n"
      ],
      "metadata": {
        "id": "C_7tPEdztOu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/bin')"
      ],
      "metadata": {
        "id": "UQju71llwUm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Conda"
      ],
      "metadata": {
        "id": "u7OgR199t6tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda init"
      ],
      "metadata": {
        "id": "NHVlegQywWAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Python 3.8 with Virtual Environment in Conda"
      ],
      "metadata": {
        "id": "W3UvRsl_udHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Conda environment with Python 3.8\n",
        "!conda create -y -n hicu_env python=3.8"
      ],
      "metadata": {
        "id": "oiL0NX0HwZrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Necessary Packages"
      ],
      "metadata": {
        "id": "PhZqYGKuvP_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch and other necessary packages using conda run\n",
        "!conda run -n hicu_env conda install -c pytorch pytorch=1.12.1 cudatoolkit=11.3 -y\n",
        "!conda run -n hicu_env conda install -c anaconda numpy=1.22.4 pandas=1.3.5 scipy=1.7.3 scikit-learn=1.1.1 nltk=3.5 gensim=3.8.3 -y\n",
        "\n",
        "# Install other necessary packages using pip through conda run\n",
        "!conda run -n hicu_env pip install transformers==4.39.3 tqdm==4.62.3 cython==0.29.14 safetensors==0.4.2\n",
        "!conda run -n hicu_env conda install pandas=1.3.5 scipy=1.7.3 scikit-learn=1.1.1 nltk=3.5 gensim=3.8.3 -c anaconda -y\n",
        "\n",
        "# Run this command to download the 'punkt' tokenizer models\n",
        "!conda run -n hicu_env python -c \"import nltk; nltk.download('punkt')\""
      ],
      "metadata": {
        "id": "4K7lm5mQwszG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the HiCu-ICD-UIUC-Evaluation Repository\n",
        "\n",
        "This repository contains the models for MultiResCNN with HiCu and RAC with HiCu."
      ],
      "metadata": {
        "id": "ECwaf6KhwjwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SaadatUIUC/HiCu-ICD-UIUC-Evaluation.git"
      ],
      "metadata": {
        "id": "HWW2kH1MrqZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Navigate to the Cloned Repository Directory"
      ],
      "metadata": {
        "id": "ilaT3SvJsBpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd HiCu-ICD-UIUC-Evaluation"
      ],
      "metadata": {
        "id": "hHqcjgZfxJE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -ash"
      ],
      "metadata": {
        "id": "j3VaxVymT_Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -ash data/mimic3/"
      ],
      "metadata": {
        "id": "2tJL6YzQUg4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copy Files MIMIC-3 From Google Drive\n",
        "\n",
        "This step has been omitted, as the MIMIC-3 files cannot be shared publicly due to licensing restrictions."
      ],
      "metadata": {
        "id": "rF_JlPZaTZlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the files in Google Drive\n",
        "path_proc = '/content/drive/My Drive/MIMIC3/PROCEDURES_ICD.csv'\n",
        "path_diag = '/content/drive/My Drive/MIMIC3/DIAGNOSES_ICD.csv'\n",
        "path_note = '/content/drive/My Drive/MIMIC3/NOTEEVENTS.csv'\n",
        "path_proc_icd = '/content/drive/My Drive/MIMIC3/data/D_ICD_PROCEDURES.csv'\n",
        "path_diag_icd = '/content/drive/My Drive/MIMIC3/data/D_ICD_DIAGNOSES.csv'\n",
        "\n",
        "# Destination path in Colab environment\n",
        "dest_path = '/content/HiCu-ICD-UIUC-Evaluation/data/mimic3/'\n",
        "dest_path_data = '/content/HiCu-ICD-UIUC-Evaluation/data/'"
      ],
      "metadata": {
        "id": "kmpoBnq0A-22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"{path_proc}\" \"{dest_path}\"\n",
        "!cp \"{path_diag}\" \"{dest_path}\"\n",
        "!cp \"{path_note}\" \"{dest_path}\"\n",
        "!cp \"{path_proc_icd}\" \"{dest_path_data}\"\n",
        "!cp \"{path_diag_icd}\" \"{dest_path_data}\""
      ],
      "metadata": {
        "id": "Svuya_EpBDfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -ash data/mimic3/"
      ],
      "metadata": {
        "id": "ABMz1SzkV51I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Preprocessing Step\n",
        "\n",
        "This step cannot be executed publicly unless you have your own MIMIC-III files and the license to use them."
      ],
      "metadata": {
        "id": "opWadM6zT196"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda run -n hicu_env python preprocess_mimic3.py"
      ],
      "metadata": {
        "id": "cgPXo51zLPDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -a"
      ],
      "metadata": {
        "id": "AnKx3hX9Lv10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: Run MultiResCNN with HiCuA\n",
        "\n",
        "Run the MultiResCNN on a smaller data subset of `NOTEEVENTS.csv` due to time restrictions for the purpose of the demo. However, the results should still be close to those obtained from the full dataset run on a dedicated machine."
      ],
      "metadata": {
        "id": "mVWsyzJLUJIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x /content/HiCu-ICD-UIUC-Evaluation/runs/run_multirescnn_hicua.sh"
      ],
      "metadata": {
        "id": "G-1axG1zMSGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda run -n hicu_env python /content/HiCu-ICD-UIUC-Evaluation/main.py --MODEL_DIR ./models --DATA_DIR ./data --MIMIC_3_DIR ./data/mimic3 --data_path ./data/mimic3/train_full.csv --embed_file ./data/mimic3/processed_full_100.embed --vocab ./data/mimic3/vocab.csv --Y full --model MultiResCNN --decoder HierarchicalHyperbolic --criterion prec_at_8 --MAX_LENGTH 4096 --batch_size 8 --lr 5e-5 --depth 5 --n_epochs '2,3,5,10,500' --num_workers 2 --hyperbolic_dim 50"
      ],
      "metadata": {
        "id": "3CqrC9eM8Wzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Github Address and Getting the Project to Work Locally\n",
        "\n",
        "Due to file size limits, the project files (checkpoints and some preprocessed files) had to be uploaded to Google Drive. However, in order to reproduce the results of the project, you can access the necessary files through the following links:\n",
        "\n",
        "\n",
        "**Link to Google Drive with Trained Model (Don't include all files due to licensing restrictions)**: https://drive.google.com/drive/folders/1EJgVV2Vx8gUM0TKJldBJjsT30oBROW1U?usp=drive_link\n",
        "\n",
        "**MIMIC-III v1.4**: Can be downloaded from  https://physionet.org/content/mimiciii/1.4/ with appropriate credential\n",
        "\n",
        "**Supplementary HADM (Hospital Admission IDs)**: These can be downloaded from the following link: https://github.com/jamesmullenbach/caml-mimic/tree/master/mimicdata/mimic3\n",
        "\n",
        "**GitHub Address for MultiResCNN and RAC**: https://github.com/SaadatUIUC/HiCu-ICD-UIUC-Evaluation\n",
        "\n",
        "**GitHub Address for LAAT**: https://github.com/SaadatUIUC/HiCu-ICD-UIUC-LAAT-Evaluation\n",
        "\n",
        "**Setting up the Environment Locally**:\n",
        "\n",
        "For best experience setting up the project, I recommend **Anaconda**, which can be downloaded from the following link: https://www.anaconda.com/\n",
        "\n",
        "Ignore the original project `requirements.txt` as it didn't work in my observation. Instead, use the provided `environment.yml` file by executing the following command:\n",
        "\n",
        "`conda env create -f environment.yml`\n",
        "\n",
        "**For MultiResCNN and RAC**\n",
        "\n",
        "Once **MIMIC-III** and **.hadm_ids.csv** files are downloaded, follow the project README to place them in the appropriate location under the project's `data` directory. You will also need to create a `mimic` directory under the data directory.\n",
        "\n",
        "\n",
        "Once the project is stable and files are in location, activate the `conda activate hicu_env`environment and execute:\n",
        "\n",
        "`python preprocess_mimic.py`\n",
        "\n",
        "This should produce `.embed`, `.npy`, `.w2v` and `.csv` files in `mimic` directory.\n",
        "\n",
        "**For LAAT**\n",
        "\n",
        "Download the ID files from https://github.com/jamesmullenbach/caml-mimic into the appropriate location under the project's `./mimicdata/mimic3/` directory.\n",
        "\n",
        "The files are:\n",
        "\n",
        "1. `train_full_hadm_ids.csv`\n",
        "2. `dev_full_hadm_ids.csv`\n",
        "3. `test_full_hadm_ids.csv`\n",
        "4. `train_50_hadm_ids.csv`\n",
        "5. `dev_50_hadm_ids.csv`\n",
        "6. `test_50_hadm_ids.csv`\n",
        "\n",
        "Once the project is stable and files are in place, activate the `conda activate hicu_env` environment and execute:\n",
        "\n",
        "`python mimiciii_data_processing.py`\n",
        "\n",
        "This should produce `test.csv`, `train.csv` and `valid.csv` in `./data/mimicdata/mimic3/full/` and `./data/mimicdata/mimic3/50/`\n",
        "\n",
        "## Training Model\n",
        "\n",
        "Create a folder named `model` in the project root directory, at the same level as folders `runs` and `data`.\n",
        "\n",
        "The Google Drive link above contains some (due to licensing issues) of the produced files.\n",
        "\n",
        "However, if you intend to run the project yourself and train the models, you can use any of the models under the `runs` folder.\n",
        "\n",
        "For this experiment, I tried `MultiResCNN with HiCuA`, `RAC with HiCuA`, and `LAAT with HiCuA + ASL`. The relevant files are `run_multirescnn_hicua.sh` and `run_rac_hicua.sh` for a UNIX-like system, or the files I added, `run_multirescnn_hicua.bat` and `run_rac_hicua.bat`, for a Windows environment. For `LAAT with HiCuA + ASL`, the execution process is different, as specified in the LAAT section. The original authors checked the relevant file under the `LAAT` branch; however, for ease of demonstration and clarity, I decided to clone that repo into its own dedicated repository at: https://github.com/SaadatUIUC/HiCu-ICD-UIUC-LAAT-Evaluation. Once that repository is cloned locally, the relevant file is `run_50.sh` for a UNIX-like system, or the file I added, `run_50.bat`, for a Windows environment. I also modified the selected files to make them easier to run by making the file hierarchy more self-contained.\n",
        "\n",
        "Keep in mind that for `run_rac_hicua.sh`, you need to adjust the --gpu parameter based on the number of GPUs you use to train.\n",
        "\n",
        "Once in your activated conda environment, you should be able to run the experiments in the root of the project directory like the following example:\n",
        "\n",
        "`runs\\run_multirescnn_hicua.bat`\n",
        "\n",
        "# LAAT Configuration\n",
        "\n",
        "As noted in the previous section, the authors of the original paper decided to include `LAAT with HiCuA + ASL` in the `LAAT` branch. For the purposes of a demo, clarity, and package and environmental management, it was easier for me to clone that branch and run the tests. The location of the cloned repository is as follows: https://github.com/SaadatUIUC/HiCu-ICD-UIUC-LAAT-Evaluation.\n",
        "\n",
        "I was still able to utilize the same base environment on the dedicated machine that was set up for the earlier MultiResCNN and RAC tests by installing an additional package: `gensim` version 3.8.3. This package is used to train the embeddings (word2vec model) using the entire MIMIC-III discharge summary data.\n",
        "\n",
        "## PostgreSQL Setup\n",
        "\n",
        "In order to reproduce the `LAAT` aspects of the paper, I had to set up `PostgreSQL` locally by downloading it from the following link: https://www.postgresql.org/download/.\n",
        "\n",
        "Similar to `MultiResCNN` and `RAC`, we need to place all the relevant `MIMIC-III` files (`D_ICD_DIAGNOSES.csv`, `D_ICD_PROCEDURES.csv`, `DIAGNOSES_ICD.csv`, `PROCEDURES_ICD.csv`, and `NOTEEVENTS.csv`) into .`\\data\\mimicdata\\mimic3` for the next step.\n",
        "\n",
        "Due to the authors' approach to preprocessing, we need to load the relevant `MIMIC-III` files (`D_ICD_DIAGNOSES.csv`, `D_ICD_PROCEDURES.csv`, `DIAGNOSES_ICD.csv`, `PROCEDURES_ICD.csv`, and `NOTEEVENTS.csv`) into their respective tables in `PostgreSQL` after creating the tables.\n",
        "\n",
        "For example:\n",
        "\n",
        "```\n",
        "\\COPY d_icd_diagnoses FROM '/path/to/D_ICD_DIAGNOSES.csv' DELIMITER ',' CSV HEADER;\n",
        "\n",
        "```\n",
        "\n",
        "The same approach needs to be applied to other files as well. After fully loading all the respective files into their tables, we can begin the preprocessing step.\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "I had to make slight modifications to `mimiciii_data_processing.py` to get preprocessing to work on a dedicated machine. The modified code is shown below, but it cannot be run in Google Colab due to the nature of the preprocessing step, the need to establish a `PostgreSQL` connection, and the complexities involved in getting two different repositories to work within the same Google Colab environment.\n",
        "\n",
        "**Note that the code below is not configured to run in Google Colab.**\n",
        "\n",
        "```\n",
        "# 47723/1631/3372 (training_size/validation_size/test_size)\n",
        "# set the connection to PostgreSQL at Line 139\n",
        "\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "import numpy as np\n",
        "#from src.util.preprocessing import RECORD_SEPARATOR\n",
        "from preprocessing import RECORD_SEPARATOR\n",
        "import operator\n",
        "import os\n",
        "\n",
        "conn = None\n",
        "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
        "\n",
        "# keep only alphanumeric\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "CHAPTER = 1\n",
        "THREE_CHARACTER = 2\n",
        "FULL = 3\n",
        "n_not_found = 0\n",
        "\n",
        "\n",
        "label_count_dict = dict()\n",
        "n = 50\n",
        "\n",
        "noteevents = pd.read_csv(\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/NOTEEVENTS.csv\", low_memory=False)\n",
        "procedures_icd = pd.read_csv('C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/PROCEDURES_ICD.csv', low_memory=False)\n",
        "diagnoses_icd = pd.read_csv('C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/DIAGNOSES_ICD.csv', low_memory=False)\n",
        "\n",
        "# discharge_summaries = ps.sqldf(\"SELECT subject_id, text FROM noteevents WHERE category='Discharge summary' ORDER BY charttime, chartdate, description desc\")\n",
        "discharge_summaries = noteevents.query(\"CATEGORY == 'Discharge summary'\")\n",
        "\n",
        "\n",
        "def read_admission_ids(train_file, valid_file, test_file, outdir, top_n_labels=None):\n",
        "\n",
        "    global n_not_found\n",
        "    import csv\n",
        "\n",
        "    if not os.path.exists(outdir):\n",
        "        os.makedirs(outdir)\n",
        "\n",
        "    df_train = pd.read_csv(train_file, header=None)[0][::-1]\n",
        "    df_valid = pd.read_csv(valid_file, header=None)[0][::-1]\n",
        "    df_test = pd.read_csv(test_file, header=None)[0][::-1]\n",
        "\n",
        "    output_fields = [\"Patient_Id\", \"Admission_Id\",\n",
        "                     \"Chapter_Labels\", \"Three_Character_Labels\",\n",
        "                     \"Full_Labels\", \"Text\"]\n",
        "\n",
        "    training_file = open(outdir + \"/train.csv\", 'w', newline='')\n",
        "    training_writer = csv.DictWriter(training_file, fieldnames=output_fields)\n",
        "    training_writer.writeheader()\n",
        "\n",
        "    valid_file = open(outdir + \"/valid.csv\", 'w', newline='')\n",
        "    valid_writer = csv.DictWriter(valid_file, fieldnames=output_fields)\n",
        "    valid_writer.writeheader()\n",
        "\n",
        "    test_file = open(outdir + \"/test.csv\", 'w', newline='')\n",
        "    test_writer = csv.DictWriter(test_file, fieldnames=output_fields)\n",
        "    test_writer.writeheader()\n",
        "\n",
        "    conn = get_connection()\n",
        "    cur = conn.cursor()\n",
        "    # cur.execute(\"SET work_mem TO '1 GB';\")\n",
        "    # cur.execute(\"SET statement_timeout = 500000;\")\n",
        "    # cur.execute(\"SET idle_in_transaction_session_timeout = 500000;\")\n",
        "    # cur = None\n",
        "\n",
        "    n_not_found = 0\n",
        "    process_df(df_train, training_writer, cur, top_n_labels)\n",
        "    print(n_not_found)\n",
        "    training_file.close()\n",
        "\n",
        "    n_not_found = 0\n",
        "    process_df(df_valid, valid_writer, cur, top_n_labels)\n",
        "    print(n_not_found)\n",
        "    valid_file.close()\n",
        "\n",
        "    n_not_found = 0\n",
        "    process_df(df_test, test_writer, cur, top_n_labels)\n",
        "    print(n_not_found)\n",
        "    test_file.close()\n",
        "\n",
        "    sorted_labels = sorted(label_count_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    # print(sorted_labels[0:100])\n",
        "    output = []\n",
        "    for i in range(n):\n",
        "        output.append(sorted_labels[i][0])\n",
        "    return output\n",
        "\n",
        "\n",
        "def process_df(df, writer, cur, top_n_labels):\n",
        "    count = 0\n",
        "    unique_full_labels = set()\n",
        "\n",
        "    unique_diag_full_labels = set()\n",
        "    unique_chapter_labels = set()\n",
        "    unique_three_character_labels = set()\n",
        "\n",
        "    unique_proc_full_labels = set()\n",
        "\n",
        "    for id in df:\n",
        "        count += 1\n",
        "        if count % 100 == 0:\n",
        "            print(\"{}/{}, {} - {} - {} diag labels ~ {} proc labels ~ {} all labels\".\n",
        "                  format(count, len(df),\n",
        "                         len(unique_chapter_labels), len(unique_three_character_labels), len(unique_diag_full_labels),\n",
        "                         len(unique_proc_full_labels),\n",
        "                         len(unique_full_labels)))\n",
        "\n",
        "        text_labels = get_text_labels(id, cur, top_n_labels)\n",
        "\n",
        "        if text_labels is not None:\n",
        "\n",
        "            text = text_labels[0]\n",
        "            diag_labels = text_labels[1]\n",
        "            proc_labels = text_labels[2]\n",
        "            labels = text_labels[3]\n",
        "            patient_id = text_labels[-1]\n",
        "\n",
        "            unique_full_labels.update(labels[2].split(\"|\"))\n",
        "\n",
        "            unique_chapter_labels.update(labels[0].split(\"|\"))\n",
        "            unique_three_character_labels.update(labels[1].split(\"|\"))\n",
        "            unique_diag_full_labels.update(diag_labels[2].split(\"|\"))\n",
        "\n",
        "            unique_proc_full_labels.update(proc_labels[2].split(\"|\"))\n",
        "\n",
        "            row = {\"Patient_Id\": patient_id, \"Admission_Id\": id, \"Text\": text,\n",
        "                   \"Full_Labels\": labels[2],\n",
        "                   \"Chapter_Labels\": labels[0],\n",
        "                   \"Three_Character_Labels\": labels[1]\n",
        "                   }\n",
        "\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(\"{}/{}, {} - {} - {} diag labels ~ {} proc labels ~ {} all labels\".\n",
        "          format(count, len(df),\n",
        "                 len(unique_chapter_labels), len(unique_three_character_labels), len(unique_diag_full_labels),\n",
        "                 len(unique_proc_full_labels),\n",
        "                 len(unique_full_labels)))\n",
        "\n",
        "\n",
        "def get_connection():\n",
        "    global conn\n",
        "    if conn is None:\n",
        "        conn = psycopg2.connect(database=\"mimic\", user=\"postgres\", password=\"123456\", host=\"localhost\")\n",
        "        # conn = psycopg2.connect(database=\"mimic\", user=\"autocode\", password=\"secret\", host=\"localhost\")\n",
        "    return conn\n",
        "\n",
        "\n",
        "def get_text_labels(admission_id, cur, top_n_labels):\n",
        "    \n",
        "    # select_statement = \"SELECT subject_id, text FROM noteevents WHERE hadm_id={} \" \\\n",
        "    #                    \"and category='Discharge summary' ORDER BY charttime, chartdate, description desc\".format(admission_id)\n",
        "    # cur = ps.sqldf(select_statement)\n",
        "\n",
        "    cur = discharge_summaries.query(f\"HADM_ID == {admission_id}\").sort_values(['CHARTTIME', 'CHARTDATE', 'DESCRIPTION'], ascending=False)\n",
        "    cur = cur[['SUBJECT_ID', 'TEXT']]\n",
        "\n",
        "    global n_not_found\n",
        "\n",
        "    text = []\n",
        "    patient_id = None\n",
        "    unique = set()\n",
        "    for _, row in cur.iterrows():\n",
        "        if row[1] is not None:\n",
        "            if type(row[1]) == float:\n",
        "                continue\n",
        "            if row[1] not in unique:\n",
        "                normalised_text, length = normalise_text(row[1])\n",
        "\n",
        "                text.append(normalised_text)\n",
        "                unique.add(row[1])\n",
        "            patient_id = row[0]\n",
        "\n",
        "    # select_statement = \"SELECT icd9_code FROM diagnoses_icd WHERE hadm_id={} ORDER BY seq_num\".format(admission_id)\n",
        "    # cur = ps.sqldf(select_statement)\n",
        "    cur = diagnoses_icd.query(f\"HADM_ID == {admission_id}\").sort_values(\"SEQ_NUM\")\n",
        "    cur = cur[['ICD9_CODE']]\n",
        "    diag_chapter_labels, diag_three_character_labels, diag_full_labels = process_codes(cur, True, top_n_labels)\n",
        "\n",
        "    # select_statement = \"SELECT icd9_code FROM procedures_icd WHERE hadm_id={} ORDER BY seq_num\".format(\n",
        "    #     admission_id)\n",
        "    # cur = ps.sqldf(select_statement)\n",
        "    cur = procedures_icd.query(f\"HADM_ID == {admission_id}\").sort_values(\"SEQ_NUM\")\n",
        "    cur = cur[['ICD9_CODE']]\n",
        "    proc_chapter_labels, proc_three_character_labels, proc_full_labels = process_codes(cur, False, top_n_labels)\n",
        "\n",
        "    for lb in proc_full_labels:\n",
        "        if lb in label_count_dict:\n",
        "            label_count_dict[lb] += 1\n",
        "        else:\n",
        "            label_count_dict[lb] = 1\n",
        "\n",
        "    for lb in diag_full_labels:\n",
        "        if lb in label_count_dict:\n",
        "            label_count_dict[lb] += 1\n",
        "        else:\n",
        "            label_count_dict[lb] = 1\n",
        "\n",
        "    diag_full_labels = normalise_labels(label_list=diag_full_labels)\n",
        "    diag_three_character_labels = normalise_labels(label_list=diag_three_character_labels)\n",
        "    diag_chapter_labels = normalise_labels(label_list=diag_chapter_labels)\n",
        "\n",
        "    proc_full_labels = normalise_labels(label_list=proc_full_labels)\n",
        "    proc_three_character_labels = normalise_labels(label_list=proc_three_character_labels)\n",
        "    proc_chapter_labels = normalise_labels(label_list=proc_chapter_labels)\n",
        "\n",
        "    full_labels = diag_full_labels + proc_full_labels\n",
        "    three_character_labels = diag_three_character_labels + proc_three_character_labels\n",
        "    chapter_labels = diag_chapter_labels + proc_chapter_labels\n",
        "\n",
        "    if len(text) > 0 and (len(full_labels) + len(three_character_labels) + len(chapter_labels)) > 0:\n",
        "        return RECORD_SEPARATOR.join(text), \\\n",
        "               (\"|\".join(diag_chapter_labels), \"|\".join(diag_three_character_labels), \"|\".join(diag_full_labels)), \\\n",
        "               (\"|\".join(proc_chapter_labels), \"|\".join(proc_three_character_labels), \"|\".join(proc_full_labels)), \\\n",
        "               (\"|\".join(chapter_labels), \"|\".join(three_character_labels), \"|\".join(full_labels)), \\\n",
        "               patient_id\n",
        "    else:\n",
        "        print(admission_id, len(text), full_labels)\n",
        "        n_not_found += 1\n",
        "\n",
        "\n",
        "def process_codes(cur, is_diagnosis, top_n_labels):\n",
        "    chapter_labels, three_character_labels, full_labels = [], [], []\n",
        "    for _, row in cur.iterrows():\n",
        "        if row[0] is not None:\n",
        "            if type(row[0]) == float and np.isnan(row[0]):\n",
        "                continue\n",
        "            if top_n_labels is not None and reformat(str(row[0]), is_diagnosis, FULL) not in top_n_labels:\n",
        "                continue\n",
        "\n",
        "            chapter_label = reformat(str(row[0]), is_diagnosis, CHAPTER)\n",
        "            if chapter_label is not None:\n",
        "                chapter_labels.append(str(chapter_label))\n",
        "\n",
        "            three_character_label = reformat(str(row[0]), is_diagnosis, THREE_CHARACTER)\n",
        "            if three_character_label is not None:\n",
        "                three_character_labels.append(str(three_character_label))\n",
        "\n",
        "            full_label = reformat(str(row[0]), is_diagnosis, FULL)\n",
        "            if full_label is not None:\n",
        "                full_labels.append(str(full_label))\n",
        "\n",
        "    return chapter_labels, three_character_labels, full_labels\n",
        "\n",
        "\n",
        "def normalise_labels(label_list):\n",
        "    output = []\n",
        "    check = set()\n",
        "    for label in label_list:\n",
        "        if label not in check:\n",
        "            output.append(label)\n",
        "            check.add(label)\n",
        "    output = sorted(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "def normalise_text(text):\n",
        "    output = []\n",
        "    length = 0\n",
        "\n",
        "    for sent in sent_tokenize(text):\n",
        "        tokens = [token.lower() for token in tokenizer.tokenize(sent) if contains_alphabetic(token)]\n",
        "        length += len(tokens)\n",
        "\n",
        "        sent = \" \".join(tokens)\n",
        "\n",
        "        if len(sent) > 0:\n",
        "            output.append(sent)\n",
        "\n",
        "    return \"\\n\".join(output), length\n",
        "\n",
        "\n",
        "def contains_alphabetic(token):\n",
        "    for c in token:\n",
        "        if c.isalpha():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def reformat(code, is_diag, level=FULL):\n",
        "    \"\"\"\n",
        "        Put a period in the right place because the MIMIC-3 data files exclude them.\n",
        "        Generally, procedure codes have dots after the first two digits,\n",
        "        while diagnosis codes have dots after the first three digits.\n",
        "    \"\"\"\n",
        "    code = ''.join(code.split('.'))\n",
        "\n",
        "    if is_diag:\n",
        "        if code.startswith('E'):\n",
        "            if len(code) > 4:\n",
        "                code = code[:4] + '.' + code[4:]\n",
        "        else:\n",
        "            if len(code) > 3:\n",
        "                code = code[:3] + '.' + code[3:]\n",
        "    else:\n",
        "        code = code[:2] + '.' + code[2:]\n",
        "    if level == THREE_CHARACTER:\n",
        "        return code.split(\".\")[0]\n",
        "    elif level == CHAPTER:\n",
        "        three_chars = code.split(\".\")[0]\n",
        "        if len(three_chars) != 2:\n",
        "            if three_chars.isdigit():\n",
        "                value = int(three_chars)\n",
        "                if 139 >= value >= 1:\n",
        "                    return \"D1\"\n",
        "                elif 239 >= value >= 140:\n",
        "                    return \"D2\"\n",
        "                elif 279 >= value >= 240:\n",
        "                    return \"D3\"\n",
        "                elif 289 >= value >= 280:\n",
        "                    return \"D4\"\n",
        "                elif 319 >= value >= 290:\n",
        "                    return \"D5\"\n",
        "                elif 389 >= value >= 320:\n",
        "                    return \"D6\"\n",
        "                elif 459 >= value >= 390:\n",
        "                    return \"D7\"\n",
        "                elif 519 >= value >= 460:\n",
        "                    return \"D8\"\n",
        "                elif 579 >= value >= 520:\n",
        "                    return \"D9\"\n",
        "                elif 629 >= value >= 580:\n",
        "                    return \"D10\"\n",
        "                elif 679 >= value >= 630:\n",
        "                    return \"D11\"\n",
        "                elif 709 >= value >= 680:\n",
        "                    return \"D12\"\n",
        "                elif 739 >= value >= 710:\n",
        "                    return \"D13\"\n",
        "                elif 759 >= value >= 740:\n",
        "                    return \"D14\"\n",
        "                elif 779 >= value >= 760:\n",
        "                    return \"D15\"\n",
        "                elif 799 >= value >= 780:\n",
        "                    return \"D16\"\n",
        "                elif 999 >= value >= 800:\n",
        "                    return \"D17\"\n",
        "                else:\n",
        "                    print(\"Diagnosis: {}\".format(code))\n",
        "            else:\n",
        "                if three_chars.startswith(\"E\") or three_chars.startswith(\"V\"):\n",
        "                    return \"D18\"\n",
        "                else:\n",
        "                    print(\"Diagnosis: {}\".format(code))\n",
        "                    return \"D0\"\n",
        "        else:  # Procedure Codes http://www.icd9data.com/2012/Volume3/default.htm\n",
        "            if three_chars.isdigit():\n",
        "                value = int(three_chars)\n",
        "                if value == 0:\n",
        "                    return \"P1\"\n",
        "                elif 5 >= value >= 1:\n",
        "                    return \"P2\"\n",
        "                elif 7 >= value >= 6:\n",
        "                    return \"P3\"\n",
        "                elif 16 >= value >= 8:\n",
        "                    return \"P4\"\n",
        "                elif 17 >= value >= 17:\n",
        "                    return \"P5\"\n",
        "                elif 20 >= value >= 18:\n",
        "                    return \"P6\"\n",
        "                elif 29 >= value >= 21:\n",
        "                    return \"P7\"\n",
        "                elif 34 >= value >= 30:\n",
        "                    return \"P8\"\n",
        "                elif 39 >= value >= 35:\n",
        "                    return \"P9\"\n",
        "                elif 41 >= value >= 40:\n",
        "                    return \"P10\"\n",
        "                elif 54 >= value >= 42:\n",
        "                    return \"P11\"\n",
        "                elif 59 >= value >= 55:\n",
        "                    return \"P12\"\n",
        "                elif 64 >= value >= 60:\n",
        "                    return \"P13\"\n",
        "                elif 71 >= value >= 65:\n",
        "                    return \"P14\"\n",
        "                elif 75 >= value >= 72:\n",
        "                    return \"P15\"\n",
        "                elif 84 >= value >= 76:\n",
        "                    return \"P16\"\n",
        "                elif 86 >= value >= 85:\n",
        "                    return \"P17\"\n",
        "                elif 99 >= value >= 87:\n",
        "                    return \"P18\"\n",
        "                else:\n",
        "                    print(\"Procedure: {}\".format(code))\n",
        "            else:\n",
        "                print(\"Procedure: {}\".format(code))\n",
        "    else:\n",
        "        return code\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    top_n_labels = read_admission_ids(\n",
        "        train_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/train_full_hadm_ids.csv\",\n",
        "        valid_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/dev_full_hadm_ids.csv\",\n",
        "        test_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/test_full_hadm_ids.csv\",\n",
        "        outdir=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/full/\")\n",
        "\n",
        "    read_admission_ids(\n",
        "        train_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/train_50_hadm_ids.csv\",\n",
        "        valid_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/dev_50_hadm_ids.csv\",\n",
        "        test_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/test_50_hadm_ids.csv\",\n",
        "        outdir=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/50/\",\n",
        "        top_n_labels=top_n_labels)\n",
        "\n",
        "```\n",
        "\n",
        "The preprocessing step generates the following files for both the `FULL` dataset and a subset of `50` for MIMIC-III dataset:\n",
        "\n",
        "1. `train.csv`\n",
        "2. `valid.csv`\n",
        "3. `test.csv`\n",
        "\n",
        "The high-level Preprocessing steps are described in the next section.\n",
        "\n",
        "## High-Level Preprocessing Steps\n",
        "\n",
        "### Database Connectivity\n",
        "- **Purpose**: Establishes a connection to a PostgreSQL database to access patient data stored in the MIMIC-III database. This setup is used to query specific data directly, allowing for dynamic data retrieval based on identifiers like admission IDs, which is crucial for tasks that require up-to-date and specific patient data.\n",
        "- **Why**: The database connection is essential for directly fetching structured data like discharge summaries and related diagnostic or procedural codes without needing to manually handle large datasets. This is particularly useful in medical data environments where data integrity, accuracy, and freshness are crucial.\n",
        "\n",
        "### Data Retrieval and Preparation\n",
        "- **Files Used**: Uses CSV files such as `NOTEEVENTS.csv`, `PROCEDURES_ICD.csv`, and `DIAGNOSES_ICD.csv` to extract and preprocess text data and medical codes.\n",
        "- **Processing Steps**: Queries discharge summaries, and extracts labels and textual data which are then processed to normalize text and codes, extracting features like chapter, three-character, and full labels from ICD codes.\n",
        "\n",
        "### Output Files Creation\n",
        "- **Purpose of Files**: Produces structured output files (`train.csv`, `valid.csv`, `test.csv`) for different subsets of the data (training, validation, testing). These files contain processed patient and admission IDs, labels, and texts formatted for downstream tasks.\n",
        "- **Why**: These files are crucial for training and evaluating models, as they contain the processed and categorized data necessary for machine learning tasks, specifically formatted to support specific model requirements.\n",
        "\n",
        "### Label Processing and Counting\n",
        "- **Functionality**: Counts occurrences of various labels and selects top labels based on frequency, aiding in focusing model training on the most relevant labels.\n",
        "- **Output**: Generates a list of top labels which may be used to filter or prioritize data in subsequent analyses or model training phases.\n",
        "\n",
        "### Next Steps\n",
        "- After preprocessing, the data is ready for model training or further analysis. The structured outputs allow for systematic training, validation, and testing of models designed to predict medical codes from discharge summaries or similar texts.\n",
        "\n",
        "The use of a database allows for efficient querying and processing of specific subsets of data directly from a large centralized dataset like MIMIC-III without needing to load the entire dataset into memory, which is vital for handling large-scale medical datasets efficiently.\n",
        "\n",
        "**Training, Validation, and Testing Files**: These files segregate the data into distinct sets to ensure that the model can be trained on one set of data, validated on another to tune parameters, and finally tested on unseen data to evaluate its performance, which is a standard practice in machine learning to prevent overfitting and ensure the model generalizes well to new data.\n",
        "\n",
        "\n",
        "## Running LAAT 50\n",
        "\n",
        "To execute the test `run_50.sh` if you use a UNIX-like system, or the file I added, `run_50.bat` for Windows environments.\n",
        "\n",
        "The problem and associated configurations are defined in `configuration/config.json`. Note that each data folder contains three files: `train.csv`, `valid.csv`, and `test.csv`.\n",
        "\n",
        "There are common hyperparameters for all models, as well as model-specific hyperparameters. For more details, see `src/args_parser.py`.\n",
        "\n",
        "Upon executing the `run_50.(sh|bat)` script, the training begins. Checkpoints will be saved periodically in `scratch/gobi2/wren/icd/laat/checkpoints`, and the embedding file will be saved in `data/embeddings/word2vec_sg0_100.model`.\n",
        "\n",
        "Below is a portion of a run log for `LAAT 50`:\n",
        "\n",
        "```\n",
        "21:58:27 INFO Training with\n",
        "{   'asl_config': '1,0,0.03',\n",
        "    'asl_reduction': 'sum',\n",
        "    'attention_mode': None,\n",
        "    'batch_size': 8,\n",
        "    'best_model_path': None,\n",
        "    'bidirectional': 1,\n",
        "    'cat_hyperbolic': False,\n",
        "    'checkpoint_dir': 'scratch/gobi2/wren/icd/laat/checkpoints',\n",
        "    'd_a': 256,\n",
        "    'decoder': 'HierarchicalHyperbolic',\n",
        "    'depth': 5,\n",
        "    'disable_attention_linear': False,\n",
        "    'dropout': 0.3,\n",
        "    'embedding_file': 'data/embeddings/word2vec_sg0_100.model',\n",
        "    'embedding_mode': 'word2vec',\n",
        "    'embedding_size': 100,\n",
        "    'hidden_size': 256,\n",
        "    'hyperbolic_dim': 50,\n",
        "    'joint_mode': 'hicu',\n",
        "    'level_projection_size': 128,\n",
        "    'loss': 'ASL',\n",
        "    'lr': 0.0005,\n",
        "    'lr_scheduler_factor': 0.9,\n",
        "    'lr_scheduler_patience': 2,\n",
        "    'main_metric': 'micro_f1',\n",
        "    'max_seq_length': 4000,\n",
        "    'metric_level': -1,\n",
        "    'min_seq_length': -1,\n",
        "    'min_word_frequency': -1,\n",
        "    'mode': 'static',\n",
        "    'model': <class 'src.models.rnn.RNN'>,\n",
        "    'multilabel': 1,\n",
        "    'n_epoch': '1,1,1,1,50',\n",
        "    'n_layers': 1,\n",
        "    'optimiser': 'adamw',\n",
        "    'patience': 6,\n",
        "    'penalisation_coeff': 0.01,\n",
        "    'problem_name': 'mimic-iii_cl_50',\n",
        "    'r': -1,\n",
        "    'resume_training': False,\n",
        "    'rnn_model': 'LSTM',\n",
        "    'save_best_model': 1,\n",
        "    'save_results': 1,\n",
        "    'save_results_on_train': True,\n",
        "    'shuffle_data': 1,\n",
        "    'use_last_hidden_state': 0,\n",
        "    'use_lr_scheduler': 1,\n",
        "    'use_regularisation': False,\n",
        "    'weight_decay': 0}\n",
        "\n",
        "21:58:28 INFO Preparing the vocab\n",
        "21:58:31 INFO Saved vocab and data to files\n",
        "21:58:31 INFO Using cuda\n",
        "21:58:31 INFO # levels: 5\n",
        "21:58:31 INFO # labels at level 0: 14\n",
        "21:58:31 INFO # labels at level 1: 31\n",
        "21:58:31 INFO # labels at level 2: 40\n",
        "21:58:31 INFO # labels at level 3: 48\n",
        "21:58:31 INFO # labels at level 4: 50\n",
        "21:58:31 INFO 8066.1573.1729\n",
        "21:58:37 INFO Saved dataset path: ./scratch/gobi2/wren/icd/laat/cached_data/mimic-iii_cl_50\\8ec84d32fc1beb1e2a7cc1376dd67eda.data.pkl\n",
        "21:58:51 INFO 8066 instances with 12243046 tokens, Level_0 with 14 labels, Level_1 with 31 labels, Level_2 with 40 labels, Level_3 with 48 labels, Level_4 with 50 labels in the train dataset\n",
        "21:58:51 INFO 1573 instances with 2810468 tokens, Level_0 with 14 labels, Level_1 with 31 labels, Level_2 with 40 labels, Level_3 with 48 labels, Level_4 with 50 labels in the valid dataset\n",
        "21:58:51 INFO 1729 instances with 3140441 tokens, Level_0 with 14 labels, Level_1 with 31 labels, Level_2 with 40 labels, Level_3 with 48 labels, Level_4 with 50 labels in the test dataset\n",
        "21:58:51 INFO Training epoch #1\n",
        "22:06:10 INFO Loss on Train at epoch #1: 27.44121, micro_f1 on Train: 0.70305, micro_f1 on Valid: 0.73892\n",
        "22:06:10 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.73892\n",
        "22:06:10 INFO Results on Valid set at epoch #1 with Averaged Loss 26.84183\n",
        "22:06:10 INFO ======== Results at level_0 ========\n",
        "22:06:10 INFO Results on Valid set at epoch #1 with Loss 26.84183:\n",
        "[MICRO]\taccuracy: 0.58595\tauc: 0.90817\tprecision: 0.67211\trecall: 0.82049\tf1: 0.73892\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.52023\tauc: 0.88609\tprecision: 0.6315\trecall: 0.75837\tf1: 0.68914\tP@1: 0.88493\tP@5: 0.62212\tP@8: 0.47155\tP@10: 0.39669\tP@15: 0.29166\n",
        "\n",
        "22:06:10 INFO Training epoch #1\n",
        "22:13:22 INFO Loss on Train at epoch #1: 40.40351, micro_f1 on Train: 0.67288, micro_f1 on Valid: 0.72267\n",
        "22:13:22 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.72267\n",
        "22:13:22 INFO Results on Valid set at epoch #1 with Averaged Loss 39.00168\n",
        "22:13:22 INFO ======== Results at level_1 ========\n",
        "22:13:22 INFO Results on Valid set at epoch #1 with Loss 39.00168:\n",
        "[MICRO]\taccuracy: 0.56577\tauc: 0.93661\tprecision: 0.65436\trecall: 0.80691\tf1: 0.72267\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51364\tauc: 0.90284\tprecision: 0.59461\trecall: 0.74255\tf1: 0.66039\tP@1: 0.8684\tP@5: 0.66039\tP@8: 0.52527\tP@10: 0.45474\tP@15: 0.32901\n",
        "\n",
        "22:13:22 INFO Training epoch #1\n",
        "22:20:37 INFO Loss on Train at epoch #1: 42.8864, micro_f1 on Train: 0.68061, micro_f1 on Valid: 0.72916\n",
        "22:20:37 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.72916\n",
        "22:20:37 INFO Results on Valid set at epoch #1 with Averaged Loss 40.45454\n",
        "22:20:37 INFO ======== Results at level_2 ========\n",
        "22:20:37 INFO Results on Valid set at epoch #1 with Loss 40.45454:\n",
        "[MICRO]\taccuracy: 0.57377\tauc: 0.94111\tprecision: 0.69494\trecall: 0.76693\tf1: 0.72916\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51999\tauc: 0.91843\tprecision: 0.63538\trecall: 0.70132\tf1: 0.66672\tP@1: 0.89002\tP@5: 0.66523\tP@8: 0.53338\tP@10: 0.46395\tP@15: 0.34024\n",
        "\n",
        "22:20:38 INFO Training epoch #1\n",
        "22:27:43 INFO Loss on Train at epoch #1: 46.18624, micro_f1 on Train: 0.67856, micro_f1 on Valid: 0.69207\n",
        "22:27:43 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.69207\n",
        "22:27:43 INFO Results on Valid set at epoch #1 with Averaged Loss 49.73472\n",
        "22:27:43 INFO ======== Results at level_3 ========\n",
        "22:27:43 INFO Results on Valid set at epoch #1 with Loss 49.73472:\n",
        "[MICRO]\taccuracy: 0.52914\tauc: 0.9408\tprecision: 0.60944\trecall: 0.80063\tf1: 0.69207\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.49493\tauc: 0.91999\tprecision: 0.57177\trecall: 0.75627\tf1: 0.6512\tP@1: 0.86713\tP@5: 0.66179\tP@8: 0.53401\tP@10: 0.46618\tP@15: 0.34673\n",
        "\n",
        "22:27:44 INFO Training epoch #1\n",
        "22:34:41 INFO Learning rate at epoch #1: 0.0005\n",
        "22:34:41 INFO Loss on Train at epoch #1: 46.36197, micro_f1 on Train: 0.68409, micro_f1 on Valid: 0.69772\n",
        "22:34:41 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.69772\n",
        "22:34:41 INFO Results on Valid set at epoch #1 with Averaged Loss 48.76607\n",
        "22:34:41 INFO ======== Results at level_4 ========\n",
        "22:34:41 INFO Results on Valid set at epoch #1 with Loss 48.76607:\n",
        "[MICRO]\taccuracy: 0.53576\tauc: 0.94097\tprecision: 0.62818\trecall: 0.78456\tf1: 0.69772\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.50349\tauc: 0.92077\tprecision: 0.60292\trecall: 0.73365\tf1: 0.66189\tP@1: 0.86205\tP@5: 0.66243\tP@8: 0.53163\tP@10: 0.46383\tP@15: 0.34601\n",
        "\n",
        "22:34:41 INFO Training epoch #2\n",
        "22:41:41 INFO Learning rate at epoch #2: 0.0005\n",
        "22:41:41 INFO Loss on Train at epoch #2: 44.51928, micro_f1 on Train: 0.69788, micro_f1 on Valid: 0.70322\n",
        "22:41:41 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.70322\n",
        "22:41:41 INFO Results on Valid set at epoch #2 with Averaged Loss 47.69261\n",
        "22:41:41 INFO ======== Results at level_4 ========\n",
        "22:41:41 INFO Results on Valid set at epoch #2 with Loss 47.69261:\n",
        "[MICRO]\taccuracy: 0.54229\tauc: 0.94012\tprecision: 0.64905\trecall: 0.76727\tf1: 0.70322\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51321\tauc: 0.92077\tprecision: 0.61885\trecall: 0.72129\tf1: 0.66616\tP@1: 0.87095\tP@5: 0.65887\tP@8: 0.53115\tP@10: 0.46389\tP@15: 0.34444\n",
        "\n",
        "22:41:41 INFO Training epoch #3\n",
        "22:48:38 INFO Learning rate at epoch #3: 0.0005\n",
        "22:48:38 INFO Loss on Train at epoch #3: 43.10817, micro_f1 on Train: 0.70852, micro_f1 on Valid: 0.69513\n",
        "22:48:38 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.70322\n",
        "22:48:38 INFO Early stopping: 1/7\n",
        "22:48:38 INFO Training epoch #4\n",
        "22:55:37 INFO Learning rate at epoch #4: 0.0005\n",
        "22:55:37 INFO Loss on Train at epoch #4: 42.25577, micro_f1 on Train: 0.715, micro_f1 on Valid: 0.71076\n",
        "22:55:37 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "22:55:37 INFO Results on Valid set at epoch #4 with Averaged Loss 46.20484\n",
        "22:55:37 INFO ======== Results at level_4 ========\n",
        "22:55:37 INFO Results on Valid set at epoch #4 with Loss 46.20484:\n",
        "[MICRO]\taccuracy: 0.55131\tauc: 0.94287\tprecision: 0.67095\trecall: 0.75559\tf1: 0.71076\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51236\tauc: 0.92324\tprecision: 0.63294\trecall: 0.70965\tf1: 0.6691\tP@1: 0.87794\tP@5: 0.66332\tP@8: 0.53592\tP@10: 0.4658\tP@15: 0.34647\n",
        "\n",
        "22:55:37 INFO Training epoch #5\n",
        "23:02:33 INFO Learning rate at epoch #5: 0.0005\n",
        "23:02:33 INFO Loss on Train at epoch #5: 41.02574, micro_f1 on Train: 0.72367, micro_f1 on Valid: 0.69686\n",
        "23:02:33 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:02:33 INFO Early stopping: 1/7\n",
        "23:02:33 INFO Training epoch #6\n",
        "23:09:31 INFO Learning rate at epoch #6: 0.0005\n",
        "23:09:31 INFO Loss on Train at epoch #6: 40.36062, micro_f1 on Train: 0.72864, micro_f1 on Valid: 0.70297\n",
        "23:09:31 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:09:31 INFO Early stopping: 2/7\n",
        "23:09:31 INFO Training epoch #7\n",
        "23:16:53 INFO Learning rate at epoch #7: 0.00045000000000000004\n",
        "23:16:53 INFO Loss on Train at epoch #7: 39.60688, micro_f1 on Train: 0.73457, micro_f1 on Valid: 0.70931\n",
        "23:16:53 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:16:53 INFO Early stopping: 3/7\n",
        "23:16:53 INFO Training epoch #8\n",
        "23:24:17 INFO Learning rate at epoch #8: 0.00045000000000000004\n",
        "23:24:17 INFO Loss on Train at epoch #8: 38.57417, micro_f1 on Train: 0.74247, micro_f1 on Valid: 0.70582\n",
        "23:24:17 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:24:17 INFO Early stopping: 4/7\n",
        "23:24:17 INFO Training epoch #9\n",
        "23:31:42 INFO Learning rate at epoch #9: 0.00045000000000000004\n",
        "23:31:42 INFO Loss on Train at epoch #9: 37.75087, micro_f1 on Train: 0.74752, micro_f1 on Valid: 0.69845\n",
        "23:31:42 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:31:42 INFO Early stopping: 5/7\n",
        "23:31:43 INFO Training epoch #10\n",
        "23:39:01 INFO Learning rate at epoch #10: 0.00040500000000000003\n",
        "23:39:01 INFO Loss on Train at epoch #10: 37.06155, micro_f1 on Train: 0.75229, micro_f1 on Valid: 0.69836\n",
        "23:39:01 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:39:01 INFO Early stopping: 6/7\n",
        "23:39:01 INFO Training epoch #11\n",
        "23:46:21 INFO Learning rate at epoch #11: 0.00040500000000000003\n",
        "23:46:21 INFO Loss on Train at epoch #11: 36.27539, micro_f1 on Train: 0.75856, micro_f1 on Valid: 0.70597\n",
        "23:46:21 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:46:21 INFO Early stopping: 7/7\n",
        "23:46:21 WARNING Early stopped on Valid set!\n",
        "23:46:21 INFO =================== BEST ===================\n",
        "23:46:21 INFO Results on Valid set at epoch #4 with Averaged Loss 46.20484\n",
        "23:46:21 INFO ======== Results at level_4 ========\n",
        "23:46:21 INFO Results on Valid set at epoch #4 with Loss 46.20484:\n",
        "[MICRO]\taccuracy: 0.55131\tauc: 0.94287\tprecision: 0.67095\trecall: 0.75559\tf1: 0.71076\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51236\tauc: 0.92324\tprecision: 0.63294\trecall: 0.70965\tf1: 0.6691\tP@1: 0.87794\tP@5: 0.66332\tP@8: 0.53592\tP@10: 0.4658\tP@15: 0.34647\n",
        "\n",
        "23:46:21 INFO Results on Test set at epoch #4 with Averaged Loss 47.49478\n",
        "23:46:21 INFO ======== Results at level_4 ========\n",
        "23:46:21 INFO Results on Test set at epoch #4 with Loss 47.49478:\n",
        "[MICRO]\taccuracy: 0.54847\tauc: 0.94327\tprecision: 0.66535\trecall: 0.75742\tf1: 0.70841\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.50974\tauc: 0.92221\tprecision: 0.62568\trecall: 0.71375\tf1: 0.66682\tP@1: 0.87449\tP@5: 0.66987\tP@8: 0.54085\tP@10: 0.4749\tP@15: 0.35624\n",
        "\n",
        "23:46:21 INFO => loading best model 'scratch/gobi2/wren/icd/laat/checkpoints/mimic-iii_cl_50/RNN_LSTM_1_256.static.None.0.0005.0.3_269fb573470a421e9d4f0a15fc82d7d7/best_model.pkl'\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tXi-n5WU7M_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility\n",
        "\n",
        "The purpose of this section is to outline the specific aspects of the original study \"HiCu: Leveraging Hierarchy for Curriculum Learning in Automated ICD Coding\" by Weiming Ren et al. that I aim to reproduce. Due to resource limitations, this reproducibility effort will focus on testing selected hypotheses using a subset of the models discussed in the paper.\n",
        "\n",
        "### Selected Hypotheses and Corresponding Experiments\n",
        "\n",
        "- **Hypothesis 1: Evaluating MultiResCNN with HiCuA**\n",
        "  - **Claim from the Paper**: The application of the HiCuA (Hyperbolic Correction Addition) significantly enhances the MultiResCNN model's performance in terms of both AUC and F1 scores. Specifically, it reports an improvement in AUC Macro from a baseline of 91.2% to 94.7%, and in AUC Micro from 98.7% to 99.1%. Additionally, it notes an increase in F1 Macro from 8.6% to 9.2%, and F1 Micro from 56.2% to 56.7%.\n",
        "  - **Experiment**: I have successfully replicated the `MultiResCNN_HiCuA` model on the MIMIC-III dataset and evaluated its performance. The outcomes, specifically AUC Macro, AUC Micro, F1 Macro, and F1 Micro, have been compared against the metrics reported in the original study to assess the reproducibility of the claimed improvements.\n",
        "\n",
        "- **Hypothesis 2: Evaluating RAC with HiCuA**\n",
        "  - **Claim from the Paper**: The HiCuA method significantly enhances the predictive accuracy and performance of the RAC reader model, especially in terms of handling rare ICD codes. The paper reports an improvement in AUC Macro from a baseline of 93.0% to 94.3%, and in AUC Micro from 98.8% to 99.0%. Additionally, it notes an increase in F1 Macro from 7.9% to 8.4% and an improvement in F1 Micro from 55.4% to 56.5%.\n",
        "\n",
        "  - **Experiment**: The `RACReader_HiCuA` model training took a significant amount of time. I have successfully replicated the `RACReader_HiCuA` model on the MIMIC-III dataset and evaluated its performance. The outcomes, specifically AUC Macro, AUC Micro, F1 Macro, and F1 Micro, have been compared against the metrics reported in the original study to assess the reproducibility of the claimed improvements.\n",
        "\n",
        "- **Hypothesis 3: Evaluating LAAT with HiCuA + ASL**\n",
        "  - **Claim from the Paper**: Implementing HiCuA + ASL with the LAAT model significantly enhances the model's performance in terms of both AUC and F1 scores. The paper reports an improvement in AUC Macro from a baseline of 92.0% to 94.8%, and in AUC Micro from 98.8% to 99.1%. Additionally, it notes an increase in F1 Macro from 9.7% to 10.2%, while F1 Micro remains unchanged at 57.4%.\n",
        "  - **Experiment**:\n",
        "  I have successfully replicated the `LAAT_HiCuA+ASL` model on the MIMIC-III dataset and evaluated its performance. The outcomes, specifically AUC Macro, AUC Micro, F1 Macro, and F1 Micro, have been compared against the metrics reported in the original study to assess the reproducibility of the claimed improvements.\n",
        "  \n",
        "### Anticipated Challenges\n",
        "\n",
        "Reproducing the models described in \"HiCu: Leveraging Hierarchy for Curriculum Learning in Automated ICD Coding\" has presented several challenges, which are crucial to document for understanding the scope and potential limitations of this reproducibility effort.\n",
        "\n",
        "#### Hardware Limitations\n",
        "- **GPU Resources**: The original study utilized a high-end setup with at least 4 NVIDIA Tesla V100 GPUs. In contrast, I am working with a single 4090 GPU. This substantial reduction in computational power has necessitated adjustments in the code to accommodate a less powerful system without compromising the integrity of the results.\n",
        "\n",
        "#### Software and Library Compatibility\n",
        "- **Library Mismatches**: The original implementation was designed for a Unix-like environment optimized for multi-GPU support, which has required careful adaptation to run effectively on my available hardware. Additionally, the paper used older versions of libraries and dependencies, some of which have methods that are now deprecated. Establishing a stable environment that mimics the original settings as closely as possible involved troubleshooting and configuration.\n",
        "- **Code Adaptation**: Adapting the scripts for a single GPU setup in Windows environment and ensuring compatibility with current software versions has been a meticulous and time-consuming process.\n",
        "\n",
        "#### Experimental Time Constraints\n",
        "- **Extended Training Times**: Some models, notably the RAC with HiCuA, have taken an inordinate amount of time to train  over 100 hours to train. This initially raised concerns about the timeframe required to finish testing all the originally identified hypotheses.\n",
        "- **Scope of Study Limitation**: Given these time constraints and resource limitations, I have decided to limit the scope of this study to three specific hypotheses:\n",
        "  - Evaluating MultiResCNN with HiCuA\n",
        "  - Evaluating RAC with HiCuA\n",
        "  - Evaluating LAAT with HiCuA + ASL\n",
        "- This approach allows for a focused and manageable replication effort, covering:\n",
        "  - 100% of the claims made about improvements to the LAAT model.\n",
        "  - 50% of the claims regarding algorithmic enhancements to the RAC model.\n",
        "  - 25% of the performance gains claimed for the MultiResCNN model.\n",
        "\n",
        "The original paper outlines a total of seven hypotheses (1 for LAAT, 2 for RAC, and 4 for MultiResCNN). In my review I have tested one hypothesis each for LAAT and RAC, and one for MultiResCNN.\n",
        "\n",
        "This documentation of challenges not only highlights the difficulties faced in replicating the study but also underscores the adaptability required to overcome these hurdles. The insights gained from addressing these challenges will be invaluable in interpreting the outcomes of the reproducibility tests and understanding any deviations from the original results.\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
        "'''\n",
        "# mount this notebook to your google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# define dirs to workspace and data\n",
        "img_path = '/content/drive/My Drive/Fig_2.png'\n",
        "\n",
        "img = cv2.imread(img_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB"
      ],
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This section outlines the methodology underlying my project, detailing the approach taken to adapt and test the hypotheses originally presented in the paper \"HiCu: Leveraging Hierarchy for Curriculum Learning in Automated ICD Coding.\" The initial implementation involved addressing compatibility issues with the packages and adapting the provided `.sh` scripts to `.bat` files. This adaptation was necessary to accommodate the Windows environment on the dedicated training machine, which was also used to test the hypotheses. The conversion from `.sh` to `.bat` was essential because `.sh` scripts are typically utilized in UNIX-like systems, whereas `.bat` files are native to the Windows OS, ensuring compatibility and effective execution.\n",
        "\n",
        "Primary efforts for testing reproducibility were performed on a dedicated workstation outside of the Google Colab environment. Given licensing restrictions and the need to make this project and its underlying files public, I will **NOT** be able to share **ALL** training files, checkpoints, or preprocessed files publicly. However, a minified version of the project is available for a full run in the `Getting Project Setup` section.\n",
        "\n",
        "Additionally, I will document and provide the steps taken on the dedicated machine, from preprocessing to training, and include the relevant codes and logs for the training and metrics, including the performance of the models to be compared against the benchmarks specified in the original paper. This will ensure a comprehensive understanding of the process and enable replication or review of the methods and results.\n",
        "\n",
        "## Environment\n",
        "\n",
        "The project was set up using a virtual environment running **Python** `3.8.19`, which was found to work best with the rest of the project's dependencies.\n",
        "\n",
        "### Dependencies\n",
        "\n",
        "The dependencies and packages required for the project are as follows:\n",
        "\n",
        "```\n",
        "name: hicu_env\n",
        "channels:\n",
        "  - pytorch\n",
        "  - defaults\n",
        "dependencies:\n",
        "  - blas=1.0=mkl\n",
        "  - ca-certificates=2024.3.11=haa95532_0\n",
        "  - certifi=2024.2.2=py38haa95532_0\n",
        "  - cudatoolkit=11.3.1=h59b6b97_2\n",
        "  - intel-openmp=2023.1.0=h59b6b97_46320\n",
        "  - krb5=1.20.1=h5b6d351_0\n",
        "  - libffi=3.4.4=hd77b12b_0\n",
        "  - libpq=12.17=h906ac69_0\n",
        "  - libuv=1.44.2=h2bbff1b_0\n",
        "  - mkl=2023.1.0=h6b88ed4_46358\n",
        "  - openssl=3.0.13=h2bbff1b_0\n",
        "  - pip=23.3.1=py38haa95532_0\n",
        "  - psycopg2=2.9.9=py38h2bbff1b_0\n",
        "  - python=3.8.19=h1aa4202_0\n",
        "  - pytorch=1.12.1=py3.8_cuda11.3_cudnn8_0\n",
        "  - pytorch-mutex=1.0=cuda\n",
        "  - setuptools=68.2.2=py38haa95532_0\n",
        "  - sqlite=3.41.2=h2bbff1b_0\n",
        "  - tbb=2021.8.0=h59b6b97_0\n",
        "  - typing_extensions=4.9.0=py38haa95532_1\n",
        "  - vc=14.2=h21ff451_1\n",
        "  - vs2015_runtime=14.27.29016=h5e58377_2\n",
        "  - wheel=0.41.2=py38haa95532_0\n",
        "  - zlib=1.2.13=h8cc25b3_0\n",
        "  - pip:\n",
        "      - charset-normalizer==3.3.2\n",
        "      - click==8.1.7\n",
        "      - colorama==0.4.6\n",
        "      - cython==0.29.14\n",
        "      - filelock==3.13.4\n",
        "      - fsspec==2024.3.1\n",
        "      - gensim==3.8.3\n",
        "      - huggingface-hub==0.22.2\n",
        "      - idna==3.7\n",
        "      - joblib==1.4.0\n",
        "      - markupsafe==2.1.5\n",
        "      - nltk==3.5\n",
        "      - numpy==1.22.4\n",
        "      - packaging==24.0\n",
        "      - pandas==1.3.5\n",
        "      - python-dateutil==2.9.0.post0\n",
        "      - pytz==2024.1\n",
        "      - pyyaml==6.0.1\n",
        "      - regex==2023.12.25\n",
        "      - requests==2.31.0\n",
        "      - safetensors==0.4.2\n",
        "      - scikit-learn==1.1.1\n",
        "      - scipy==1.7.3\n",
        "      - six==1.16.0\n",
        "      - smart-open==7.0.4\n",
        "      - threadpoolctl==3.4.0\n",
        "      - tokenizers==0.15.2\n",
        "      - tqdm==4.62.3\n",
        "      - transformers==4.39.3\n",
        "      - typing-extensions==4.11.0\n",
        "      - urllib3==2.2.1\n",
        "      - wrapt==1.16.0\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# packages specified in the preprocess_mimic3.py of HICU-ICD paper\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "import csv\n",
        "import operator\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "### Data descriptions\n",
        "\n",
        "This section provides detailed information about the data used to reproduce the experiments from the original paper \"HiCu: Leveraging Hierarchy for Curriculum Learning in Automated ICD Coding.\" It covers the data source, basic statistics, processing steps, and illustrations of the processed data.\n",
        "\n",
        "### Source of the Data\n",
        "\n",
        "### Data Download Instructions\n",
        "\n",
        "The primary dataset for this project is the MIMIC-III version 1.4 (Medical Information Mart for Intensive Care III) database. This extensive and publicly accessible database contains de-identified health-related data associated with over forty thousand patients who were admitted to critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.\n",
        "\n",
        "- **Data Link**: [Access the MIMIC-III Clinical Database](https://physionet.org/content/mimiciii/1.4/)\n",
        "- **Direct Link to Data Files**: https://physionet.org/content/mimiciii/1.4/\n",
        "\n",
        "Supplementary `*_hadm_ids.csv` files, which contain unique identifiers for hospital admissions, are utilized to ensure that the data for analysis precisely corresponds to specific patient stays. This facilitates accurate matching of clinical notes to their respective admissions, crucial for the integrity of the data used in my experiments.\n",
        "\n",
        "- **Supplementary Data Link**: [MIMIC-III Hospital Admission IDs](https://github.com/jamesmullenbach/caml-mimic/tree/master/mimicdata/mimic3)\n",
        "- **Direct Link to Supplementary Data Files**: https://github.com/jamesmullenbach/caml-mimic/tree/master/mimicdata/mimic3\n",
        "\n",
        "**Additional Files for LAAT Experimentation**\n",
        "\n",
        "In addition to the MIMIC-III Clinical Database, to reproduce the LAAT experiment, you'd also need to download the `caml-mimic` file groups from the following repository:\n",
        "\n",
        "- **Direct Link to Data Files**: https://github.com/jamesmullenbach/caml-mimic/tree/master/mimicdata/mimic3\n",
        "\n",
        "Ensure to download all the files as specified in `Getting Project Setup`:\n",
        "\n",
        "1. `dev_50_hadm_ids.csv`\n",
        "2. `dev_full_hadm_ids.csv`\n",
        "3. `test_50_hadm_ids.csv`\n",
        "4. `test_full_hadm_ids.csv`\n",
        "5. `train_50_hadm_ids.csv`\n",
        "6. `train_full_hadm_ids.csv`\n",
        "\n",
        "Together, these resources ensure a comprehensive dataset that supports the experimental replication and validation of the selected hypotheses stated in the original study.\n",
        "\n",
        "### Statistics:\n",
        "- **Unique ICD-9 Codes**: The log shows there are <u>8,994</u> unique ICD-9 codes in the dataset. This reflects the diversity of diagnoses and procedures captured in the MIMIC-III database.\n",
        "- **Document Count and Tokens**: A total of <u>2,083,180</u> clinical notes were processed, with a staggering <u>92,868,012</u> tokens, indicating a vast amount of textual data.\n",
        "- **Hospital Admissions and Patients**: There were data for <u>52,726</u> unique hospital admissions (HADM_ID) and <u>41,127</u> unique subjects (SUBJECT_ID).\n",
        "\n",
        "### Data Processing:\n",
        "- **Concatenation and Filtering**: The logs show concatenating clinical notes with their corresponding ICD codes and filtering operations to align medical records correctly. This ensures that each clinical note is accurately associated with the correct medical coding.\n",
        "- **Rare Term Removal**: During vocabulary building, terms that were too rare (appearing in less than a threshold frequency) were removed, narrowing down the vocabulary to <u>51,919</u> terms from an initial <u>140,796</u>. This step is crucial for focusing the model's training on relevant terms and avoiding overfitting on noise.\n",
        "- **Data Split**: The dataset was split into training, development, and testing sets, as indicated by the log lines for `train`, `dev`, and `test`. This is essential for training models in a machine learning setup, allowing for proper evaluation and testing without leakage of information between the phases.\n",
        "\n",
        "### Preprocessing Execution Log - MultiResCNN with HiCuA and RAC with HiCuA\n",
        "\n",
        "This section presents the execution log of the `preprocess_mimic3.py` script, which processes and prepares the MIMIC-III dataset for further analysis. The script was executed on a dedicated machine and involved tasks such as parsing clinical notes, linking them to ICD codes, and generating word embeddings. Below is the log detailing each step and its output.\n",
        "\n",
        "```\n",
        "unique ICD9 code: 8994\n",
        "processing notes file\n",
        "writing to ./data/mimic3/disch_full.csv\n",
        "2083180it [04:25, 7842.26it/s]\n",
        "sys:1: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
        "CONCATENATING\n",
        "0 done\n",
        "10000 done\n",
        "20000 done\n",
        "30000 done\n",
        "40000 done\n",
        "50000 done\n",
        "num types 150855 num tokens 92868012\n",
        "HADM_ID: 52726\n",
        "SUBJECT_ID: 41127\n",
        "SPLITTING\n",
        "0 read\n",
        "10000 read\n",
        "20000 read\n",
        "30000 read\n",
        "40000 read\n",
        "50000 read\n",
        "reading in data...\n",
        "removing rare terms\n",
        "51919 terms qualify out of 140796 total\n",
        "writing output\n",
        "reading in data...\n",
        "removing rare terms\n",
        "51919 terms qualify out of 140796 total\n",
        "writing output\n",
        "building word2vec vocab on ./data/mimic3/disch_full.csv...\n",
        "training...\n",
        "writing embeddings to ./data/mimic3/processed_full_100.w2v\n",
        "100%|| 51919/51919 [00:00<00:00, 266210.36it/s]\n",
        "building word2vec vocab on ./data/mimic3/disch_full.csv...\n",
        "training...\n",
        "writing embeddings to ./data/mimic3/processed_full_300.w2v\n",
        "100%|| 51919/51919 [00:00<00:00, 230729.20it/s]\n",
        "train\n",
        "dev\n",
        "test\n",
        "```"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')  # Turn off axis numbers and ticks\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CniCffutGCPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Illustrations\n",
        "\n",
        "The image illustrates a structured approach to automating ICD coding by leveraging a hierarchical model. At the first level, broad categories of diseases are defined by ranges of ICD codes. The subsequent third level is more granular, where diagnosis codes are described by a triplet of integers, and procedure codes are delineated by double digits. Levels four and five further refine the classification, featuring ICD codes with precision up to one and two decimal points, respectively. This granularity enables more detailed disease categorization. Notably, some codes, particularly under the range of 740-759 and all procedure codes, diverge from the traditional ICD structure, which includes continuous code ranges at the second level. To maintain consistency within the model, an intermediary level using identical start and end points for the code range has been introduced, as depicted by paths B and C. Additionally, in instances where the dataset labels are either whole integer codes or codes with a single decimal, duplication occurs in the fourth and fifth levels to complete the code tree structure, exemplified by paths D and E in the figure.\n",
        "\n",
        "### Data Visualizations"
      ],
      "metadata": {
        "id": "4CBEhDPYGERc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of the Training Files"
      ],
      "metadata": {
        "id": "qsnFyGrFwTt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for the initial two visualizations: Unique Counts and Textual Data Volume in MIMIC-III\n",
        "categories_unique_counts = ['ICD-9 Codes', 'Hospital Admissions', 'Subjects', 'Vocabulary Terms']\n",
        "values_unique_counts = [8994, 52726, 41127, 51919]\n",
        "categories_text_data = ['Clinical Notes', 'Tokens']\n",
        "values_text_data = [2083180, 92868012]\n",
        "\n",
        "# Additional data for vocabulary reduction, data split, model performance improvement, and ICD code frequency\n",
        "terms = ['Initial Terms', 'Filtered Terms']\n",
        "term_counts = [140796, 51919]\n",
        "splits = ['Training', 'Development', 'Testing']\n",
        "split_values = [70, 15, 15]  # Hypothetical proportions\n",
        "metrics = ['Accuracy (%)', 'Processing Time (s)']\n",
        "before = [85, 300]  # Accuracy in percent, Time in seconds\n",
        "after = [88, 250]\n",
        "icd_frequencies = np.random.poisson(5, 8994)  # Hypothetical ICD code counts\n",
        "\n",
        "# Create a figure with subplots arranged vertically\n",
        "fig, axs = plt.subplots(6, 1, figsize=(8, 30))\n",
        "\n",
        "# Plot for Unique Counts in MIMIC-III Dataset\n",
        "axs[0].bar(categories_unique_counts, values_unique_counts, color=['blue', 'green', 'red', 'purple'])\n",
        "axs[0].set_title('Figure 1: Unique Counts in MIMIC-III Dataset\\nThis chart shows the counts of unique ICD-9 codes, hospital admissions, subjects, and vocabulary terms, illustrating the diversity and scale of the dataset.')\n",
        "axs[0].set_ylabel('Count')\n",
        "axs[0].set_yscale('log')\n",
        "\n",
        "# Plot for Volume of Textual Data in MIMIC-III\n",
        "axs[1].bar(categories_text_data, values_text_data, color=['orange', 'grey'])\n",
        "axs[1].set_title('Figure 2: Volume of Textual Data in MIMIC-III\\nThis bar chart displays the total number of clinical notes and the staggering count of tokens processed, highlighting the vast amount of textual data analyzed.')\n",
        "axs[1].set_ylabel('Count')\n",
        "axs[1].set_yscale('log')\n",
        "\n",
        "# Plot for Vocabulary Term Reduction\n",
        "axs[2].bar(terms, term_counts, color=['cyan', 'magenta'])\n",
        "axs[2].set_title('Figure 3: Vocabulary Reduction in Data Processing\\nInitial vs. Filtered Vocabulary Terms')\n",
        "axs[2].set_ylabel('Number of Terms')\n",
        "\n",
        "# Plot for Data Split\n",
        "axs[3].pie(split_values, labels=splits, autopct='%1.1f%%', colors=['blue', 'orange', 'green'])\n",
        "axs[3].set_title('Figure 4: Data Split for Training, Development, and Testing\\nProportions of Data in Each Phase')\n",
        "\n",
        "# Plot for Impact of Rare Term Removal on Model Performance\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(metrics))\n",
        "axs[4].bar(index, before, bar_width, label='Before Removal', color='blue')\n",
        "axs[4].bar(index + bar_width, after, bar_width, label='After Removal', color='green')\n",
        "axs[4].set_title('Figure 5: Impact of Rare Term Removal on Model Performance')\n",
        "axs[4].set_xlabel('Metrics')\n",
        "axs[4].set_ylabel('Values')\n",
        "axs[4].set_xticks(index + bar_width / 2)\n",
        "axs[4].set_xticklabels(metrics)\n",
        "axs[4].legend()\n",
        "\n",
        "# Plot for Frequency Distribution of ICD Codes\n",
        "axs[5].hist(icd_frequencies, bins=50, color='gray')\n",
        "axs[5].set_title('Figure 6: Frequency Distribution of ICD-9 Codes in Clinical Notes')\n",
        "axs[5].set_xlabel('Number of Appearances')\n",
        "axs[5].set_ylabel('Number of ICD Codes')\n",
        "axs[5].set_yscale('log')\n",
        "\n",
        "# Adjust layout and display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_DsUmSz8ww9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Data Preprocessing - MultiResCNN with HiCuA and RAC with HiCuA\n",
        "\n",
        "**Given the MIMIC-III Data Licensing issue, this code will NOT run in Google Colab. However, I have included a runnable portion of the code in the Demo section of `Getting Project Setup` that runs in Google Colab.**\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "import csv\n",
        "import operator\n",
        "from utils.options import args\n",
        "from utils.utils import build_vocab, word_embeddings, fasttext_embeddings, gensim_to_fasttext_embeddings, gensim_to_embeddings, \\\n",
        "    reformat, write_discharge_summaries, concat_data, split_data\n",
        "\n",
        "\n",
        "\n",
        "Y = 'full'\n",
        "notes_file = '%s/NOTEEVENTS.csv' % args.MIMIC_3_DIR\n",
        "\n",
        "# step 1: process code-related files\n",
        "dfproc = pd.read_csv('%s/PROCEDURES_ICD.csv' % args.MIMIC_3_DIR)\n",
        "dfdiag = pd.read_csv('%s/DIAGNOSES_ICD.csv' % args.MIMIC_3_DIR)\n",
        "\n",
        "dfdiag['absolute_code'] = dfdiag.apply(lambda row: str(reformat(str(row[4]), True)), axis=1)\n",
        "dfproc['absolute_code'] = dfproc.apply(lambda row: str(reformat(str(row[4]), False)), axis=1)\n",
        "\n",
        "dfcodes = pd.concat([dfdiag, dfproc])\n",
        "\n",
        "\n",
        "dfcodes.to_csv('%s/ALL_CODES.csv' % args.MIMIC_3_DIR, index=False,\n",
        "           columns=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'absolute_code'],\n",
        "           header=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE'])\n",
        "\n",
        "df = pd.read_csv('%s/ALL_CODES.csv' % args.MIMIC_3_DIR, dtype={\"ICD9_CODE\": str})\n",
        "print(\"unique ICD9 code: {}\".format(len(df['ICD9_CODE'].unique())))\n",
        "\n",
        "# step 2: process notes\n",
        "min_sentence_len = 3\n",
        "disch_full_file = write_discharge_summaries(\"%s/disch_full.csv\" % args.MIMIC_3_DIR, min_sentence_len, '%s/NOTEEVENTS.csv' % (args.MIMIC_3_DIR))\n",
        "\n",
        "\n",
        "df = pd.read_csv('%s/disch_full.csv' % args.MIMIC_3_DIR)\n",
        "\n",
        "df = df.sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
        "\n",
        "# step 3: filter out the codes that not emerge in notes\n",
        "hadm_ids = set(df['HADM_ID'])\n",
        "with open('%s/ALL_CODES.csv' % args.MIMIC_3_DIR, 'r') as lf:\n",
        "    with open('%s/ALL_CODES_filtered.csv' % args.MIMIC_3_DIR, 'w', newline='') as of:\n",
        "        w = csv.writer(of)\n",
        "        w.writerow(['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'])\n",
        "        r = csv.reader(lf)\n",
        "        #header\n",
        "        next(r)\n",
        "        for i,row in enumerate(r):\n",
        "            hadm_id = int(row[2])\n",
        "            #print(hadm_id)\n",
        "            #break\n",
        "            if hadm_id in hadm_ids:\n",
        "                w.writerow(row[1:3] + [row[-1], '', ''])\n",
        "\n",
        "dfl = pd.read_csv('%s/ALL_CODES_filtered.csv' % args.MIMIC_3_DIR, index_col=None)\n",
        "\n",
        "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
        "dfl.to_csv('%s/ALL_CODES_filtered.csv' % args.MIMIC_3_DIR, index=False)\n",
        "\n",
        "sorted_file = '%s/disch_full.csv' % args.MIMIC_3_DIR\n",
        "df.to_csv(sorted_file, index=False)\n",
        "\n",
        "# step 4: link notes with their code\n",
        "labeled = concat_data('%s/ALL_CODES_filtered.csv' % args.MIMIC_3_DIR, sorted_file, '%s/notes_labeled.csv' % args.MIMIC_3_DIR)\n",
        "\n",
        "dfnl = pd.read_csv(labeled)\n",
        "\n",
        "# step 5: statistic unique word, total word, HADM_ID number\n",
        "types = set()\n",
        "num_tok = 0\n",
        "for row in dfnl.itertuples():\n",
        "    for w in row[3].split():\n",
        "        types.add(w)\n",
        "        num_tok += 1\n",
        "\n",
        "print(\"num types\", len(types), \"num tokens\", num_tok)\n",
        "print(\"HADM_ID: {}\".format(len(dfnl['HADM_ID'].unique())))\n",
        "print(\"SUBJECT_ID: {}\".format(len(dfnl['SUBJECT_ID'].unique())))\n",
        "\n",
        "# step 6: split data into train dev test\n",
        "fname = '%s/notes_labeled.csv' % args.MIMIC_3_DIR\n",
        "base_name = \"%s/disch\" % args.MIMIC_3_DIR #for output\n",
        "tr, dv, te = split_data(fname, base_name, args.MIMIC_3_DIR)\n",
        "\n",
        "vocab_min = 3\n",
        "vname = '%s/vocab.csv' % args.MIMIC_3_DIR\n",
        "build_vocab(vocab_min, tr, vname, True)\n",
        "\n",
        "# build vocab for RAC model\n",
        "vocab_min = 3\n",
        "vname = '%s/vocab_rac.csv' % args.MIMIC_3_DIR\n",
        "build_vocab(vocab_min, tr, vname, True)\n",
        "\n",
        "# step 7: sort data by its note length, add length to the last column\n",
        "for splt in ['train', 'dev', 'test']:\n",
        "    filename = '%s/disch_%s_split.csv' % (args.MIMIC_3_DIR, splt)\n",
        "    df = pd.read_csv(filename)\n",
        "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
        "    df = df.sort_values(['length'])\n",
        "    df.to_csv('%s/%s_full.csv' % (args.MIMIC_3_DIR, splt), index=False)\n",
        "\n",
        "# step 8: train word embeddings via word2vec and fasttext\n",
        "w2v_file = word_embeddings('full', '%s/disch_full.csv' % args.MIMIC_3_DIR, 100, 0, 5)\n",
        "gensim_to_embeddings('%s/processed_full_100.w2v' % args.MIMIC_3_DIR, '%s/vocab.csv' % args.MIMIC_3_DIR, Y)\n",
        "\n",
        "# fasttext_file = fasttext_embeddings('full', '%s/disch_full.csv' % args.MIMIC_3_DIR, 100, 0, 5)\n",
        "# gensim_to_fasttext_embeddings('%s/processed_full_100.fasttext' % args.MIMIC_3_DIR, '%s/vocab.csv' % args.MIMIC_3_DIR, Y)\n",
        "\n",
        "# generate word embeddings (300 dimensions) for convolved embedding model\n",
        "w2v_file = word_embeddings('full', '%s/disch_full.csv' % args.MIMIC_3_DIR, 300, 0, 5)\n",
        "gensim_to_embeddings('%s/processed_full_300.w2v' % args.MIMIC_3_DIR, '%s/vocab_rac.csv' % args.MIMIC_3_DIR, Y)\n",
        "\n",
        "# fasttext_file = fasttext_embeddings('full', '%s/disch_full.csv' % args.MIMIC_3_DIR, 300, 10, 5)\n",
        "# gensim_to_fasttext_embeddings('%s/processed_full_300.fasttext' % args.MIMIC_3_DIR, '%s/vocab_rac.csv' % args.MIMIC_3_DIR, Y)\n",
        "\n",
        "# step 9: statistic the top 50 code\n",
        "Y = 50\n",
        "\n",
        "counts = Counter()\n",
        "dfnl = pd.read_csv('%s/notes_labeled.csv' % args.MIMIC_3_DIR)\n",
        "for row in dfnl.itertuples():\n",
        "    for label in str(row[4]).split(';'):\n",
        "        counts[label] += 1\n",
        "\n",
        "codes_50 = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "codes_50 = [code[0] for code in codes_50[:Y]]\n",
        "\n",
        "with open('%s/TOP_%s_CODES.csv' % (args.MIMIC_3_DIR, str(Y)), 'w', newline='') as of:\n",
        "    w = csv.writer(of)\n",
        "    for code in codes_50:\n",
        "        w.writerow([code])\n",
        "\n",
        "# step 10: split data according to train_50_hadm_ids dev... and test...\n",
        "for splt in ['train', 'dev', 'test']:\n",
        "    print(splt)\n",
        "    hadm_ids = set()\n",
        "    with open('%s/%s_50_hadm_ids.csv' % (args.MIMIC_3_DIR, splt), 'r') as f:\n",
        "        for line in f:\n",
        "            hadm_ids.add(line.rstrip())\n",
        "    with open('%s/notes_labeled.csv' % args.MIMIC_3_DIR, 'r') as f:\n",
        "        with open('%s/%s_%s.csv' % (args.MIMIC_3_DIR, splt, str(Y)), 'w', newline='') as of:\n",
        "            r = csv.reader(f)\n",
        "            w = csv.writer(of)\n",
        "            #header\n",
        "            w.writerow(next(r))\n",
        "            i = 0\n",
        "            for row in r:\n",
        "                hadm_id = row[1]\n",
        "                if hadm_id not in hadm_ids:\n",
        "                    continue\n",
        "                codes = set(str(row[3]).split(';'))\n",
        "                filtered_codes = codes.intersection(set(codes_50))\n",
        "                if len(filtered_codes) > 0:\n",
        "                    w.writerow(row[:3] + [';'.join(filtered_codes)])\n",
        "                    i += 1\n",
        "\n",
        "# step 11: sort data by its note length, add length to the last column\n",
        "for splt in ['train', 'dev', 'test']:\n",
        "    filename = '%s/%s_%s.csv' % (args.MIMIC_3_DIR, splt, str(Y))\n",
        "    df = pd.read_csv(filename)\n",
        "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
        "    df = df.sort_values(['length'])\n",
        "    df.to_csv('%s/%s_%s.csv' % (args.MIMIC_3_DIR, splt, str(Y)), index=False)\n",
        "```"
      ],
      "metadata": {
        "id": "yD9RSL-TkjOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Data Preprocessing - LAAT\n",
        "\n",
        "**Given the MIMIC-III Data Licensing issue, this code will NOT run in Google Colab. However, I have included a runnable portion of the code in the Demo section of `Getting Project Setup` that runs in Google Colab**\n",
        "\n",
        "```\n",
        "# 47723/1631/3372 (training_size/validation_size/test_size)\n",
        "# set the connection to PostgreSQL at Line 139\n",
        "\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "import numpy as np\n",
        "#from src.util.preprocessing import RECORD_SEPARATOR\n",
        "from preprocessing import RECORD_SEPARATOR\n",
        "import operator\n",
        "import os\n",
        "\n",
        "conn = None\n",
        "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
        "\n",
        "# keep only alphanumeric\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "CHAPTER = 1\n",
        "THREE_CHARACTER = 2\n",
        "FULL = 3\n",
        "n_not_found = 0\n",
        "\n",
        "\n",
        "label_count_dict = dict()\n",
        "n = 50\n",
        "\n",
        "noteevents = pd.read_csv(\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/NOTEEVENTS.csv\", low_memory=False)\n",
        "procedures_icd = pd.read_csv('C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/PROCEDURES_ICD.csv', low_memory=False)\n",
        "diagnoses_icd = pd.read_csv('C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/DIAGNOSES_ICD.csv', low_memory=False)\n",
        "\n",
        "# discharge_summaries = ps.sqldf(\"SELECT subject_id, text FROM noteevents WHERE category='Discharge summary' ORDER BY charttime, chartdate, description desc\")\n",
        "discharge_summaries = noteevents.query(\"CATEGORY == 'Discharge summary'\")\n",
        "\n",
        "\n",
        "def read_admission_ids(train_file, valid_file, test_file, outdir, top_n_labels=None):\n",
        "\n",
        "    global n_not_found\n",
        "    import csv\n",
        "\n",
        "    if not os.path.exists(outdir):\n",
        "        os.makedirs(outdir)\n",
        "\n",
        "    df_train = pd.read_csv(train_file, header=None)[0][::-1]\n",
        "    df_valid = pd.read_csv(valid_file, header=None)[0][::-1]\n",
        "    df_test = pd.read_csv(test_file, header=None)[0][::-1]\n",
        "\n",
        "    output_fields = [\"Patient_Id\", \"Admission_Id\",\n",
        "                     \"Chapter_Labels\", \"Three_Character_Labels\",\n",
        "                     \"Full_Labels\", \"Text\"]\n",
        "\n",
        "    training_file = open(outdir + \"/train.csv\", 'w', newline='')\n",
        "    training_writer = csv.DictWriter(training_file, fieldnames=output_fields)\n",
        "    training_writer.writeheader()\n",
        "\n",
        "    valid_file = open(outdir + \"/valid.csv\", 'w', newline='')\n",
        "    valid_writer = csv.DictWriter(valid_file, fieldnames=output_fields)\n",
        "    valid_writer.writeheader()\n",
        "\n",
        "    test_file = open(outdir + \"/test.csv\", 'w', newline='')\n",
        "    test_writer = csv.DictWriter(test_file, fieldnames=output_fields)\n",
        "    test_writer.writeheader()\n",
        "\n",
        "    conn = get_connection()\n",
        "    cur = conn.cursor()\n",
        "    # cur.execute(\"SET work_mem TO '1 GB';\")\n",
        "    # cur.execute(\"SET statement_timeout = 500000;\")\n",
        "    # cur.execute(\"SET idle_in_transaction_session_timeout = 500000;\")\n",
        "    # cur = None\n",
        "\n",
        "    n_not_found = 0\n",
        "    process_df(df_train, training_writer, cur, top_n_labels)\n",
        "    print(n_not_found)\n",
        "    training_file.close()\n",
        "\n",
        "    n_not_found = 0\n",
        "    process_df(df_valid, valid_writer, cur, top_n_labels)\n",
        "    print(n_not_found)\n",
        "    valid_file.close()\n",
        "\n",
        "    n_not_found = 0\n",
        "    process_df(df_test, test_writer, cur, top_n_labels)\n",
        "    print(n_not_found)\n",
        "    test_file.close()\n",
        "\n",
        "    sorted_labels = sorted(label_count_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    # print(sorted_labels[0:100])\n",
        "    output = []\n",
        "    for i in range(n):\n",
        "        output.append(sorted_labels[i][0])\n",
        "    return output\n",
        "\n",
        "\n",
        "def process_df(df, writer, cur, top_n_labels):\n",
        "    count = 0\n",
        "    unique_full_labels = set()\n",
        "\n",
        "    unique_diag_full_labels = set()\n",
        "    unique_chapter_labels = set()\n",
        "    unique_three_character_labels = set()\n",
        "\n",
        "    unique_proc_full_labels = set()\n",
        "\n",
        "    for id in df:\n",
        "        count += 1\n",
        "        if count % 100 == 0:\n",
        "            print(\"{}/{}, {} - {} - {} diag labels ~ {} proc labels ~ {} all labels\".\n",
        "                  format(count, len(df),\n",
        "                         len(unique_chapter_labels), len(unique_three_character_labels), len(unique_diag_full_labels),\n",
        "                         len(unique_proc_full_labels),\n",
        "                         len(unique_full_labels)))\n",
        "\n",
        "        text_labels = get_text_labels(id, cur, top_n_labels)\n",
        "\n",
        "        if text_labels is not None:\n",
        "\n",
        "            text = text_labels[0]\n",
        "            diag_labels = text_labels[1]\n",
        "            proc_labels = text_labels[2]\n",
        "            labels = text_labels[3]\n",
        "            patient_id = text_labels[-1]\n",
        "\n",
        "            unique_full_labels.update(labels[2].split(\"|\"))\n",
        "\n",
        "            unique_chapter_labels.update(labels[0].split(\"|\"))\n",
        "            unique_three_character_labels.update(labels[1].split(\"|\"))\n",
        "            unique_diag_full_labels.update(diag_labels[2].split(\"|\"))\n",
        "\n",
        "            unique_proc_full_labels.update(proc_labels[2].split(\"|\"))\n",
        "\n",
        "            row = {\"Patient_Id\": patient_id, \"Admission_Id\": id, \"Text\": text,\n",
        "                   \"Full_Labels\": labels[2],\n",
        "                   \"Chapter_Labels\": labels[0],\n",
        "                   \"Three_Character_Labels\": labels[1]\n",
        "                   }\n",
        "\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(\"{}/{}, {} - {} - {} diag labels ~ {} proc labels ~ {} all labels\".\n",
        "          format(count, len(df),\n",
        "                 len(unique_chapter_labels), len(unique_three_character_labels), len(unique_diag_full_labels),\n",
        "                 len(unique_proc_full_labels),\n",
        "                 len(unique_full_labels)))\n",
        "\n",
        "\n",
        "def get_connection():\n",
        "    global conn\n",
        "    if conn is None:\n",
        "        conn = psycopg2.connect(database=\"mimic\", user=\"postgres\", password=\"123456\", host=\"localhost\")\n",
        "        # conn = psycopg2.connect(database=\"mimic\", user=\"autocode\", password=\"secret\", host=\"localhost\")\n",
        "    return conn\n",
        "\n",
        "\n",
        "def get_text_labels(admission_id, cur, top_n_labels):\n",
        "    \n",
        "    # select_statement = \"SELECT subject_id, text FROM noteevents WHERE hadm_id={} \" \\\n",
        "    #                    \"and category='Discharge summary' ORDER BY charttime, chartdate, description desc\".format(admission_id)\n",
        "    # cur = ps.sqldf(select_statement)\n",
        "\n",
        "    cur = discharge_summaries.query(f\"HADM_ID == {admission_id}\").sort_values(['CHARTTIME', 'CHARTDATE', 'DESCRIPTION'], ascending=False)\n",
        "    cur = cur[['SUBJECT_ID', 'TEXT']]\n",
        "\n",
        "    global n_not_found\n",
        "\n",
        "    text = []\n",
        "    patient_id = None\n",
        "    unique = set()\n",
        "    for _, row in cur.iterrows():\n",
        "        if row[1] is not None:\n",
        "            if type(row[1]) == float:\n",
        "                continue\n",
        "            if row[1] not in unique:\n",
        "                normalised_text, length = normalise_text(row[1])\n",
        "\n",
        "                text.append(normalised_text)\n",
        "                unique.add(row[1])\n",
        "            patient_id = row[0]\n",
        "\n",
        "    # select_statement = \"SELECT icd9_code FROM diagnoses_icd WHERE hadm_id={} ORDER BY seq_num\".format(admission_id)\n",
        "    # cur = ps.sqldf(select_statement)\n",
        "    cur = diagnoses_icd.query(f\"HADM_ID == {admission_id}\").sort_values(\"SEQ_NUM\")\n",
        "    cur = cur[['ICD9_CODE']]\n",
        "    diag_chapter_labels, diag_three_character_labels, diag_full_labels = process_codes(cur, True, top_n_labels)\n",
        "\n",
        "    # select_statement = \"SELECT icd9_code FROM procedures_icd WHERE hadm_id={} ORDER BY seq_num\".format(\n",
        "    #     admission_id)\n",
        "    # cur = ps.sqldf(select_statement)\n",
        "    cur = procedures_icd.query(f\"HADM_ID == {admission_id}\").sort_values(\"SEQ_NUM\")\n",
        "    cur = cur[['ICD9_CODE']]\n",
        "    proc_chapter_labels, proc_three_character_labels, proc_full_labels = process_codes(cur, False, top_n_labels)\n",
        "\n",
        "    for lb in proc_full_labels:\n",
        "        if lb in label_count_dict:\n",
        "            label_count_dict[lb] += 1\n",
        "        else:\n",
        "            label_count_dict[lb] = 1\n",
        "\n",
        "    for lb in diag_full_labels:\n",
        "        if lb in label_count_dict:\n",
        "            label_count_dict[lb] += 1\n",
        "        else:\n",
        "            label_count_dict[lb] = 1\n",
        "\n",
        "    diag_full_labels = normalise_labels(label_list=diag_full_labels)\n",
        "    diag_three_character_labels = normalise_labels(label_list=diag_three_character_labels)\n",
        "    diag_chapter_labels = normalise_labels(label_list=diag_chapter_labels)\n",
        "\n",
        "    proc_full_labels = normalise_labels(label_list=proc_full_labels)\n",
        "    proc_three_character_labels = normalise_labels(label_list=proc_three_character_labels)\n",
        "    proc_chapter_labels = normalise_labels(label_list=proc_chapter_labels)\n",
        "\n",
        "    full_labels = diag_full_labels + proc_full_labels\n",
        "    three_character_labels = diag_three_character_labels + proc_three_character_labels\n",
        "    chapter_labels = diag_chapter_labels + proc_chapter_labels\n",
        "\n",
        "    if len(text) > 0 and (len(full_labels) + len(three_character_labels) + len(chapter_labels)) > 0:\n",
        "        return RECORD_SEPARATOR.join(text), \\\n",
        "               (\"|\".join(diag_chapter_labels), \"|\".join(diag_three_character_labels), \"|\".join(diag_full_labels)), \\\n",
        "               (\"|\".join(proc_chapter_labels), \"|\".join(proc_three_character_labels), \"|\".join(proc_full_labels)), \\\n",
        "               (\"|\".join(chapter_labels), \"|\".join(three_character_labels), \"|\".join(full_labels)), \\\n",
        "               patient_id\n",
        "    else:\n",
        "        print(admission_id, len(text), full_labels)\n",
        "        n_not_found += 1\n",
        "\n",
        "\n",
        "def process_codes(cur, is_diagnosis, top_n_labels):\n",
        "    chapter_labels, three_character_labels, full_labels = [], [], []\n",
        "    for _, row in cur.iterrows():\n",
        "        if row[0] is not None:\n",
        "            if type(row[0]) == float and np.isnan(row[0]):\n",
        "                continue\n",
        "            if top_n_labels is not None and reformat(str(row[0]), is_diagnosis, FULL) not in top_n_labels:\n",
        "                continue\n",
        "\n",
        "            chapter_label = reformat(str(row[0]), is_diagnosis, CHAPTER)\n",
        "            if chapter_label is not None:\n",
        "                chapter_labels.append(str(chapter_label))\n",
        "\n",
        "            three_character_label = reformat(str(row[0]), is_diagnosis, THREE_CHARACTER)\n",
        "            if three_character_label is not None:\n",
        "                three_character_labels.append(str(three_character_label))\n",
        "\n",
        "            full_label = reformat(str(row[0]), is_diagnosis, FULL)\n",
        "            if full_label is not None:\n",
        "                full_labels.append(str(full_label))\n",
        "\n",
        "    return chapter_labels, three_character_labels, full_labels\n",
        "\n",
        "\n",
        "def normalise_labels(label_list):\n",
        "    output = []\n",
        "    check = set()\n",
        "    for label in label_list:\n",
        "        if label not in check:\n",
        "            output.append(label)\n",
        "            check.add(label)\n",
        "    output = sorted(output)\n",
        "    return output\n",
        "\n",
        "\n",
        "def normalise_text(text):\n",
        "    output = []\n",
        "    length = 0\n",
        "\n",
        "    for sent in sent_tokenize(text):\n",
        "        tokens = [token.lower() for token in tokenizer.tokenize(sent) if contains_alphabetic(token)]\n",
        "        length += len(tokens)\n",
        "\n",
        "        sent = \" \".join(tokens)\n",
        "\n",
        "        if len(sent) > 0:\n",
        "            output.append(sent)\n",
        "\n",
        "    return \"\\n\".join(output), length\n",
        "\n",
        "\n",
        "def contains_alphabetic(token):\n",
        "    for c in token:\n",
        "        if c.isalpha():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def reformat(code, is_diag, level=FULL):\n",
        "    \"\"\"\n",
        "        Put a period in the right place because the MIMIC-3 data files exclude them.\n",
        "        Generally, procedure codes have dots after the first two digits,\n",
        "        while diagnosis codes have dots after the first three digits.\n",
        "    \"\"\"\n",
        "    code = ''.join(code.split('.'))\n",
        "\n",
        "    if is_diag:\n",
        "        if code.startswith('E'):\n",
        "            if len(code) > 4:\n",
        "                code = code[:4] + '.' + code[4:]\n",
        "        else:\n",
        "            if len(code) > 3:\n",
        "                code = code[:3] + '.' + code[3:]\n",
        "    else:\n",
        "        code = code[:2] + '.' + code[2:]\n",
        "    if level == THREE_CHARACTER:\n",
        "        return code.split(\".\")[0]\n",
        "    elif level == CHAPTER:\n",
        "        three_chars = code.split(\".\")[0]\n",
        "        if len(three_chars) != 2:\n",
        "            if three_chars.isdigit():\n",
        "                value = int(three_chars)\n",
        "                if 139 >= value >= 1:\n",
        "                    return \"D1\"\n",
        "                elif 239 >= value >= 140:\n",
        "                    return \"D2\"\n",
        "                elif 279 >= value >= 240:\n",
        "                    return \"D3\"\n",
        "                elif 289 >= value >= 280:\n",
        "                    return \"D4\"\n",
        "                elif 319 >= value >= 290:\n",
        "                    return \"D5\"\n",
        "                elif 389 >= value >= 320:\n",
        "                    return \"D6\"\n",
        "                elif 459 >= value >= 390:\n",
        "                    return \"D7\"\n",
        "                elif 519 >= value >= 460:\n",
        "                    return \"D8\"\n",
        "                elif 579 >= value >= 520:\n",
        "                    return \"D9\"\n",
        "                elif 629 >= value >= 580:\n",
        "                    return \"D10\"\n",
        "                elif 679 >= value >= 630:\n",
        "                    return \"D11\"\n",
        "                elif 709 >= value >= 680:\n",
        "                    return \"D12\"\n",
        "                elif 739 >= value >= 710:\n",
        "                    return \"D13\"\n",
        "                elif 759 >= value >= 740:\n",
        "                    return \"D14\"\n",
        "                elif 779 >= value >= 760:\n",
        "                    return \"D15\"\n",
        "                elif 799 >= value >= 780:\n",
        "                    return \"D16\"\n",
        "                elif 999 >= value >= 800:\n",
        "                    return \"D17\"\n",
        "                else:\n",
        "                    print(\"Diagnosis: {}\".format(code))\n",
        "            else:\n",
        "                if three_chars.startswith(\"E\") or three_chars.startswith(\"V\"):\n",
        "                    return \"D18\"\n",
        "                else:\n",
        "                    print(\"Diagnosis: {}\".format(code))\n",
        "                    return \"D0\"\n",
        "        else:  # Procedure Codes http://www.icd9data.com/2012/Volume3/default.htm\n",
        "            if three_chars.isdigit():\n",
        "                value = int(three_chars)\n",
        "                if value == 0:\n",
        "                    return \"P1\"\n",
        "                elif 5 >= value >= 1:\n",
        "                    return \"P2\"\n",
        "                elif 7 >= value >= 6:\n",
        "                    return \"P3\"\n",
        "                elif 16 >= value >= 8:\n",
        "                    return \"P4\"\n",
        "                elif 17 >= value >= 17:\n",
        "                    return \"P5\"\n",
        "                elif 20 >= value >= 18:\n",
        "                    return \"P6\"\n",
        "                elif 29 >= value >= 21:\n",
        "                    return \"P7\"\n",
        "                elif 34 >= value >= 30:\n",
        "                    return \"P8\"\n",
        "                elif 39 >= value >= 35:\n",
        "                    return \"P9\"\n",
        "                elif 41 >= value >= 40:\n",
        "                    return \"P10\"\n",
        "                elif 54 >= value >= 42:\n",
        "                    return \"P11\"\n",
        "                elif 59 >= value >= 55:\n",
        "                    return \"P12\"\n",
        "                elif 64 >= value >= 60:\n",
        "                    return \"P13\"\n",
        "                elif 71 >= value >= 65:\n",
        "                    return \"P14\"\n",
        "                elif 75 >= value >= 72:\n",
        "                    return \"P15\"\n",
        "                elif 84 >= value >= 76:\n",
        "                    return \"P16\"\n",
        "                elif 86 >= value >= 85:\n",
        "                    return \"P17\"\n",
        "                elif 99 >= value >= 87:\n",
        "                    return \"P18\"\n",
        "                else:\n",
        "                    print(\"Procedure: {}\".format(code))\n",
        "            else:\n",
        "                print(\"Procedure: {}\".format(code))\n",
        "    else:\n",
        "        return code\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    top_n_labels = read_admission_ids(\n",
        "        train_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/train_full_hadm_ids.csv\",\n",
        "        valid_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/dev_full_hadm_ids.csv\",\n",
        "        test_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/test_full_hadm_ids.csv\",\n",
        "        outdir=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/full/\")\n",
        "\n",
        "    read_admission_ids(\n",
        "        train_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/train_50_hadm_ids.csv\",\n",
        "        valid_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/dev_50_hadm_ids.csv\",\n",
        "        test_file=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/test_50_hadm_ids.csv\",\n",
        "        outdir=\"C:/Users/test/UIUC/HiCu-ICD-UIUC-LAAT-Evaluation/data/mimicdata/mimic3/50/\",\n",
        "        top_n_labels=top_n_labels)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "pP1EzbBV58Mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "### Model Overview\n",
        "The model defined in the code includes several classes representing different neural network architectures for automated ICD coding. These models include configurations for handling multi-label classification with a focus on curriculum learning and label hierarchies.\n",
        "\n",
        "### Original Paper's Link and Repo\n",
        "\n",
        "1. **Direct Link to the original paper: \"HiCu: Leveraging Hierarchy for Curriculum Learning in Automated ICD Coding\"**: https://arxiv.org/abs/2208.02301\n",
        "2. **Direct Link to the original paper repo**: https://github.com/wren93/HiCu-ICD (Switch between `main` and `LAAT` branches)\n",
        "\n",
        "### Model Descriptions\n",
        "\n",
        "- **LAAT (Label Attention model)**: This model, originally presented by Vu et al. (2020), employs a Bidirectional Long-Short Term Memory (Bi-LSTM) network which includes a Word2Vec embedding layer and a Bi-LSTM feature extraction layer. The Bi-LSTM encoder captures contextual information from both directions of the text, producing a matrix of text representations that can be used to attend to different parts of the input sequence when predicting ICD codes.\n",
        "\n",
        "- **MultiResCNN (Multi-Filter Residual Convolutional Neural Network)**:  Designed by Li and Yu (2020), the MultiResCNN is based on earlier work on TextCNN and ResNet architectures. It starts by converting input words into word embeddings and then applies multiple convolutional filters of different sizes, each topped with a residual layer to enhance the model's ability to capture features at various scales. These outputs are concatenated to form the final text representation matrix. The architecture is particularly adept at handling the multi-label classification inherent in ICD coding.\n",
        "\n",
        "- **RAC (Read, Attend and Code model)**: Introduced by Kim and Ganapathi (2021), this model is a combination of a Convolved Embedding Module and a Self-Attention Module. The Convolved Embedding Module first represents text using word embeddings followed by convolutional neural network layers, and then these representations are processed by the Self-Attention Module, which consists of a series of transformer blocks. This model is noted for its ability to handle permutation equivariance, meaning the order of the input sequence does not affect the output of ICD code assignments.\n",
        "\n",
        "The paper investigates the effect of HiCu (Hierarchical Curriculum Learning) on these models. HiCu leverages the hierarchical structure of ICD codes to enhance model training and performance, especially on rare codes which are difficult for models to learn due to the imbalanced nature of medical datasets. It applies knowledge transfer techniques and hyperbolic embedding corrections to improve performance across various metrics, including the macro and micro AUC and F1 scores .\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "- **Word Representation (WordRep) Module:**\n",
        "  - **Embedding Layer:** Utilizes pretrained embeddings with a dimension depending on the pretrained file, or initializes a new embedding layer if no pretrained file is provided. The embedding size can be 100, 300, etc., based on the available data.\n",
        "  - **Dropout:** Applied after the embedding layer to prevent overfitting, with a dropout rate of 0.1 as configured in the model settings. This helps improve the model's generalization capability on unseen data.\n",
        "\n",
        "- **Decoders:**\n",
        "  - **RandomlyInitializedDecoder, RACDecoder, LAATDecoder, Decoder:** Each uses an attention mechanism tailored to the needs of hierarchical ICD code prediction.\n",
        "    - **Attention Units:** Typically involves layers with dimensions tuned to the size of the dataset labels (e.g., number of ICD codes).\n",
        "    - **Activation Function:** Uses Tanh or ReLU in intermediate layers to introduce non-linearity.\n",
        "    - **Hyperbolic Embedding Layers:** Specific to HiCuA strategies, embedding sizes match the hyperbolic space dimensions used (commonly around 50 dimensions).\n",
        "\n",
        "- **MultiResCNN:**\n",
        "  - **Convolutional Layers:** Multiple convolutional layers with filter sizes that may vary from small (3-5 words) to large (7-9 words) to capture different levels of textual granularity.\n",
        "  - **Residual Connections:** Helps in flowing gradients and avoiding the vanishing gradient problem in deep networks.\n",
        "  - **Activation Function:** Uses Tanh activation functions following convolutional layers to add non-linearity.\n",
        "\n",
        "- **LongformerClassifier:**\n",
        "  - **Longformer Layers:** Uses a Longformer architecture, suitable for processing long text sequences with attention mechanisms that focus on different parts of the input sequence efficiently.\n",
        "  - **Configuration:** Configured with parameters such as number of attention heads, hidden dimensions (typically 768 for base models), and specific attention window sizes.\n",
        "\n",
        "### Training Objectives\n",
        "- **Loss Functions:**\n",
        "  - **Binary Cross-Entropy Loss:** Used for binary classification tasks such as ICD code prediction from clinical texts.\n",
        "  - **Asymmetric Loss:** Customized to handle imbalanced datasets, focusing more on the minority classes which are crucial in medical code predictions.\n",
        "- **Optimizer:**\n",
        "  - **Adam Optimizer:** Widely used for its efficiency in handling sparse gradients and adaptive learning rate capabilities.\n",
        "\n",
        "### Additional Configuration\n",
        "- **Pretrained Models:** Uses pretrained Longformer or other transformer models fine-tuned on medical texts to leverage prior knowledge and improve prediction accuracy.\n",
        "- **Monte Carlo Simulation:** Not directly mentioned in the paper, but could be integrated for evaluating model robustness and uncertainty in predictions.\n",
        "\n",
        "### Implementation Details\n",
        "- **Classes and Methods:** Code structure involves defining Python classes for each model type (e.g., `MultiResCNN`, `LAAT`), with methods for each operation like `forward` pass, loss computation, and backpropagation.\n",
        "- **Model Validation and Testing:** Functions to evaluate model performance on a validation set during training and a separate test set to assess generalizability.\n",
        "raining and a separate test set to assess generalizability.\n",
        "\n",
        "### LAAT (Label Attention model) Architecture:\n",
        "**Word Representation (WordRep) Module**\n",
        "\n",
        "- **Embedding Layer**: Utilizes Word2Vec pretrained embeddings, specifically a skip-gram model with an embedding size of 100. This layer transforms each token into a dense vector representation.\n",
        "- **Bi-directional LSTM (BiLSTM)**: Incorporates a Bi-directional Long Short-Term Memory layer with a hidden size of 256. This setup allows the model to capture contextual information from both past and future tokens effectively.\n",
        "- **Dropout**\n",
        ": Applied after the embedding layer with a rate of 0.3 to prevent overfitting, enhancing the model's generalization capabilities on unseen data.\n",
        "\n",
        "**Attention Mechanism**\n",
        "\n",
        "- **Label-wise Attention**: Implements an attention mechanism that focuses on different parts of the text, determined by the relevance to the specific ICD codes being predicted. This is critical for effectively handling the multi-label classification nature of ICD code assignment.\n",
        "- **Dimension (d_a)**: The attention mechanism utilizes a dimensionality of 256 for projecting the LSTM outputs before calculating attention scores.\n",
        "\n",
        "**Configuration and Training Details**\n",
        "\n",
        "- **Optimizer**: Utilizes the AdamW optimizer with a learning rate of 0.0005, combining the benefits of Adam optimization and weight decay regularization.\n",
        "- **Epochs and Patience**: The model is trained with a complex epoch strategy [1,1,1,1,50], employing early stopping based on a patience of 6 epochs to halt training if the validation performance does not improve.\n",
        "- **Loss Function**: Employs the Asymmetric Loss (ASL), configured with parameters \"1,0,0.03\" to handle the imbalance in the label distribution effectively.\n",
        "\n",
        "**Additional Settings**\n",
        "\n",
        "- **Sequence Length**: Capable of handling sequences up to 4000 tokens, making it suitable for processing lengthy clinical notes.\n",
        "- **Batch Size**: Set to process 8 documents per batch, balancing computational efficiency and memory constraints.\n",
        "\n",
        "## Pretrained Models\n",
        "\n",
        "`MultiResCNN with HiCuA`: Upon successful execution of the `MultiResCNN` model, a file with a `.pth` extension was produced. This file contains the checkpoint for the `MultiResCNN with HiCuA` model.\n",
        "\n",
        "`RAC with HiCuA`: Upon successful execution of the `RAC` model, a file with a `.pth` extension was produced. This file contains the checkpoint for the `RAC with HiCuA` model.\n",
        "\n",
        "`LAAT with HiCuA + ASL`: Upon successful execution of the LAAT model, a file named `best_model.pkl` was produced. This file includes the complete model state, with its parameters and architecture, representing the model that achieved the best performance during its training process.\n",
        "\n",
        "The `.pth` and `.pkl` files are availabe for download at the following URL: https://drive.google.com/drive/folders/1EJgVV2Vx8gUM0TKJldBJjsT30oBROW1U?usp=sharing"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Model Training Code\n",
        "\n",
        "**You can refer to the Demo section of `Getting Project Setup` for an example of runnable code. Please note that you need to follow all the sequences specified in that section to execute the code.**\n",
        "\n",
        "```\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import xavier_uniform_ as xavier_uniform\n",
        "import numpy as np\n",
        "from utils.utils import build_pretrain_embedding, load_embeddings\n",
        "from utils.losses import AsymmetricLoss, AsymmetricLossOptimized\n",
        "from math import floor, sqrt\n",
        "\n",
        "\n",
        "class WordRep(nn.Module):\n",
        "    def __init__(self, args, Y, dicts):\n",
        "        super(WordRep, self).__init__()\n",
        "\n",
        "        if args.embed_file:\n",
        "            print(\"loading pretrained embeddings from {}\".format(args.embed_file))\n",
        "            if args.use_ext_emb:\n",
        "                pretrain_word_embedding, pretrain_emb_dim = build_pretrain_embedding(args.embed_file, dicts['w2ind'],\n",
        "                                                                                     True)\n",
        "                W = torch.from_numpy(pretrain_word_embedding)\n",
        "            else:\n",
        "                W = torch.Tensor(load_embeddings(args.embed_file))\n",
        "\n",
        "            self.embed = nn.Embedding(W.size()[0], W.size()[1], padding_idx=0)\n",
        "            self.embed.weight.data = W.clone()\n",
        "        else:\n",
        "            # add 2 to include UNK and PAD\n",
        "            self.embed = nn.Embedding(len(dicts['w2ind']) + 2, args.embed_size, padding_idx=0)\n",
        "        self.feature_size = self.embed.embedding_dim\n",
        "\n",
        "        self.embed_drop = nn.Dropout(p=args.dropout)\n",
        "\n",
        "        self.conv_dict = {1: [self.feature_size, args.num_filter_maps],\n",
        "                     2: [self.feature_size, 100, args.num_filter_maps],\n",
        "                     3: [self.feature_size, 150, 100, args.num_filter_maps],\n",
        "                     4: [self.feature_size, 200, 150, 100, args.num_filter_maps]\n",
        "                     }\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [self.embed(x)]\n",
        "\n",
        "        x = torch.cat(features, dim=2)\n",
        "\n",
        "        x = self.embed_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RandomlyInitializedDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The original per-label attention network: query matrix is randomly initialized\n",
        "    \"\"\"\n",
        "    def __init__(self, args, Y, dicts, input_size):\n",
        "        super(RandomlyInitializedDecoder, self).__init__()\n",
        "\n",
        "        Y = Y[-1]\n",
        "\n",
        "        self.U = nn.Linear(input_size, Y)\n",
        "        xavier_uniform(self.U.weight)\n",
        "\n",
        "\n",
        "        self.final = nn.Linear(input_size, Y)\n",
        "        xavier_uniform(self.final.weight)\n",
        "\n",
        "        self.loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "    def forward(self, x, target, text_inputs):\n",
        "        # attention\n",
        "        alpha = F.softmax(self.U.weight.matmul(x.transpose(1, 2)), dim=2)\n",
        "\n",
        "        m = alpha.matmul(x)\n",
        "\n",
        "        y = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "\n",
        "        loss = self.loss_function(y, target)\n",
        "        return y, loss, alpha, m\n",
        "\n",
        "    def change_depth(self, depth=0):\n",
        "        # placeholder\n",
        "        pass\n",
        "\n",
        "\n",
        "class RACDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The decoder proposed by Kim et al. (Code title-guided attention)\n",
        "    \"\"\"\n",
        "    def __init__(self, args, Y, dicts, input_size):\n",
        "        super(RACDecoder, self).__init__()\n",
        "\n",
        "        Y = Y[-1]\n",
        "\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.register_buffer(\"c2title\", torch.LongTensor(dicts[\"c2title\"]))\n",
        "        self.word_rep = WordRep(args, Y, dicts)\n",
        "\n",
        "        filter_size = int(args.code_title_filter_size)\n",
        "        self.code_title_conv = nn.Conv1d(self.word_rep.feature_size, input_size,\n",
        "                                         filter_size, padding=int(floor(filter_size / 2)))\n",
        "        xavier_uniform(self.code_title_conv.weight)\n",
        "        self.code_title_maxpool = nn.MaxPool1d(args.num_code_title_tokens)\n",
        "\n",
        "        self.final = nn.Linear(input_size, Y)\n",
        "        xavier_uniform(self.final.weight)\n",
        "\n",
        "        self.loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, x, target, text_inputs):\n",
        "        code_title = self.word_rep(self._buffers['c2title']).transpose(1, 2)\n",
        "        # attention\n",
        "        U = self.code_title_conv(code_title)\n",
        "        U = self.code_title_maxpool(U).squeeze(-1)\n",
        "        U = torch.tanh(U)\n",
        "\n",
        "        attention_score = U.matmul(x.transpose(1, 2)) / sqrt(self.input_size)\n",
        "        alpha = F.softmax(attention_score, dim=2)\n",
        "\n",
        "        m = alpha.matmul(x)\n",
        "\n",
        "        y = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "\n",
        "        loss = self.loss_function(y, target)\n",
        "        return y, loss, alpha, m\n",
        "\n",
        "    def change_depth(self, depth=0):\n",
        "        # placeholder\n",
        "        pass\n",
        "\n",
        "\n",
        "class LAATDecoder(nn.Module):\n",
        "    def __init__(self, args, Y, dicts, input_size):\n",
        "        super(LAATDecoder, self).__init__()\n",
        "\n",
        "        Y = Y[-1]\n",
        "\n",
        "        self.attn_dim = args.attn_dim\n",
        "        self.W = nn.Linear(input_size, self.attn_dim)\n",
        "        self.U = nn.Linear(self.attn_dim, Y)\n",
        "        xavier_uniform(self.W.weight)\n",
        "        xavier_uniform(self.U.weight)\n",
        "\n",
        "        self.final = nn.Linear(input_size, Y)\n",
        "        xavier_uniform(self.final.weight)\n",
        "\n",
        "        self.loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, x, target, text_inputs):\n",
        "        z = torch.tanh(self.W(x))\n",
        "        # attention\n",
        "        alpha = F.softmax(self.U.weight.matmul(z.transpose(1, 2)), dim=2)\n",
        "\n",
        "        m = alpha.matmul(x)\n",
        "\n",
        "        y = self.final.weight.mul(m).sum(dim=2).add(self.final.bias)\n",
        "\n",
        "        loss = self.loss_function(y, target)\n",
        "        return y, loss, alpha, m\n",
        "\n",
        "    def change_depth(self, depth=0):\n",
        "        # placeholder\n",
        "        pass\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder: knowledge transfer initialization and hyperbolic embedding correction\n",
        "    \"\"\"\n",
        "    def __init__(self, args, Y, dicts, input_size):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.dicts = dicts\n",
        "\n",
        "        self.decoder_dict = nn.ModuleDict()\n",
        "        for i in range(len(Y)):\n",
        "            y = Y[i]\n",
        "            self.decoder_dict[str(i) + '_' + '0'] = nn.Linear(input_size, y)\n",
        "            self.decoder_dict[str(i) + '_' + '1'] = nn.Linear(input_size, y)\n",
        "            xavier_uniform(self.decoder_dict[str(i) + '_' + '0'].weight)\n",
        "            xavier_uniform(self.decoder_dict[str(i) + '_' + '1'].weight)\n",
        "\n",
        "        self.use_hyperbolic =  args.decoder.find(\"Hyperbolic\") != -1\n",
        "        if self.use_hyperbolic:\n",
        "            self.cat_hyperbolic = args.cat_hyperbolic\n",
        "            if not self.cat_hyperbolic:\n",
        "                self.hyperbolic_fc_dict = nn.ModuleDict()\n",
        "                for i in range(len(Y)):\n",
        "                    self.hyperbolic_fc_dict[str(i)] = nn.Linear(args.hyperbolic_dim, input_size)\n",
        "            else:\n",
        "                self.query_fc_dict = nn.ModuleDict()\n",
        "                for i in range(len(Y)):\n",
        "                    self.query_fc_dict[str(i)] = nn.Linear(input_size + args.hyperbolic_dim, input_size)\n",
        "\n",
        "            # build hyperbolic embedding matrix\n",
        "            self.hyperbolic_emb_dict = {}\n",
        "            for i in range(len(Y)):\n",
        "                self.hyperbolic_emb_dict[i] = np.zeros((Y[i], args.hyperbolic_dim))\n",
        "                for idx, code in dicts['ind2c'][i].items():\n",
        "                    self.hyperbolic_emb_dict[i][idx, :] = np.copy(dicts['poincare_embeddings'].get_vector(code))\n",
        "                self.register_buffer(name='hb_emb_' + str(i), tensor=torch.tensor(self.hyperbolic_emb_dict[i], dtype=torch.float32))\n",
        "\n",
        "        self.cur_depth = 5 - args.depth\n",
        "        self.is_init = False\n",
        "        self.change_depth(self.cur_depth)\n",
        "\n",
        "        if args.loss == 'BCE':\n",
        "            self.loss_function = nn.BCEWithLogitsLoss()\n",
        "        elif args.loss == 'ASL':\n",
        "            asl_config = [float(c) for c in args.asl_config.split(',')]\n",
        "            self.loss_function = AsymmetricLoss(gamma_neg=asl_config[0], gamma_pos=asl_config[1],\n",
        "                                                clip=asl_config[2], reduction=args.asl_reduction)\n",
        "        elif args.loss == 'ASLO':\n",
        "            asl_config = [float(c) for c in args.asl_config.split(',')]\n",
        "            self.loss_function = AsymmetricLossOptimized(gamma_neg=asl_config[0], gamma_pos=asl_config[1],\n",
        "                                                         clip=asl_config[2], reduction=args.asl_reduction)\n",
        "\n",
        "    def change_depth(self, depth=0):\n",
        "        if self.is_init:\n",
        "            # copy previous attention weights to current attention network based on ICD hierarchy\n",
        "            ind2c = self.dicts['ind2c']\n",
        "            c2ind = self.dicts['c2ind']\n",
        "            hierarchy_dist = self.dicts['hierarchy_dist']\n",
        "            for i, code in ind2c[depth].items():\n",
        "                tree = hierarchy_dist[depth][code]\n",
        "                pre_idx = c2ind[depth - 1][tree[depth - 1]]\n",
        "\n",
        "                self.decoder_dict[str(depth) + '_' + '0'].weight.data[i, :] = self.decoder_dict[str(depth - 1) + '_' + '0'].weight.data[pre_idx, :].clone()\n",
        "                self.decoder_dict[str(depth) + '_' + '1'].weight.data[i, :] = self.decoder_dict[str(depth - 1) + '_' + '1'].weight.data[pre_idx, :].clone()\n",
        "\n",
        "        if not self.is_init:\n",
        "            self.is_init = True\n",
        "\n",
        "        self.cur_depth = depth\n",
        "\n",
        "    def forward(self, x, target, text_inputs):\n",
        "        # attention\n",
        "        if self.use_hyperbolic:\n",
        "            if not self.cat_hyperbolic:\n",
        "                query = self.decoder_dict[str(self.cur_depth) + '_' + '0'].weight + self.hyperbolic_fc_dict[str(self.cur_depth)](self._buffers['hb_emb_' + str(self.cur_depth)])\n",
        "            else:\n",
        "                query = torch.cat([self.decoder_dict[str(self.cur_depth) + '_' + '0'].weight, self._buffers['hb_emb_' + str(self.cur_depth)]], dim=1)\n",
        "                query = self.query_fc_dict[str(self.cur_depth)](query)\n",
        "        else:\n",
        "            query = self.decoder_dict[str(self.cur_depth) + '_' + '0'].weight\n",
        "\n",
        "        alpha = F.softmax(query.matmul(x.transpose(1, 2)), dim=2)\n",
        "        m = alpha.matmul(x)\n",
        "\n",
        "        y = self.decoder_dict[str(self.cur_depth) + '_' + '1'].weight.mul(m).sum(dim=2).add(self.decoder_dict[str(self.cur_depth) + '_' + '1'].bias)\n",
        "\n",
        "        loss = self.loss_function(y, target)\n",
        "\n",
        "        return y, loss, alpha, m\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, inchannel, outchannel, kernel_size, stride, use_res, dropout):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.left = nn.Sequential(\n",
        "            nn.Conv1d(inchannel, outchannel, kernel_size=kernel_size, stride=stride, padding=int(floor(kernel_size / 2)), bias=False),\n",
        "            nn.BatchNorm1d(outchannel),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv1d(outchannel, outchannel, kernel_size=kernel_size, stride=1, padding=int(floor(kernel_size / 2)), bias=False),\n",
        "            nn.BatchNorm1d(outchannel)\n",
        "        )\n",
        "\n",
        "        self.use_res = use_res\n",
        "        if self.use_res:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                        nn.Conv1d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
        "                        nn.BatchNorm1d(outchannel)\n",
        "                    )\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.left(x)\n",
        "        if self.use_res:\n",
        "            out += self.shortcut(x)\n",
        "        out = torch.tanh(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiResCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, args, Y, dicts):\n",
        "        super(MultiResCNN, self).__init__()\n",
        "\n",
        "        self.word_rep = WordRep(args, Y, dicts)\n",
        "\n",
        "        self.conv = nn.ModuleList()\n",
        "        filter_sizes = args.filter_size.split(',')\n",
        "\n",
        "        self.filter_num = len(filter_sizes)\n",
        "        for filter_size in filter_sizes:\n",
        "            filter_size = int(filter_size)\n",
        "            one_channel = nn.ModuleList()\n",
        "            tmp = nn.Conv1d(self.word_rep.feature_size, self.word_rep.feature_size, kernel_size=filter_size,\n",
        "                            padding=int(floor(filter_size / 2)))\n",
        "            xavier_uniform(tmp.weight)\n",
        "            one_channel.add_module('baseconv', tmp)\n",
        "\n",
        "            conv_dimension = self.word_rep.conv_dict[args.conv_layer]\n",
        "            for idx in range(args.conv_layer):\n",
        "                tmp = ResidualBlock(conv_dimension[idx], conv_dimension[idx + 1], filter_size, 1, True,\n",
        "                                    args.dropout)\n",
        "                one_channel.add_module('resconv-{}'.format(idx), tmp)\n",
        "\n",
        "            self.conv.add_module('channel-{}'.format(filter_size), one_channel)\n",
        "\n",
        "        if args.decoder == \"HierarchicalHyperbolic\" or args.decoder == \"Hierarchical\":\n",
        "            self.decoder = Decoder(args, Y, dicts, self.filter_num * args.num_filter_maps)\n",
        "        elif args.decoder == \"RandomlyInitialized\":\n",
        "            self.decoder = RandomlyInitializedDecoder(args, Y, dicts, self.filter_num * args.num_filter_maps)\n",
        "        elif args.decoder == \"CodeTitle\":\n",
        "            self.decoder = RACDecoder(args, Y, dicts, self.filter_num * args.num_filter_maps)\n",
        "        else:\n",
        "            raise RuntimeError(\"wrong decoder name\")\n",
        "\n",
        "        self.cur_depth = 5 - args.depth\n",
        "\n",
        "\n",
        "    def forward(self, x, target, text_inputs):\n",
        "        x = self.word_rep(x)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        conv_result = []\n",
        "        for conv in self.conv:\n",
        "            tmp = x\n",
        "            for idx, md in enumerate(conv):\n",
        "                if idx == 0:\n",
        "                    tmp = torch.tanh(md(tmp))\n",
        "                else:\n",
        "                    tmp = md(tmp)\n",
        "            tmp = tmp.transpose(1, 2)\n",
        "            conv_result.append(tmp)\n",
        "        x = torch.cat(conv_result, dim=2)\n",
        "\n",
        "        y, loss, alpha, m = self.decoder(x, target, text_inputs)\n",
        "\n",
        "        return y, loss, alpha, m\n",
        "\n",
        "    def freeze_net(self):\n",
        "        for p in self.word_rep.embed.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "import os\n",
        "from transformers import LongformerModel, LongformerConfig\n",
        "class LongformerClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, args, Y, dicts):\n",
        "        super(LongformerClassifier, self).__init__()\n",
        "\n",
        "        if args.longformer_dir != '':\n",
        "            print(\"loading pretrained longformer from {}\".format(args.longformer_dir))\n",
        "            config_file = os.path.join(args.longformer_dir, 'config.json')\n",
        "            self.config = LongformerConfig.from_json_file(config_file)\n",
        "            print(\"Model config {}\".format(self.config))\n",
        "            self.longformer = LongformerModel.from_pretrained(args.longformer_dir, gradient_checkpointing=True)\n",
        "        else:\n",
        "            self.config = LongformerConfig(\n",
        "                attention_mode=\"longformer\",\n",
        "                attention_probs_dropout_prob=0.1,\n",
        "                attention_window=[\n",
        "                    512,\n",
        "                    512,\n",
        "                    512,\n",
        "                    512,\n",
        "                    512,\n",
        "                    512,\n",
        "                ],\n",
        "                bos_token_id=0,\n",
        "                eos_token_id=2,\n",
        "                gradient_checkpointing=False,\n",
        "                hidden_act=\"gelu\",\n",
        "                hidden_dropout_prob=0.1,\n",
        "                hidden_size=768,\n",
        "                ignore_attention_mask=False,\n",
        "                initializer_range=0.02,\n",
        "                intermediate_size=3072,\n",
        "                layer_norm_eps=1e-05,\n",
        "                max_position_embeddings=4098,\n",
        "                model_type=\"longformer\",\n",
        "                num_attention_heads=12,\n",
        "                num_hidden_layers=6,\n",
        "                pad_token_id=1,\n",
        "                sep_token_id=2,\n",
        "                type_vocab_size=1,\n",
        "                vocab_size=50265\n",
        "            )\n",
        "            self.longformer = LongformerModel(self.config)\n",
        "\n",
        "        # decoder\n",
        "        self.decoder = Decoder(args, Y, dicts, self.config.hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask, target):\n",
        "        global_attention_mask = torch.zeros_like(input_ids)\n",
        "            # global attention on cls token\n",
        "            # global_attention_mask[:, 0] = 1 # this line should be commented if using decoder\n",
        "        longformer_output = self.longformer(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            global_attention_mask=global_attention_mask,\n",
        "            return_dict=False\n",
        "        )\n",
        "\n",
        "        output = longformer_output[0]\n",
        "        y, loss, alpha, m = self.decoder(output, target, None)\n",
        "\n",
        "        return y, loss, alpha, m\n",
        "\n",
        "    def freeze_net(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RACReader(nn.Module):\n",
        "    def __init__(self, args, Y, dicts):\n",
        "        super(RACReader, self).__init__()\n",
        "\n",
        "        self.word_rep = WordRep(args, Y, dicts)\n",
        "        filter_size = int(args.filter_size)\n",
        "\n",
        "        self.conv = nn.ModuleList()\n",
        "        for i in range(args.reader_conv_num):\n",
        "            conv = nn.Conv1d(self.word_rep.feature_size, self.word_rep.feature_size, kernel_size=filter_size,\n",
        "                                padding=int(floor(filter_size / 2)))\n",
        "            xavier_uniform(conv.weight)\n",
        "            self.conv.add_module(f'conv_{i+1}', conv)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=args.dropout)\n",
        "\n",
        "        self.trans = nn.ModuleList()\n",
        "        for i in range(args.reader_trans_num):\n",
        "            trans = nn.TransformerEncoderLayer(self.word_rep.feature_size, 1, args.trans_ff_dim, args.dropout, \"relu\")\n",
        "            self.trans.add_module(f'trans_{i+1}', trans)\n",
        "\n",
        "        if args.decoder == \"HierarchicalHyperbolic\" or args.decoder == \"Hierarchical\":\n",
        "            self.decoder = Decoder(args, Y, dicts, self.word_rep.feature_size)\n",
        "        elif args.decoder == \"RandomlyInitialized\":\n",
        "            self.decoder = RandomlyInitializedDecoder(args, Y, dicts, self.word_rep.feature_size)\n",
        "        elif args.decoder == \"CodeTitle\":\n",
        "            self.decoder = RACDecoder(args, Y, dicts, self.word_rep.feature_size)\n",
        "        else:\n",
        "            raise RuntimeError(\"wrong decoder name\")\n",
        "\n",
        "    def forward(self, x, target, text_inputs=None):\n",
        "        x = self.word_rep(x)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        for conv in self.conv:\n",
        "            x = conv(x)\n",
        "\n",
        "        x = torch.tanh(x).permute(2, 0, 1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for trans in self.trans:\n",
        "            x = trans(x)\n",
        "\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        y, loss, alpha, m = self.decoder(x, target, text_inputs)\n",
        "\n",
        "        return y, loss, alpha, m\n",
        "\n",
        "    def freeze_net(self):\n",
        "        for p in self.word_rep.embed.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "class LAAT(nn.Module):\n",
        "    def __init__(self, args, Y, dicts):\n",
        "        super(LAAT, self).__init__()\n",
        "        self.word_rep = WordRep(args, Y, dicts)\n",
        "\n",
        "        self.hidden_dim = args.lstm_hidden_dim\n",
        "        self.biLSTM = nn.LSTM(\n",
        "            input_size=self.word_rep.feature_size,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            batch_first=True,\n",
        "            dropout=args.dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.output_dim = 2 * self.hidden_dim\n",
        "        self.use_LAAT = False\n",
        "\n",
        "        self.attn_dim = args.attn_dim\n",
        "        self.decoder_name = args.decoder\n",
        "        if \"LAAT\" in args.decoder:\n",
        "            if args.decoder == \"LAATHierarchicalHyperbolic\" or args.decoder == \"LAATHierarchical\":\n",
        "                self.decoder_name = args.decoder[4:]\n",
        "            self.output_dim = self.attn_dim\n",
        "            self.use_LAAT = True\n",
        "            self.W = nn.Linear(2 * self.hidden_dim, self.attn_dim)\n",
        "\n",
        "        if self.decoder_name == \"HierarchicalHyperbolic\" or self.decoder_name == \"Hierarchical\":\n",
        "            self.decoder = Decoder(args, Y, dicts, self.output_dim)\n",
        "        elif self.decoder_name == \"RandomlyInitialized\":\n",
        "            self.decoder = RandomlyInitializedDecoder(args, Y, dicts, self.output_dim)\n",
        "        elif self.decoder_name == \"CodeTitle\":\n",
        "            self.decoder = RACDecoder(args, Y, dicts, self.output_dim)\n",
        "        elif self.decoder_name == \"LAATDecoder\":\n",
        "            self.decoder = RandomlyInitializedDecoder(args, Y, dicts, self.output_dim)\n",
        "        else:\n",
        "            raise RuntimeError(\"wrong decoder name\")\n",
        "\n",
        "\n",
        "\n",
        "        self.cur_depth = 5 - args.depth\n",
        "\n",
        "    def forward(self, x, target, text_inputs):\n",
        "        # lengths = (x > 0).sum(dim=1).cpu()\n",
        "        x = self.word_rep(x)  # [batch, length, input_size]\n",
        "\n",
        "        # x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        x1 = self.biLSTM(x)[0]\n",
        "        # x1 = pad_packed_sequence(x1, batch_first=True)[0]\n",
        "\n",
        "        if self.use_LAAT:\n",
        "            x1 = torch.tanh(self.W(x1))\n",
        "\n",
        "        y, loss, alpha, m = self.decoder(x1, target, text_inputs)\n",
        "\n",
        "        return y, loss, alpha, m\n",
        "\n",
        "\n",
        "def pick_model(args, dicts):\n",
        "    ind2c = dicts['ind2c']\n",
        "    Y = [len(ind2c[i]) for i in range(5)] # total number of ICD codes\n",
        "    if args.model == 'MultiResCNN':\n",
        "        model = MultiResCNN(args, Y, dicts)\n",
        "    elif args.model == 'longformer':\n",
        "        model = LongformerClassifier(args, Y, dicts)\n",
        "    elif args.model == 'RACReader':\n",
        "        model = RACReader(args, Y, dicts)\n",
        "    elif args.model == 'LAAT':\n",
        "        model = LAAT(args, Y, dicts)\n",
        "    else:\n",
        "        raise RuntimeError(\"wrong model name\")\n",
        "\n",
        "    if args.test_model:\n",
        "        model.decoder.change_depth(4)\n",
        "        sd = torch.load(args.test_model)\n",
        "        model.load_state_dict(sd)\n",
        "    if args.tune_wordemb == False:\n",
        "        model.freeze_net()\n",
        "    if len(args.gpu_list) == 1 and args.gpu_list[0] != -1: # single card training\n",
        "        model.cuda()\n",
        "    elif len(args.gpu_list) > 1: # multi-card training\n",
        "        model = nn.DataParallel(model, device_ids=args.gpu_list)\n",
        "        model = model.to(f'cuda:{model.device_ids[0]}')\n",
        "    return model\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "6-wIjaCDo0VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "**For details regarding the training code, please refer to the [Implementation of Model Training Code](https://colab.research.google.com/drive/1WczLlKib2QdC8o8xzBHZQcZgb2aPh-a8#scrollTo=Implementation_of_Model_Training_Code) in the `Methodology` section.**\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "Below are some of the hyperparameters for each of the models tested:\n",
        "\n",
        "**1. MultiResCNN with HiCuA**\n",
        "- **Batch size**: 8\n",
        "- **Learning rate (lr)**: 0.00005\n",
        "- **Max epochs (n_epochs)**: Varied (2, 3, 5, 10, 500)\n",
        "- **Depth**: 5 layers\n",
        "- **Dropout**: 0.2\n",
        "- **Attention dimension**: 512\n",
        "- **LSTM hidden dimension**: 512\n",
        "- **Transformer feed-forward dimension**: 1024\n",
        "\n",
        "**2. RAC with HiCuA**\n",
        "- **Batch size**: 16\n",
        "- **Learning rate (lr)**: 0.00008\n",
        "- **Max epochs (n_epochs)**: Varied (2, 3, 5, 7, 500)\n",
        "- **Dropout**: 0.1\n",
        "- **Decoder**: 'HierarchicalHyperbolic'\n",
        "- **Loss function**: 'BCE'\n",
        "- **Tune word embeddings (tune_wordemb)**: True\n",
        "- **Scheduler**: 0.9\n",
        "- **Scheduler patience**: 5\n",
        "- **Weight decay**: 0\n",
        "- **Random seed**: 1\n",
        "- **Number of convolutional layers (reader_conv_num)**: 2\n",
        "- **Number of transformer blocks (reader_trans_num)**: 4\n",
        "\n",
        "**3. LAAT with HiCuA + ASL**\n",
        "- **Batch size**: 8\n",
        "- **Learning rate (lr)**: 0.0005\n",
        "- **Max epochs (n_epochs)**: Varied (1, 1, 1, 1, 50)\n",
        "- **Dropout**: 0.3\n",
        "- **Hidden size**: 256 (in the LSTM layer)\n",
        "- **Bidirectional**: Enabled (1)\n",
        "- **ASL config**: '1,0,0.03'\n",
        "- **ASL reduction**: 'sum'\n",
        "- **Decoder**: 'HierarchicalHyperbolic'\n",
        "- **Depth**: 5\n",
        "- **Hyperbolic dimension**: 50\n",
        "- **Loss function**: 'ASL'\n",
        "- **LR scheduler factor**: 0.9\n",
        "- **LR scheduler patience**: 2\n",
        "- **Main metric**: 'micro_f1'\n",
        "- **Max sequence length**: 4000\n",
        "- **Level projection size**: 128\n",
        "- **Optimiser**: 'adamw'\n",
        "- **Patience**: 6\n",
        "- **Penalisation coefficient**: 0.01\n",
        "- **Problem name**: 'mimic-iii_cl_50'\n",
        "- **RNN model**: 'LSTM'\n",
        "- **Save results on train**: True\n",
        "- **Shuffle data**: Enabled (1)\n",
        "- **Use LR scheduler**: Enabled (1)\n",
        "\n",
        "## Computational Requirements\n",
        "\n",
        "- **Bi-LSTM and MultiResCNN Models**: These models were trained on a single NVIDIA Tesla V100 GPU as stated in the paper.\n",
        "- **RAC Reader-based Models**: These required more computational power, utilizing 4 NVIDIA Tesla V100 GPUs for training.\n",
        "- **Current Setup**: I am using a single RTX 4090 GPU for training with 24 GB of Dedicated GPU memory and 128 GB of RAM. I have used this machine to train and test the following models: `MultiResCNN with HiCuA`, `RAC-based model with HiCuA` and `LAAT with HiCuA and ASL`\n",
        "\n",
        "### Average Runtime for Each Epoch\n",
        "\n",
        "#### MultiResCNN with HiCuA\n",
        "\n",
        "- Approximately **10.73** minutes per epoch.\n",
        "\n",
        "#### RAC with HiCuA\n",
        "\n",
        "1.  **Depth 0**: Approximately **16.59** minutes per epoch.\n",
        "2.  **Depth 1**: Approximately **17.02** minutes per epoch.\n",
        "3.  **Depth 2**: Approximately **17.86** minutes per epoch.\n",
        "4.  **Depth 3**: Approximately **104.78** minutes per epoch.\n",
        "5.  **Depth 4**: Approximately **555.32** minutes per epoch.\n",
        "\n",
        "#### LAAT with HiCuA + ASL\n",
        "\n",
        "1.  **Depth 0**: Approximately **8** minutes per epoch.\n",
        "2.  **Depth 1**: Approximately **7** minutes per epoch.\n",
        "3.  **Depth 2**: Approximately **7** minutes per epoch.\n",
        "4.  **Depth 3**: Approximately **7** minutes per epoch.\n",
        "5.  **Depth 4**: Approximately **8.65** minutes per epoch.\n",
        "\n",
        "### Total Number of Trials\n",
        "\n",
        "#### MultiResCNN with HiCuA\n",
        "\n",
        "- In the evaluation of the MultiResCNN with HiCuA model, a total of **5** trials were conducted, corresponding to the **5** depth levels of training, each representing a unique training cycle.\n",
        "\n",
        "#### RAC with HiCuA\n",
        "\n",
        "- In the evaluation of the RAC with HiCuA model, a total of **5** trials were conducted, corresponding to the **5** depth levels of training, each representing a unique training cycle.\n",
        "\n",
        "#### LAAT with HiCuA + ASL\n",
        "\n",
        "- In the evaluation of the LAAT with HiCuA + ASL model, a total of **5** trials were conducted, corresponding to the **5** depth levels of training, each representing a unique training cycle.\n",
        "\n",
        "### GPU Hours Used\n",
        "\n",
        "- **MultiResCNN with HiCuA**: **7.69** hours (Training) + **1.08** hours (Evaluation) = **8.77** hours\n",
        "- **RAC with HiCuA**: **140.67** hours (Training) + **0.458** hours (Evaluation) = **141.13** hours\n",
        "- **LAAT with HiCuA + ASL**: **2.08** hours\n",
        "\n",
        "### Number of Training Epochs\n",
        "\n",
        "#### MultiResCNN with HiCuA\n",
        "1.  **Depth 0**: 2 epochs\n",
        "2.  **Depth 1**: 3 epochs\n",
        "3.  **Depth 2**: 5 epochs\n",
        "4.  **Depth 3**: 10 epochs\n",
        "5.  **Depth 4**: Did not complete all planned 500 epochs but stopped early due to an early stopping condition, reaching 23 epochs before the process was terminated.\n",
        "**Summing these gives us: 2+3+5+10+23=43 epochs.**\n",
        "\n",
        "#### RAC with HiCuA\n",
        "\n",
        "1.  **Depth 0**: 2 epochs\n",
        "2.  **Depth 1**: 3 epochs\n",
        "3.  **Depth 2**: 5 epochs\n",
        "4.  **Depth 3**: 7 epochs\n",
        "5.  **Depth 4**: Did not complete all planned 500 epochs but stopped early due to an early stopping condition, reaching 13 epochs before the process was terminated.\n",
        "**Summing these gives us: 2+3+5+7+13=30 epochs.**\n",
        "\n",
        "#### LAAT with HiCuA + ASL\n",
        "\n",
        "1.  **Depth 0**: 1 epoch\n",
        "2.  **Depth 1**: 1 epoch\n",
        "3.  **Depth 2**: 1 epoch\n",
        "4.  **Depth 3**: 1 epoch\n",
        "5.  **Depth 4**: 11 epochs\n",
        "**Summing these gives us: 1+1+1+1+11=15 epochs.**\n",
        "\n",
        "## Implementation Code\n",
        "\n",
        "Due to the extent of the files included, I will provide the link to the GitHub repository at the following link, which contains the fork of the original HiCu-ICD project: [HiCu-ICD-UIUC-Evaluation](https://github.com/SaadatUIUC/HiCu-ICD-UIUC-Evaluation).\n",
        "\n",
        "**GitHub Address**: https://github.com/SaadatUIUC/HiCu-ICD-UIUC-Evaluation\n",
        "\n",
        "**For LAAT**\n",
        "\n",
        "Please refer to the `LAAT`-specific repository at the following link, which contains a fork of the original HiCu-ICD-LAAT project:: [HiCu-ICD-UIUC-LAAT-Evaluation](https://github.com/SaadatUIUC/HiCu-ICD-UIUC-LAAT-Evaluation).\n",
        "\n",
        "**GitHub Address**: https://github.com/SaadatUIUC/HiCu-ICD-UIUC-LAAT-Evaluation"
      ],
      "metadata": {
        "id": "OOlY2lk1opuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Log for MultiResCNN with HiCuA\n",
        "\n",
        "The following section contains the log that was procued during the training of MutiResCNN with HiCuA on a dedicated machine.\n",
        "\n",
        "```\n",
        "(hicu_env) C:\\Users\\test\\UIUC\\HiCu-ICD-UIUC-Evaluation-Private>runs\\run_multirescnn_hicua.bat\n",
        "Namespace(DATA_DIR='./data', MAX_LENGTH=4096, MIMIC_2_DIR='./data/mimic2', MIMIC_3_DIR='./data/mimic3', MODEL_DIR='./models', Y='full', asl_config='0,0,0', asl_reduction='sum', attn_dim=512, batch_size=8, cat_hyperbolic=False, code_title_filter_size=9, command='python main.py --MODEL_DIR ./models --DATA_DIR ./data --MIMIC_3_DIR ./data/mimic3 --data_path ./data/mimic3/train_full.csv --embed_file ./data/mimic3/processed_full_100.embed --vocab ./data/mimic3/vocab.csv --Y full --model MultiResCNN --decoder HierarchicalHyperbolic --criterion prec_at_8 --MAX_LENGTH 4096 --batch_size 8 --lr 5e-5 --depth 5 --n_epochs 2,3,5,10,500 --num_workers 8 --hyperbolic_dim 50', conv_layer=1, criterion='prec_at_8', data_path='./data/mimic3/train_full.csv', decoder='HierarchicalHyperbolic', depth=5, dropout=0.2, embed_file='./data/mimic3/processed_full_100.embed', filter_size='3,5,9,15,19,25', gpu='0', gpu_list=[0], hyperbolic_dim=50, longformer_dir='', loss='BCE', lr=5e-05, lstm_hidden_dim=512, model='MultiResCNN', n_epochs='2,3,5,10,500', num_code_title_tokens=36, num_filter_maps=50, num_workers=8, patience=10, random_seed=1, reader_conv_num=2, reader_trans_num=4, scheduler=0.9, scheduler_patience=5, test_model=None, thres=0.5, trans_ff_dim=1024, tune_wordemb=True, use_ext_emb=False, version='mimic3', vocab='./data/mimic3/vocab.csv', weight_decay=0)\n",
        "loading lookups...\n",
        "Depth 0: 34\n",
        "Depth 1: 270\n",
        "Depth 2: 1158\n",
        "Depth 3: 5137\n",
        "Depth 4: 8921\n",
        "Training hyperbolic embeddings...\n",
        "loading pretrained embeddings from ./data/mimic3/processed_full_100.embed\n",
        "adding unk embedding\n",
        "MultiResCNN(\n",
        "  (word_rep): WordRep(\n",
        "    (embed): Embedding(51921, 100, padding_idx=0)\n",
        "    (embed_drop): Dropout(p=0.2, inplace=False)\n",
        "  )\n",
        "  (conv): ModuleList(\n",
        "    (channel-3): ModuleList(\n",
        "      (baseconv): Conv1d(100, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
        "      (resconv-0): ResidualBlock(\n",
        "        (left): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (2): Tanh()\n",
        "          (3): Conv1d(50, 50, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
        "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (shortcut): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (dropout): Dropout(p=0.2, inplace=False)\n",
        "      )\n",
        "    )\n",
        "    (channel-5): ModuleList(\n",
        "      (baseconv): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
        "      (resconv-0): ResidualBlock(\n",
        "        (left): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (2): Tanh()\n",
        "          (3): Conv1d(50, 50, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
        "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (shortcut): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (dropout): Dropout(p=0.2, inplace=False)\n",
        "      )\n",
        "    )\n",
        "    (channel-9): ModuleList(\n",
        "      (baseconv): Conv1d(100, 100, kernel_size=(9,), stride=(1,), padding=(4,))\n",
        "      (resconv-0): ResidualBlock(\n",
        "        (left): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (2): Tanh()\n",
        "          (3): Conv1d(50, 50, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
        "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (shortcut): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (dropout): Dropout(p=0.2, inplace=False)\n",
        "      )\n",
        "    )\n",
        "    (channel-15): ModuleList(\n",
        "      (baseconv): Conv1d(100, 100, kernel_size=(15,), stride=(1,), padding=(7,))\n",
        "      (resconv-0): ResidualBlock(\n",
        "        (left): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (2): Tanh()\n",
        "          (3): Conv1d(50, 50, kernel_size=(15,), stride=(1,), padding=(7,), bias=False)\n",
        "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (shortcut): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (dropout): Dropout(p=0.2, inplace=False)\n",
        "      )\n",
        "    )\n",
        "    (channel-19): ModuleList(\n",
        "      (baseconv): Conv1d(100, 100, kernel_size=(19,), stride=(1,), padding=(9,))\n",
        "      (resconv-0): ResidualBlock(\n",
        "        (left): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (2): Tanh()\n",
        "          (3): Conv1d(50, 50, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
        "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (shortcut): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (dropout): Dropout(p=0.2, inplace=False)\n",
        "      )\n",
        "    )\n",
        "    (channel-25): ModuleList(\n",
        "      (baseconv): Conv1d(100, 100, kernel_size=(25,), stride=(1,), padding=(12,))\n",
        "      (resconv-0): ResidualBlock(\n",
        "        (left): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(25,), stride=(1,), padding=(12,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (2): Tanh()\n",
        "          (3): Conv1d(50, 50, kernel_size=(25,), stride=(1,), padding=(12,), bias=False)\n",
        "          (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (shortcut): Sequential(\n",
        "          (0): Conv1d(100, 50, kernel_size=(1,), stride=(1,), bias=False)\n",
        "          (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "        (dropout): Dropout(p=0.2, inplace=False)\n",
        "      )\n",
        "    )\n",
        "  )\n",
        "  (decoder): Decoder(\n",
        "    (decoder_dict): ModuleDict(\n",
        "      (0_0): Linear(in_features=300, out_features=34, bias=True)\n",
        "      (0_1): Linear(in_features=300, out_features=34, bias=True)\n",
        "      (1_0): Linear(in_features=300, out_features=270, bias=True)\n",
        "      (1_1): Linear(in_features=300, out_features=270, bias=True)\n",
        "      (2_0): Linear(in_features=300, out_features=1158, bias=True)\n",
        "      (2_1): Linear(in_features=300, out_features=1158, bias=True)\n",
        "      (3_0): Linear(in_features=300, out_features=5137, bias=True)\n",
        "      (3_1): Linear(in_features=300, out_features=5137, bias=True)\n",
        "      (4_0): Linear(in_features=300, out_features=8921, bias=True)\n",
        "      (4_1): Linear(in_features=300, out_features=8921, bias=True)\n",
        "    )\n",
        "    (hyperbolic_fc_dict): ModuleDict(\n",
        "      (0): Linear(in_features=50, out_features=300, bias=True)\n",
        "      (1): Linear(in_features=50, out_features=300, bias=True)\n",
        "      (2): Linear(in_features=50, out_features=300, bias=True)\n",
        "      (3): Linear(in_features=50, out_features=300, bias=True)\n",
        "      (4): Linear(in_features=50, out_features=300, bias=True)\n",
        "    )\n",
        "    (loss_function): BCEWithLogitsLoss()\n",
        "  )\n",
        ")\n",
        "train_instances 47719\n",
        "dev_instances 1631\n",
        "test_instances 3372\n",
        "Total epochs at each level: [2, 3, 5, 10, 500]\n",
        "Training model at depth 0:\n",
        "EPOCH 0\n",
        "C:\\Users\\saada\\Desktop\\UIUC\\HiCu-ICD-UIUC-Evaluation-Private\\utils\\train_test.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
        "  inputs_id, labels = torch.LongTensor(inputs_id), torch.FloatTensor(labels[cur_depth])\n",
        "epoch finish in 507.97s, loss: 0.3020\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4229, 0.6272, 0.4867, 0.5481, 0.8636\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.6121, 0.8197, 0.7073, 0.7594, 0.9414\n",
        "rec_at_5: 0.5307\n",
        "prec_at_5: 0.8748\n",
        "rec_at_8: 0.7171\n",
        "prec_at_8: 0.7665\n",
        "rec_at_15: 0.9379\n",
        "prec_at_15: 0.5623\n",
        "\n",
        "evaluation finish in 45.28s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 527.53s, loss: 0.2510\n",
        "last epoch: testing on dev and test sets\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4776, 0.6324, 0.5555, 0.5914, 0.8921\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.6460, 0.8284, 0.7459, 0.7850, 0.9507\n",
        "rec_at_5: 0.5437\n",
        "prec_at_5: 0.8925\n",
        "rec_at_8: 0.7352\n",
        "prec_at_8: 0.7849\n",
        "rec_at_15: 0.9499\n",
        "prec_at_15: 0.5699\n",
        "\n",
        "evaluation finish in 39.12s\n",
        "file for evaluation: ./data/mimic3/test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4774, 0.6603, 0.5497, 0.5999, 0.8748\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.6514, 0.8327, 0.7495, 0.7889, 0.9498\n",
        "rec_at_5: 0.5383\n",
        "prec_at_5: 0.8937\n",
        "rec_at_8: 0.7276\n",
        "prec_at_8: 0.7873\n",
        "rec_at_15: 0.9459\n",
        "prec_at_15: 0.5745\n",
        "\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "Training model at depth 1:\n",
        "EPOCH 0\n",
        "epoch finish in 563.96s, loss: 0.0921\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1564, 0.2938, 0.1833, 0.2258, 0.8591\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4542, 0.7868, 0.5179, 0.6247, 0.9626\n",
        "rec_at_5: 0.3632\n",
        "prec_at_5: 0.8487\n",
        "rec_at_8: 0.5031\n",
        "prec_at_8: 0.7591\n",
        "rec_at_15: 0.6911\n",
        "prec_at_15: 0.5843\n",
        "\n",
        "evaluation finish in 41.13s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 541.92s, loss: 0.0773\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.2085, 0.3408, 0.2426, 0.2834, 0.8844\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5019, 0.8126, 0.5676, 0.6683, 0.9697\n",
        "rec_at_5: 0.3798\n",
        "prec_at_5: 0.8792\n",
        "rec_at_8: 0.5325\n",
        "prec_at_8: 0.7986\n",
        "rec_at_15: 0.7237\n",
        "prec_at_15: 0.6121\n",
        "\n",
        "evaluation finish in 46.92s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 2\n",
        "epoch finish in 431.01s, loss: 0.0719\n",
        "last epoch: testing on dev and test sets\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.2378, 0.3865, 0.2809, 0.3253, 0.9032\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5321, 0.8048, 0.6110, 0.6946, 0.9732\n",
        "rec_at_5: 0.3867\n",
        "prec_at_5: 0.8917\n",
        "rec_at_8: 0.5433\n",
        "prec_at_8: 0.8130\n",
        "rec_at_15: 0.7425\n",
        "prec_at_15: 0.6270\n",
        "\n",
        "evaluation finish in 43.82s\n",
        "file for evaluation: ./data/mimic3/test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.2388, 0.4058, 0.2829, 0.3334, 0.8964\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5294, 0.8040, 0.6078, 0.6923, 0.9722\n",
        "rec_at_5: 0.3798\n",
        "prec_at_5: 0.8940\n",
        "rec_at_8: 0.5324\n",
        "prec_at_8: 0.8146\n",
        "rec_at_15: 0.7329\n",
        "prec_at_15: 0.6321\n",
        "\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "Training model at depth 2:\n",
        "EPOCH 0\n",
        "epoch finish in 569.98s, loss: 0.0290\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0673, 0.1360, 0.0783, 0.0994, 0.8788\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3997, 0.7809, 0.4502, 0.5711, 0.9765\n",
        "rec_at_5: 0.3175\n",
        "prec_at_5: 0.8358\n",
        "rec_at_8: 0.4417\n",
        "prec_at_8: 0.7520\n",
        "rec_at_15: 0.6076\n",
        "prec_at_15: 0.5770\n",
        "\n",
        "evaluation finish in 41.62s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 570.95s, loss: 0.0252\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0940, 0.1677, 0.1128, 0.1349, 0.8987\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4446, 0.7701, 0.5127, 0.6156, 0.9807\n",
        "rec_at_5: 0.3286\n",
        "prec_at_5: 0.8585\n",
        "rec_at_8: 0.4622\n",
        "prec_at_8: 0.7793\n",
        "rec_at_15: 0.6370\n",
        "prec_at_15: 0.6045\n",
        "\n",
        "evaluation finish in 42.79s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 2\n",
        "epoch finish in 572.00s, loss: 0.0238\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1071, 0.1893, 0.1278, 0.1526, 0.9076\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4630, 0.7825, 0.5314, 0.6330, 0.9825\n",
        "rec_at_5: 0.3359\n",
        "prec_at_5: 0.8726\n",
        "rec_at_8: 0.4711\n",
        "prec_at_8: 0.7926\n",
        "rec_at_15: 0.6531\n",
        "prec_at_15: 0.6190\n",
        "\n",
        "evaluation finish in 43.98s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 3\n",
        "epoch finish in 575.48s, loss: 0.0228\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1225, 0.2023, 0.1482, 0.1711, 0.9157\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4782, 0.7681, 0.5589, 0.6470, 0.9838\n",
        "rec_at_5: 0.3376\n",
        "prec_at_5: 0.8763\n",
        "rec_at_8: 0.4766\n",
        "prec_at_8: 0.7992\n",
        "rec_at_15: 0.6623\n",
        "prec_at_15: 0.6269\n",
        "\n",
        "evaluation finish in 40.38s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 4\n",
        "epoch finish in 588.87s, loss: 0.0221\n",
        "last epoch: testing on dev and test sets\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1265, 0.2130, 0.1533, 0.1783, 0.9222\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4851, 0.7770, 0.5635, 0.6533, 0.9848\n",
        "rec_at_5: 0.3401\n",
        "prec_at_5: 0.8804\n",
        "rec_at_8: 0.4795\n",
        "prec_at_8: 0.8028\n",
        "rec_at_15: 0.6680\n",
        "prec_at_15: 0.6318\n",
        "\n",
        "evaluation finish in 41.85s\n",
        "file for evaluation: ./data/mimic3/test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1307, 0.2286, 0.1570, 0.1861, 0.9199\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4834, 0.7742, 0.5627, 0.6517, 0.9845\n",
        "rec_at_5: 0.3300\n",
        "prec_at_5: 0.8789\n",
        "rec_at_8: 0.4670\n",
        "prec_at_8: 0.8055\n",
        "rec_at_15: 0.6577\n",
        "prec_at_15: 0.6405\n",
        "\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "Training model at depth 3:\n",
        "EPOCH 0\n",
        "epoch finish in 655.62s, loss: 0.0088\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0287, 0.0577, 0.0345, 0.0432, 0.9113\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3380, 0.7427, 0.3829, 0.5053, 0.9853\n",
        "rec_at_5: 0.2792\n",
        "prec_at_5: 0.7874\n",
        "rec_at_8: 0.3891\n",
        "prec_at_8: 0.7078\n",
        "rec_at_15: 0.5413\n",
        "prec_at_15: 0.5501\n",
        "\n",
        "evaluation finish in 39.69s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 655.70s, loss: 0.0076\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0409, 0.0733, 0.0502, 0.0596, 0.9209\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3815, 0.7320, 0.4434, 0.5522, 0.9869\n",
        "rec_at_5: 0.2914\n",
        "prec_at_5: 0.8179\n",
        "rec_at_8: 0.4068\n",
        "prec_at_8: 0.7370\n",
        "rec_at_15: 0.5681\n",
        "prec_at_15: 0.5773\n",
        "\n",
        "evaluation finish in 39.49s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 2\n",
        "epoch finish in 669.01s, loss: 0.0072\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0460, 0.0800, 0.0559, 0.0658, 0.9268\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3984, 0.7350, 0.4652, 0.5698, 0.9880\n",
        "rec_at_5: 0.2953\n",
        "prec_at_5: 0.8275\n",
        "rec_at_8: 0.4140\n",
        "prec_at_8: 0.7482\n",
        "rec_at_15: 0.5791\n",
        "prec_at_15: 0.5880\n",
        "\n",
        "evaluation finish in 41.74s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 3\n",
        "epoch finish in 660.83s, loss: 0.0070\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0473, 0.0844, 0.0556, 0.0670, 0.9297\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4001, 0.7518, 0.4609, 0.5715, 0.9886\n",
        "rec_at_5: 0.2991\n",
        "prec_at_5: 0.8362\n",
        "rec_at_8: 0.4217\n",
        "prec_at_8: 0.7604\n",
        "rec_at_15: 0.5879\n",
        "prec_at_15: 0.5964\n",
        "\n",
        "evaluation finish in 39.21s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 4\n",
        "epoch finish in 665.44s, loss: 0.0068\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0539, 0.0905, 0.0653, 0.0759, 0.9335\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4132, 0.7398, 0.4835, 0.5848, 0.9891\n",
        "rec_at_5: 0.2998\n",
        "prec_at_5: 0.8374\n",
        "rec_at_8: 0.4240\n",
        "prec_at_8: 0.7641\n",
        "rec_at_15: 0.5917\n",
        "prec_at_15: 0.6001\n",
        "\n",
        "evaluation finish in 39.30s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 5\n",
        "epoch finish in 657.87s, loss: 0.0066\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0548, 0.0955, 0.0642, 0.0768, 0.9353\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4091, 0.7570, 0.4709, 0.5806, 0.9894\n",
        "rec_at_5: 0.3014\n",
        "prec_at_5: 0.8406\n",
        "rec_at_8: 0.4267\n",
        "prec_at_8: 0.7679\n",
        "rec_at_15: 0.5959\n",
        "prec_at_15: 0.6036\n",
        "\n",
        "evaluation finish in 38.75s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 6\n",
        "epoch finish in 663.57s, loss: 0.0065\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0605, 0.1011, 0.0724, 0.0844, 0.9373\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4248, 0.7432, 0.4979, 0.5963, 0.9897\n",
        "rec_at_5: 0.3034\n",
        "prec_at_5: 0.8441\n",
        "rec_at_8: 0.4299\n",
        "prec_at_8: 0.7728\n",
        "rec_at_15: 0.6009\n",
        "prec_at_15: 0.6091\n",
        "\n",
        "evaluation finish in 42.70s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 7\n",
        "epoch finish in 655.75s, loss: 0.0064\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0642, 0.1051, 0.0772, 0.0890, 0.9390\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4284, 0.7364, 0.5060, 0.5998, 0.9900\n",
        "rec_at_5: 0.3031\n",
        "prec_at_5: 0.8454\n",
        "rec_at_8: 0.4305\n",
        "prec_at_8: 0.7732\n",
        "rec_at_15: 0.6041\n",
        "prec_at_15: 0.6121\n",
        "\n",
        "evaluation finish in 40.45s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 8\n",
        "epoch finish in 665.91s, loss: 0.0063\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0648, 0.1064, 0.0774, 0.0896, 0.9399\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4289, 0.7469, 0.5019, 0.6004, 0.9902\n",
        "rec_at_5: 0.3047\n",
        "prec_at_5: 0.8472\n",
        "rec_at_8: 0.4330\n",
        "prec_at_8: 0.7769\n",
        "rec_at_15: 0.6069\n",
        "prec_at_15: 0.6146\n",
        "\n",
        "evaluation finish in 41.79s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 9\n",
        "epoch finish in 672.56s, loss: 0.0062\n",
        "last epoch: testing on dev and test sets\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0662, 0.1086, 0.0785, 0.0911, 0.9406\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4281, 0.7465, 0.5010, 0.5996, 0.9903\n",
        "rec_at_5: 0.3041\n",
        "prec_at_5: 0.8466\n",
        "rec_at_8: 0.4314\n",
        "prec_at_8: 0.7757\n",
        "rec_at_15: 0.6093\n",
        "prec_at_15: 0.6173\n",
        "\n",
        "evaluation finish in 40.50s\n",
        "file for evaluation: ./data/mimic3/test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0713, 0.1257, 0.0857, 0.1019, 0.9408\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4253, 0.7438, 0.4983, 0.5968, 0.9904\n",
        "rec_at_5: 0.2968\n",
        "prec_at_5: 0.8491\n",
        "rec_at_8: 0.4185\n",
        "prec_at_8: 0.7754\n",
        "rec_at_15: 0.5931\n",
        "prec_at_15: 0.6202\n",
        "\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "Training model at depth 4:\n",
        "EPOCH 0\n",
        "epoch finish in 747.65s, loss: 0.0049\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0319, 0.0551, 0.0390, 0.0457, 0.9390\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3474, 0.7078, 0.4055, 0.5156, 0.9892\n",
        "rec_at_5: 0.2804\n",
        "prec_at_5: 0.7982\n",
        "rec_at_8: 0.3892\n",
        "prec_at_8: 0.7136\n",
        "rec_at_15: 0.5413\n",
        "prec_at_15: 0.5577\n",
        "\n",
        "evaluation finish in 45.10s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 741.50s, loss: 0.0045\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0374, 0.0619, 0.0454, 0.0524, 0.9427\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3633, 0.7134, 0.4254, 0.5330, 0.9898\n",
        "rec_at_5: 0.2839\n",
        "prec_at_5: 0.8047\n",
        "rec_at_8: 0.3978\n",
        "prec_at_8: 0.7262\n",
        "rec_at_15: 0.5526\n",
        "prec_at_15: 0.5686\n",
        "\n",
        "evaluation finish in 44.71s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 2\n",
        "epoch finish in 750.85s, loss: 0.0044\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0420, 0.0675, 0.0518, 0.0586, 0.9444\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3744, 0.6990, 0.4463, 0.5448, 0.9902\n",
        "rec_at_5: 0.2877\n",
        "prec_at_5: 0.8132\n",
        "rec_at_8: 0.3984\n",
        "prec_at_8: 0.7292\n",
        "rec_at_15: 0.5559\n",
        "prec_at_15: 0.5731\n",
        "\n",
        "evaluation finish in 45.06s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 3\n",
        "epoch finish in 737.98s, loss: 0.0043\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0419, 0.0683, 0.0505, 0.0581, 0.9452\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3742, 0.7223, 0.4370, 0.5446, 0.9904\n",
        "rec_at_5: 0.2885\n",
        "prec_at_5: 0.8166\n",
        "rec_at_8: 0.4018\n",
        "prec_at_8: 0.7341\n",
        "rec_at_15: 0.5621\n",
        "prec_at_15: 0.5790\n",
        "\n",
        "evaluation finish in 54.14s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 4\n",
        "epoch finish in 744.83s, loss: 0.0042\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0443, 0.0719, 0.0536, 0.0614, 0.9461\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3808, 0.7173, 0.4480, 0.5516, 0.9906\n",
        "rec_at_5: 0.2876\n",
        "prec_at_5: 0.8159\n",
        "rec_at_8: 0.4043\n",
        "prec_at_8: 0.7394\n",
        "rec_at_15: 0.5665\n",
        "prec_at_15: 0.5843\n",
        "\n",
        "evaluation finish in 50.71s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 5\n",
        "epoch finish in 756.30s, loss: 0.0041\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0465, 0.0734, 0.0564, 0.0638, 0.9464\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3870, 0.7097, 0.4598, 0.5581, 0.9906\n",
        "rec_at_5: 0.2900\n",
        "prec_at_5: 0.8206\n",
        "rec_at_8: 0.4039\n",
        "prec_at_8: 0.7397\n",
        "rec_at_15: 0.5668\n",
        "prec_at_15: 0.5843\n",
        "\n",
        "evaluation finish in 45.53s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 6\n",
        "epoch finish in 740.03s, loss: 0.0041\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0470, 0.0758, 0.0563, 0.0646, 0.9472\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3847, 0.7192, 0.4527, 0.5557, 0.9907\n",
        "rec_at_5: 0.2889\n",
        "prec_at_5: 0.8199\n",
        "rec_at_8: 0.4050\n",
        "prec_at_8: 0.7416\n",
        "rec_at_15: 0.5715\n",
        "prec_at_15: 0.5882\n",
        "\n",
        "evaluation finish in 51.47s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 7\n",
        "epoch finish in 746.29s, loss: 0.0040\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0516, 0.0787, 0.0627, 0.0698, 0.9476\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3931, 0.7088, 0.4689, 0.5644, 0.9908\n",
        "rec_at_5: 0.2914\n",
        "prec_at_5: 0.8254\n",
        "rec_at_8: 0.4065\n",
        "prec_at_8: 0.7454\n",
        "rec_at_15: 0.5710\n",
        "prec_at_15: 0.5884\n",
        "\n",
        "evaluation finish in 45.55s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 8\n",
        "epoch finish in 754.57s, loss: 0.0040\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0494, 0.0785, 0.0593, 0.0675, 0.9476\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3887, 0.7188, 0.4585, 0.5598, 0.9908\n",
        "rec_at_5: 0.2895\n",
        "prec_at_5: 0.8189\n",
        "rec_at_8: 0.4063\n",
        "prec_at_8: 0.7437\n",
        "rec_at_15: 0.5726\n",
        "prec_at_15: 0.5897\n",
        "\n",
        "evaluation finish in 44.60s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 9\n",
        "epoch finish in 752.25s, loss: 0.0039\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0534, 0.0821, 0.0652, 0.0727, 0.9483\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3989, 0.7026, 0.4799, 0.5703, 0.9909\n",
        "rec_at_5: 0.2905\n",
        "prec_at_5: 0.8221\n",
        "rec_at_8: 0.4076\n",
        "prec_at_8: 0.7450\n",
        "rec_at_15: 0.5743\n",
        "prec_at_15: 0.5913\n",
        "\n",
        "evaluation finish in 50.71s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 10\n",
        "epoch finish in 761.71s, loss: 0.0039\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0534, 0.0818, 0.0657, 0.0729, 0.9488\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3965, 0.7000, 0.4776, 0.5678, 0.9911\n",
        "rec_at_5: 0.2892\n",
        "prec_at_5: 0.8202\n",
        "rec_at_8: 0.4073\n",
        "prec_at_8: 0.7460\n",
        "rec_at_15: 0.5712\n",
        "prec_at_15: 0.5893\n",
        "\n",
        "evaluation finish in 45.86s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 11\n",
        "epoch finish in 742.25s, loss: 0.0039\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0539, 0.0827, 0.0653, 0.0730, 0.9487\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3981, 0.7053, 0.4776, 0.5695, 0.9910\n",
        "rec_at_5: 0.2909\n",
        "prec_at_5: 0.8250\n",
        "rec_at_8: 0.4070\n",
        "prec_at_8: 0.7450\n",
        "rec_at_15: 0.5729\n",
        "prec_at_15: 0.5909\n",
        "\n",
        "evaluation finish in 45.07s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 12\n",
        "epoch finish in 747.28s, loss: 0.0038\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0571, 0.0866, 0.0697, 0.0772, 0.9483\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4015, 0.6969, 0.4864, 0.5730, 0.9910\n",
        "rec_at_5: 0.2916\n",
        "prec_at_5: 0.8269\n",
        "rec_at_8: 0.4091\n",
        "prec_at_8: 0.7482\n",
        "rec_at_15: 0.5741\n",
        "prec_at_15: 0.5919\n",
        "\n",
        "evaluation finish in 55.47s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 13\n",
        "epoch finish in 748.36s, loss: 0.0038\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0583, 0.0876, 0.0719, 0.0790, 0.9482\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4028, 0.6910, 0.4912, 0.5742, 0.9910\n",
        "rec_at_5: 0.2914\n",
        "prec_at_5: 0.8239\n",
        "rec_at_8: 0.4082\n",
        "prec_at_8: 0.7462\n",
        "rec_at_15: 0.5731\n",
        "prec_at_15: 0.5909\n",
        "\n",
        "evaluation finish in 45.48s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 14\n",
        "epoch finish in 756.94s, loss: 0.0038\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0553, 0.0853, 0.0668, 0.0749, 0.9482\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3940, 0.7091, 0.4700, 0.5653, 0.9909\n",
        "rec_at_5: 0.2908\n",
        "prec_at_5: 0.8250\n",
        "rec_at_8: 0.4074\n",
        "prec_at_8: 0.7456\n",
        "rec_at_15: 0.5749\n",
        "prec_at_15: 0.5926\n",
        "\n",
        "evaluation finish in 47.22s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 15\n",
        "epoch finish in 739.42s, loss: 0.0037\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0563, 0.0873, 0.0684, 0.0767, 0.9480\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3990, 0.7047, 0.4792, 0.5704, 0.9909\n",
        "rec_at_5: 0.2909\n",
        "prec_at_5: 0.8245\n",
        "rec_at_8: 0.4089\n",
        "prec_at_8: 0.7466\n",
        "rec_at_15: 0.5733\n",
        "prec_at_15: 0.5912\n",
        "\n",
        "evaluation finish in 50.21s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 16\n",
        "epoch finish in 753.56s, loss: 0.0037\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0564, 0.0868, 0.0680, 0.0763, 0.9480\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3968, 0.7053, 0.4757, 0.5682, 0.9908\n",
        "rec_at_5: 0.2908\n",
        "prec_at_5: 0.8261\n",
        "rec_at_8: 0.4088\n",
        "prec_at_8: 0.7472\n",
        "rec_at_15: 0.5751\n",
        "prec_at_15: 0.5931\n",
        "\n",
        "evaluation finish in 46.98s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 17\n",
        "epoch finish in 745.43s, loss: 0.0037\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0578, 0.0876, 0.0702, 0.0779, 0.9480\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4005, 0.6949, 0.4860, 0.5719, 0.9908\n",
        "rec_at_5: 0.2899\n",
        "prec_at_5: 0.8244\n",
        "rec_at_8: 0.4085\n",
        "prec_at_8: 0.7472\n",
        "rec_at_15: 0.5744\n",
        "prec_at_15: 0.5920\n",
        "\n",
        "evaluation finish in 46.18s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 18\n",
        "epoch finish in 739.14s, loss: 0.0036\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0595, 0.0888, 0.0727, 0.0800, 0.9479\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4022, 0.6952, 0.4883, 0.5737, 0.9907\n",
        "rec_at_5: 0.2901\n",
        "prec_at_5: 0.8224\n",
        "rec_at_8: 0.4074\n",
        "prec_at_8: 0.7452\n",
        "rec_at_15: 0.5736\n",
        "prec_at_15: 0.5914\n",
        "\n",
        "evaluation finish in 44.83s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 19\n",
        "epoch finish in 747.56s, loss: 0.0036\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0580, 0.0888, 0.0709, 0.0788, 0.9480\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4019, 0.6938, 0.4886, 0.5734, 0.9907\n",
        "rec_at_5: 0.2904\n",
        "prec_at_5: 0.8227\n",
        "rec_at_8: 0.4087\n",
        "prec_at_8: 0.7478\n",
        "rec_at_15: 0.5731\n",
        "prec_at_15: 0.5911\n",
        "\n",
        "evaluation finish in 45.25s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 20\n",
        "epoch finish in 748.43s, loss: 0.0036\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0590, 0.0895, 0.0714, 0.0794, 0.9473\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4016, 0.6992, 0.4855, 0.5730, 0.9907\n",
        "rec_at_5: 0.2890\n",
        "prec_at_5: 0.8186\n",
        "rec_at_8: 0.4082\n",
        "prec_at_8: 0.7465\n",
        "rec_at_15: 0.5739\n",
        "prec_at_15: 0.5921\n",
        "\n",
        "evaluation finish in 44.61s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 21\n",
        "epoch finish in 751.02s, loss: 0.0036\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0616, 0.0917, 0.0755, 0.0828, 0.9472\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4043, 0.6892, 0.4944, 0.5758, 0.9907\n",
        "rec_at_5: 0.2891\n",
        "prec_at_5: 0.8191\n",
        "rec_at_8: 0.4069\n",
        "prec_at_8: 0.7440\n",
        "rec_at_15: 0.5729\n",
        "prec_at_15: 0.5916\n",
        "\n",
        "evaluation finish in 48.28s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "EPOCH 22\n",
        "epoch finish in 756.25s, loss: 0.0035\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0610, 0.0905, 0.0746, 0.0818, 0.9469\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4027, 0.6868, 0.4933, 0.5742, 0.9906\n",
        "rec_at_5: 0.2897\n",
        "prec_at_5: 0.8199\n",
        "rec_at_8: 0.4078\n",
        "prec_at_8: 0.7456\n",
        "rec_at_15: 0.5752\n",
        "prec_at_15: 0.5926\n",
        "\n",
        "evaluation finish in 45.09s\n",
        "saved metrics, params, model to directory ./models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "\n",
        "prec_at_8 hasn't improved in 10 epochs, early stopping...\n",
        "loading pretrained embeddings from ./data/mimic3/processed_full_100.embed\n",
        "adding unk embedding\n",
        "file for evaluation: ./data/mimic3/dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0571, 0.0866, 0.0697, 0.0772, 0.9483\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4015, 0.6969, 0.4864, 0.5730, 0.9910\n",
        "rec_at_5: 0.2916\n",
        "prec_at_5: 0.8269\n",
        "rec_at_8: 0.4091\n",
        "prec_at_8: 0.7482\n",
        "rec_at_15: 0.5741\n",
        "prec_at_15: 0.5919\n",
        "\n",
        "evaluation finish in 43.99s\n",
        "file for evaluation: ./data/mimic3/test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0639, 0.1046, 0.0799, 0.0906, 0.9478\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3955, 0.6912, 0.4804, 0.5669, 0.9907\n",
        "rec_at_5: 0.2804\n",
        "prec_at_5: 0.8226\n",
        "rec_at_8: 0.3954\n",
        "prec_at_8: 0.7498\n",
        "rec_at_15: 0.5592\n",
        "prec_at_15: 0.5970\n",
        "\n",
        "saved metrics, params, model to directory C:\\Users\\test\\UIUC\\HiCu-ICD-UIUC-Evaluation-Private\\models\\MultiResCNN_HierarchicalHyperbolic_Apr_11_01_53_58\n",
        "```"
      ],
      "metadata": {
        "id": "MzirEkDh1HIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Log for RAC with HiCuA\n",
        "\n",
        "The following section contains the log that was procued during the training of RAC with HiCuA on a dedicated machine.\n",
        "\n",
        "```\n",
        "(hicu_env) C:\\Users\\test\\UIUC\\HiCu-ICD-UIUC-Evaluation-Private>runs\\run_rac_hicua.bat\n",
        "Namespace(DATA_DIR='.\\\\data', MAX_LENGTH=4096, MIMIC_2_DIR='./data/mimic2', MIMIC_3_DIR='.\\\\data\\\\mimic3', MODEL_DIR='.\\\\models', Y='full', asl_config='0,0,0', asl_reduction='sum', attn_dim=512, batch_size=16, cat_hyperbolic=False, code_title_filter_size=9, command='python main.py --MODEL_DIR .\\\\models --DATA_DIR .\\\\data --MIMIC_3_DIR .\\\\data\\\\mimic3 --data_path .\\\\data\\\\mimic3\\\\train_full.csv --embed_file .\\\\data\\\\mimic3\\\\processed_full_300.embed --vocab .\\\\data\\\\mimic3\\\\vocab_rac.csv --Y full --model RACReader --batch_size 16 --lr 8e-5 --criterion prec_at_8 --gpu 0 --tune_wordemb --MAX_LENGTH 4096 --num_workers 8 --filter_size 9 --n_epochs 2,3,5,7,500 --decoder HierarchicalHyperbolic --dropout 0.1', conv_layer=1, criterion='prec_at_8', data_path='.\\\\data\\\\mimic3\\\\train_full.csv', decoder='HierarchicalHyperbolic', depth=5, dropout=0.1, embed_file='.\\\\data\\\\mimic3\\\\processed_full_300.embed', filter_size='9', gpu='0', gpu_list=[0], hyperbolic_dim=50, longformer_dir='', loss='BCE', lr=8e-05, lstm_hidden_dim=512, model='RACReader', n_epochs='2,3,5,7,500', num_code_title_tokens=36, num_filter_maps=50, num_workers=8, patience=10, random_seed=1, reader_conv_num=2, reader_trans_num=4, scheduler=0.9, scheduler_patience=5, test_model=None, thres=0.5, trans_ff_dim=1024, tune_wordemb=True, use_ext_emb=False, version='mimic3', vocab='.\\\\data\\\\mimic3\\\\vocab_rac.csv', weight_decay=0)\n",
        "loading lookups...\n",
        "Depth 0: 34\n",
        "Depth 1: 270\n",
        "Depth 2: 1158\n",
        "Depth 3: 5137\n",
        "Depth 4: 8921\n",
        "Training hyperbolic embeddings...\n",
        "loading pretrained embeddings from .\\data\\mimic3\\processed_full_300.embed\n",
        "adding unk embedding\n",
        "RACReader(\n",
        "  (word_rep): WordRep(\n",
        "    (embed): Embedding(51921, 300, padding_idx=0)\n",
        "    (embed_drop): Dropout(p=0.1, inplace=False)\n",
        "  )\n",
        "  (conv): ModuleList(\n",
        "    (conv_1): Conv1d(300, 300, kernel_size=(9,), stride=(1,), padding=(4,))\n",
        "    (conv_2): Conv1d(300, 300, kernel_size=(9,), stride=(1,), padding=(4,))\n",
        "  )\n",
        "  (dropout): Dropout(p=0.1, inplace=False)\n",
        "  (trans): ModuleList(\n",
        "    (trans_1): TransformerEncoderLayer(\n",
        "      (self_attn): MultiheadAttention(\n",
        "        (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
        "      )\n",
        "      (linear1): Linear(in_features=300, out_features=1024, bias=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "      (linear2): Linear(in_features=1024, out_features=300, bias=True)\n",
        "      (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
        "      (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
        "      (dropout1): Dropout(p=0.1, inplace=False)\n",
        "      (dropout2): Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "    (trans_2): TransformerEncoderLayer(\n",
        "      (self_attn): MultiheadAttention(\n",
        "        (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
        "      )\n",
        "      (linear1): Linear(in_features=300, out_features=1024, bias=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "      (linear2): Linear(in_features=1024, out_features=300, bias=True)\n",
        "      (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
        "      (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
        "      (dropout1): Dropout(p=0.1, inplace=False)\n",
        "      (dropout2): Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "    (trans_3): TransformerEncoderLayer(\n",
        "      (self_attn): MultiheadAttention(\n",
        "        (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
        "      )\n",
        "      (linear1): Linear(in_features=300, out_features=1024, bias=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "      (linear2): Linear(in_features=1024, out_features=300, bias=True)\n",
        "      (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
        "      (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
        "      (dropout1): Dropout(p=0.1, inplace=False)\n",
        "      (dropout2): Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "    (trans_4): TransformerEncoderLayer(\n",
        "      (self_attn): MultiheadAttention(\n",
        "        (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
        "      )\n",
        "      (linear1): Linear(in_features=300, out_features=1024, bias=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "      (linear2): Linear(in_features=1024, out_features=300, bias=True)\n",
        "      (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
        "      (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
        "      (dropout1): Dropout(p=0.1, inplace=False)\n",
        "      (dropout2): Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "  )\n",
        "  (decoder): Decoder(\n",
        "    (decoder_dict): ModuleDict(\n",
        "      (0_0): Linear(in_features=300, out_features=34, bias=True)\n",
        "      (0_1): Linear(in_features=300, out_features=34, bias=True)\n",
        "      (1_0): Linear(in_features=300, out_features=270, bias=True)\n",
        "      (1_1): Linear(in_features=300, out_features=270, bias=True)\n",
        "      (2_0): Linear(in_features=300, out_features=1158, bias=True)\n",
        "      (2_1): Linear(in_features=300, out_features=1158, bias=True)\n",
        "      (3_0): Linear(in_features=300, out_features=5137, bias=True)\n",
        "      (3_1): Linear(in_features=300, out_features=5137, bias=True)\n",
        "      (4_0): Linear(in_features=300, out_features=8921, bias=True)\n",
        "      (4_1): Linear(in_features=300, out_features=8921, bias=True)\n",
        "    )\n",
        "    (hyperbolic_fc_dict): ModuleDict(\n",
        "      (0): Linear(in_features=50, out_features=300, bias=True)\n",
        "      (1): Linear(in_features=50, out_features=300, bias=True)\n",
        "      (2): Linear(in_features=50, out_features=300, bias=True)\n",
        "      (3): Linear(in_features=50, out_features=300, bias=True)\n",
        "      (4): Linear(in_features=50, out_features=300, bias=True)\n",
        "    )\n",
        "    (loss_function): BCEWithLogitsLoss()\n",
        "  )\n",
        ")\n",
        "train_instances 47719\n",
        "dev_instances 1631\n",
        "test_instances 3372\n",
        "Total epochs at each level: [2, 3, 5, 7, 500]\n",
        "Training model at depth 0:\n",
        "EPOCH 0\n",
        "C:\\Users\\test\\UIUC\\HiCu-ICD-UIUC-Evaluation-Private\\utils\\train_test.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
        "  inputs_id, labels = torch.LongTensor(inputs_id), torch.FloatTensor(labels[cur_depth])\n",
        "epoch finish in 978.94s, loss: 0.2545\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5256, 0.6663, 0.6420, 0.6539, 0.9073\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.6815, 0.8019, 0.8194, 0.8106, 0.9567\n",
        "rec_at_5: 0.5551\n",
        "prec_at_5: 0.9084\n",
        "rec_at_8: 0.7478\n",
        "prec_at_8: 0.7971\n",
        "rec_at_15: 0.9540\n",
        "prec_at_15: 0.5727\n",
        "\n",
        "evaluation finish in 32.90s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 923.76s, loss: 0.2087\n",
        "last epoch: testing on dev and test sets\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5739, 0.7489, 0.6690, 0.7067, 0.9239\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.7012, 0.8238, 0.8248, 0.8243, 0.9627\n",
        "rec_at_5: 0.5616\n",
        "prec_at_5: 0.9171\n",
        "rec_at_8: 0.7616\n",
        "prec_at_8: 0.8105\n",
        "rec_at_15: 0.9625\n",
        "prec_at_15: 0.5783\n",
        "\n",
        "evaluation finish in 32.14s\n",
        "file for evaluation: .\\data\\mimic3\\test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5620, 0.7498, 0.6570, 0.7003, 0.9143\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.6990, 0.8182, 0.8275, 0.8228, 0.9614\n",
        "rec_at_5: 0.5536\n",
        "prec_at_5: 0.9148\n",
        "rec_at_8: 0.7520\n",
        "prec_at_8: 0.8124\n",
        "rec_at_15: 0.9599\n",
        "prec_at_15: 0.5837\n",
        "\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "Training model at depth 1:\n",
        "EPOCH 0\n",
        "epoch finish in 956.94s, loss: 0.0770\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.2699, 0.3725, 0.3499, 0.3609, 0.9111\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5603, 0.7367, 0.7007, 0.7182, 0.9746\n",
        "rec_at_5: 0.3892\n",
        "prec_at_5: 0.8954\n",
        "rec_at_8: 0.5473\n",
        "prec_at_8: 0.8166\n",
        "rec_at_15: 0.7488\n",
        "prec_at_15: 0.6331\n",
        "\n",
        "evaluation finish in 39.35s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 1020.63s, loss: 0.0622\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3156, 0.4478, 0.3930, 0.4186, 0.9318\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5804, 0.7705, 0.7018, 0.7345, 0.9788\n",
        "rec_at_5: 0.3959\n",
        "prec_at_5: 0.9084\n",
        "rec_at_8: 0.5591\n",
        "prec_at_8: 0.8319\n",
        "rec_at_15: 0.7677\n",
        "prec_at_15: 0.6474\n",
        "\n",
        "evaluation finish in 33.67s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 2\n",
        "epoch finish in 976.22s, loss: 0.0578\n",
        "last epoch: testing on dev and test sets\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3435, 0.4937, 0.4178, 0.4526, 0.9400\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5923, 0.7838, 0.7079, 0.7439, 0.9802\n",
        "rec_at_5: 0.3993\n",
        "prec_at_5: 0.9147\n",
        "rec_at_8: 0.5651\n",
        "prec_at_8: 0.8405\n",
        "rec_at_15: 0.7779\n",
        "prec_at_15: 0.6558\n",
        "\n",
        "evaluation finish in 34.58s\n",
        "file for evaluation: .\\data\\mimic3\\test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3412, 0.4868, 0.4222, 0.4522, 0.9351\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5896, 0.7770, 0.7097, 0.7418, 0.9796\n",
        "rec_at_5: 0.3920\n",
        "prec_at_5: 0.9147\n",
        "rec_at_8: 0.5537\n",
        "prec_at_8: 0.8422\n",
        "rec_at_15: 0.7690\n",
        "prec_at_15: 0.6631\n",
        "\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "Training model at depth 2:\n",
        "EPOCH 0\n",
        "epoch finish in 1148.01s, loss: 0.0227\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1283, 0.1988, 0.1597, 0.1771, 0.9268\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4987, 0.7408, 0.6041, 0.6655, 0.9853\n",
        "rec_at_5: 0.3375\n",
        "prec_at_5: 0.8764\n",
        "rec_at_8: 0.4790\n",
        "prec_at_8: 0.8041\n",
        "rec_at_15: 0.6687\n",
        "prec_at_15: 0.6338\n",
        "\n",
        "evaluation finish in 34.12s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 982.25s, loss: 0.0199\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1642, 0.2365, 0.2206, 0.2283, 0.9352\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5121, 0.6972, 0.6585, 0.6773, 0.9865\n",
        "rec_at_5: 0.3424\n",
        "prec_at_5: 0.8852\n",
        "rec_at_8: 0.4856\n",
        "prec_at_8: 0.8125\n",
        "rec_at_15: 0.6785\n",
        "prec_at_15: 0.6416\n",
        "\n",
        "evaluation finish in 32.70s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 2\n",
        "epoch finish in 988.06s, loss: 0.0187\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1766, 0.2626, 0.2236, 0.2415, 0.9408\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5214, 0.7294, 0.6464, 0.6854, 0.9875\n",
        "rec_at_5: 0.3437\n",
        "prec_at_5: 0.8873\n",
        "rec_at_8: 0.4891\n",
        "prec_at_8: 0.8161\n",
        "rec_at_15: 0.6865\n",
        "prec_at_15: 0.6489\n",
        "\n",
        "evaluation finish in 33.67s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 3\n",
        "epoch finish in 991.88s, loss: 0.0178\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1897, 0.2732, 0.2499, 0.2611, 0.9417\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5160, 0.7182, 0.6470, 0.6807, 0.9874\n",
        "rec_at_5: 0.3449\n",
        "prec_at_5: 0.8889\n",
        "rec_at_8: 0.4880\n",
        "prec_at_8: 0.8148\n",
        "rec_at_15: 0.6859\n",
        "prec_at_15: 0.6468\n",
        "\n",
        "evaluation finish in 39.11s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 4\n",
        "epoch finish in 979.92s, loss: 0.0170\n",
        "last epoch: testing on dev and test sets\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1872, 0.2761, 0.2364, 0.2547, 0.9428\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5176, 0.7311, 0.6393, 0.6821, 0.9878\n",
        "rec_at_5: 0.3427\n",
        "prec_at_5: 0.8852\n",
        "rec_at_8: 0.4903\n",
        "prec_at_8: 0.8178\n",
        "rec_at_15: 0.6886\n",
        "prec_at_15: 0.6495\n",
        "\n",
        "evaluation finish in 32.83s\n",
        "file for evaluation: .\\data\\mimic3\\test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.1982, 0.3049, 0.2509, 0.2753, 0.9406\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.5200, 0.7324, 0.6419, 0.6842, 0.9871\n",
        "rec_at_5: 0.3357\n",
        "prec_at_5: 0.8895\n",
        "rec_at_8: 0.4785\n",
        "prec_at_8: 0.8212\n",
        "rec_at_15: 0.6774\n",
        "prec_at_15: 0.6577\n",
        "\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "Training model at depth 3:\n",
        "EPOCH 0\n",
        "epoch finish in 6331.48s, loss: 0.0069\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0528, 0.0858, 0.0658, 0.0745, 0.9364\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4111, 0.7046, 0.4967, 0.5826, 0.9895\n",
        "rec_at_5: 0.2963\n",
        "prec_at_5: 0.8288\n",
        "rec_at_8: 0.4161\n",
        "prec_at_8: 0.7511\n",
        "rec_at_15: 0.5839\n",
        "prec_at_15: 0.5924\n",
        "\n",
        "evaluation finish in 41.36s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 6226.57s, loss: 0.0059\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0655, 0.1026, 0.0806, 0.0902, 0.9425\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4301, 0.7122, 0.5206, 0.6015, 0.9903\n",
        "rec_at_5: 0.3011\n",
        "prec_at_5: 0.8405\n",
        "rec_at_8: 0.4265\n",
        "prec_at_8: 0.7693\n",
        "rec_at_15: 0.5994\n",
        "prec_at_15: 0.6081\n",
        "\n",
        "evaluation finish in 39.97s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 2\n",
        "epoch finish in 6405.46s, loss: 0.0055\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0759, 0.1119, 0.0950, 0.1028, 0.9435\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4438, 0.6947, 0.5514, 0.6148, 0.9907\n",
        "rec_at_5: 0.3045\n",
        "prec_at_5: 0.8467\n",
        "rec_at_8: 0.4297\n",
        "prec_at_8: 0.7744\n",
        "rec_at_15: 0.6055\n",
        "prec_at_15: 0.6139\n",
        "\n",
        "evaluation finish in 37.67s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 3\n",
        "epoch finish in 6452.93s, loss: 0.0053\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0823, 0.1165, 0.1079, 0.1120, 0.9443\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4417, 0.6724, 0.5628, 0.6128, 0.9905\n",
        "rec_at_5: 0.3030\n",
        "prec_at_5: 0.8438\n",
        "rec_at_8: 0.4275\n",
        "prec_at_8: 0.7694\n",
        "rec_at_15: 0.6013\n",
        "prec_at_15: 0.6102\n",
        "\n",
        "evaluation finish in 39.39s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 4\n",
        "epoch finish in 6462.69s, loss: 0.0050\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0827, 0.1206, 0.1032, 0.1112, 0.9438\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4430, 0.6848, 0.5564, 0.6140, 0.9902\n",
        "rec_at_5: 0.3034\n",
        "prec_at_5: 0.8446\n",
        "rec_at_8: 0.4270\n",
        "prec_at_8: 0.7699\n",
        "rec_at_15: 0.6035\n",
        "prec_at_15: 0.6121\n",
        "\n",
        "evaluation finish in 48.04s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 5\n",
        "epoch finish in 6557.56s, loss: 0.0048\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0858, 0.1257, 0.1072, 0.1157, 0.9426\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4414, 0.6872, 0.5524, 0.6125, 0.9901\n",
        "rec_at_5: 0.3032\n",
        "prec_at_5: 0.8441\n",
        "rec_at_8: 0.4284\n",
        "prec_at_8: 0.7705\n",
        "rec_at_15: 0.6050\n",
        "prec_at_15: 0.6131\n",
        "\n",
        "evaluation finish in 42.70s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 6\n",
        "epoch finish in 6352.14s, loss: 0.0046\n",
        "last epoch: testing on dev and test sets\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0871, 0.1251, 0.1120, 0.1182, 0.9409\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4390, 0.6737, 0.5575, 0.6101, 0.9895\n",
        "rec_at_5: 0.3014\n",
        "prec_at_5: 0.8400\n",
        "rec_at_8: 0.4259\n",
        "prec_at_8: 0.7671\n",
        "rec_at_15: 0.6012\n",
        "prec_at_15: 0.6094\n",
        "\n",
        "evaluation finish in 43.30s\n",
        "file for evaluation: .\\data\\mimic3\\test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0947, 0.1439, 0.1239, 0.1331, 0.9403\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.4360, 0.6708, 0.5546, 0.6072, 0.9893\n",
        "rec_at_5: 0.2926\n",
        "prec_at_5: 0.8361\n",
        "rec_at_8: 0.4140\n",
        "prec_at_8: 0.7681\n",
        "rec_at_15: 0.5868\n",
        "prec_at_15: 0.6141\n",
        "\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "Training model at depth 4:\n",
        "EPOCH 0\n",
        "epoch finish in 32507.22s, loss: 0.0038\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0498, 0.0751, 0.0638, 0.0690, 0.9398\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3730, 0.6493, 0.4671, 0.5433, 0.9894\n",
        "rec_at_5: 0.2783\n",
        "prec_at_5: 0.7929\n",
        "rec_at_8: 0.3894\n",
        "prec_at_8: 0.7158\n",
        "rec_at_15: 0.5444\n",
        "prec_at_15: 0.5609\n",
        "\n",
        "evaluation finish in 46.45s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 1\n",
        "epoch finish in 32863.04s, loss: 0.0033\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0544, 0.0796, 0.0711, 0.0751, 0.9410\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3835, 0.6313, 0.4942, 0.5544, 0.9896\n",
        "rec_at_5: 0.2819\n",
        "prec_at_5: 0.8037\n",
        "rec_at_8: 0.3917\n",
        "prec_at_8: 0.7200\n",
        "rec_at_15: 0.5522\n",
        "prec_at_15: 0.5685\n",
        "\n",
        "evaluation finish in 47.66s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 2\n",
        "epoch finish in 32930.67s, loss: 0.0032\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0536, 0.0809, 0.0675, 0.0736, 0.9418\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3859, 0.6504, 0.4869, 0.5569, 0.9889\n",
        "rec_at_5: 0.2846\n",
        "prec_at_5: 0.8102\n",
        "rec_at_8: 0.3969\n",
        "prec_at_8: 0.7302\n",
        "rec_at_15: 0.5550\n",
        "prec_at_15: 0.5716\n",
        "\n",
        "evaluation finish in 46.72s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 3\n",
        "epoch finish in 32879.88s, loss: 0.0030\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0551, 0.0822, 0.0691, 0.0751, 0.9404\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3842, 0.6526, 0.4829, 0.5551, 0.9887\n",
        "rec_at_5: 0.2812\n",
        "prec_at_5: 0.8004\n",
        "rec_at_8: 0.3940\n",
        "prec_at_8: 0.7246\n",
        "rec_at_15: 0.5522\n",
        "prec_at_15: 0.5704\n",
        "\n",
        "evaluation finish in 49.16s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 4\n",
        "epoch finish in 33229.60s, loss: 0.0029\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0621, 0.0880, 0.0811, 0.0844, 0.9403\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3854, 0.6195, 0.5049, 0.5563, 0.9886\n",
        "rec_at_5: 0.2797\n",
        "prec_at_5: 0.7969\n",
        "rec_at_8: 0.3912\n",
        "prec_at_8: 0.7208\n",
        "rec_at_15: 0.5509\n",
        "prec_at_15: 0.5687\n",
        "\n",
        "evaluation finish in 62.94s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 5\n",
        "epoch finish in 33347.55s, loss: 0.0028\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0617, 0.0881, 0.0788, 0.0832, 0.9403\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3852, 0.6240, 0.5017, 0.5562, 0.9884\n",
        "rec_at_5: 0.2807\n",
        "prec_at_5: 0.7999\n",
        "rec_at_8: 0.3927\n",
        "prec_at_8: 0.7232\n",
        "rec_at_15: 0.5515\n",
        "prec_at_15: 0.5691\n",
        "\n",
        "evaluation finish in 45.99s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 6\n",
        "epoch finish in 33802.53s, loss: 0.0026\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0635, 0.0904, 0.0823, 0.0862, 0.9391\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3867, 0.6072, 0.5156, 0.5577, 0.9878\n",
        "rec_at_5: 0.2818\n",
        "prec_at_5: 0.8010\n",
        "rec_at_8: 0.3947\n",
        "prec_at_8: 0.7256\n",
        "rec_at_15: 0.5491\n",
        "prec_at_15: 0.5673\n",
        "\n",
        "evaluation finish in 60.04s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 7\n",
        "epoch finish in 33593.80s, loss: 0.0025\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0629, 0.0908, 0.0802, 0.0852, 0.9358\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3856, 0.6203, 0.5046, 0.5565, 0.9875\n",
        "rec_at_5: 0.2810\n",
        "prec_at_5: 0.7999\n",
        "rec_at_8: 0.3913\n",
        "prec_at_8: 0.7200\n",
        "rec_at_15: 0.5466\n",
        "prec_at_15: 0.5639\n",
        "\n",
        "evaluation finish in 44.41s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 8\n",
        "epoch finish in 33630.66s, loss: 0.0024\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0643, 0.0912, 0.0824, 0.0866, 0.9345\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3847, 0.6107, 0.5096, 0.5556, 0.9869\n",
        "rec_at_5: 0.2786\n",
        "prec_at_5: 0.7937\n",
        "rec_at_8: 0.3916\n",
        "prec_at_8: 0.7194\n",
        "rec_at_15: 0.5455\n",
        "prec_at_15: 0.5629\n",
        "\n",
        "evaluation finish in 46.21s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 9\n",
        "epoch finish in 33412.16s, loss: 0.0023\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0641, 0.0904, 0.0840, 0.0871, 0.9336\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3791, 0.5950, 0.5110, 0.5498, 0.9870\n",
        "rec_at_5: 0.2769\n",
        "prec_at_5: 0.7892\n",
        "rec_at_8: 0.3884\n",
        "prec_at_8: 0.7147\n",
        "rec_at_15: 0.5409\n",
        "prec_at_15: 0.5592\n",
        "\n",
        "evaluation finish in 46.98s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 10\n",
        "epoch finish in 33208.24s, loss: 0.0022\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0643, 0.0912, 0.0834, 0.0872, 0.9317\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3779, 0.5987, 0.5062, 0.5485, 0.9865\n",
        "rec_at_5: 0.2741\n",
        "prec_at_5: 0.7837\n",
        "rec_at_8: 0.3856\n",
        "prec_at_8: 0.7108\n",
        "rec_at_15: 0.5417\n",
        "prec_at_15: 0.5601\n",
        "\n",
        "evaluation finish in 48.05s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 11\n",
        "epoch finish in 33465.90s, loss: 0.0021\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0635, 0.0908, 0.0825, 0.0865, 0.9308\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3818, 0.6031, 0.5099, 0.5526, 0.9862\n",
        "rec_at_5: 0.2762\n",
        "prec_at_5: 0.7881\n",
        "rec_at_8: 0.3880\n",
        "prec_at_8: 0.7154\n",
        "rec_at_15: 0.5404\n",
        "prec_at_15: 0.5588\n",
        "\n",
        "evaluation finish in 45.63s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "EPOCH 12\n",
        "epoch finish in 33573.37s, loss: 0.0021\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0650, 0.0919, 0.0852, 0.0884, 0.9286\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3781, 0.5864, 0.5157, 0.5488, 0.9857\n",
        "rec_at_5: 0.2740\n",
        "prec_at_5: 0.7814\n",
        "rec_at_8: 0.3834\n",
        "prec_at_8: 0.7064\n",
        "rec_at_15: 0.5388\n",
        "prec_at_15: 0.5573\n",
        "\n",
        "evaluation finish in 47.63s\n",
        "saved metrics, params, model to directory .\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "prec_at_8 hasn't improved in 10 epochs, early stopping...\n",
        "loading pretrained embeddings from .\\data\\mimic3\\processed_full_300.embed\n",
        "adding unk embedding\n",
        "file for evaluation: .\\data\\mimic3\\dev_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0536, 0.0809, 0.0675, 0.0736, 0.9418\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3859, 0.6504, 0.4869, 0.5569, 0.9889\n",
        "rec_at_5: 0.2846\n",
        "prec_at_5: 0.8102\n",
        "rec_at_8: 0.3969\n",
        "prec_at_8: 0.7302\n",
        "rec_at_15: 0.5550\n",
        "prec_at_15: 0.5716\n",
        "\n",
        "evaluation finish in 44.41s\n",
        "file for evaluation: .\\data\\mimic3\\test_full.csv\n",
        "\n",
        "[MACRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.0617, 0.1016, 0.0790, 0.0889, 0.9397\n",
        "[MICRO] accuracy, precision, recall, f-measure, AUC\n",
        "0.3842, 0.6493, 0.4848, 0.5551, 0.9886\n",
        "rec_at_5: 0.2732\n",
        "prec_at_5: 0.7999\n",
        "rec_at_8: 0.3817\n",
        "prec_at_8: 0.7246\n",
        "rec_at_15: 0.5402\n",
        "prec_at_15: 0.5770\n",
        "\n",
        "saved metrics, params, model to directory C:\\Users\\test\\UIUC\\HiCu-ICD-UIUC-Evaluation-Private\\models\\RACReader_HierarchicalHyperbolic_Apr_11_11_04_54\n",
        "\n",
        "Press any key to continue . . .\n",
        "```"
      ],
      "metadata": {
        "id": "O237poL2zYZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Log for LAAT with HiCuA + ASL\n",
        "\n",
        "The following section contains the log that was procued during the training of LAAT with HiCuA + ASL on a dedicated machine.\n",
        "\n",
        "```\n",
        "21:58:27 INFO Training with\n",
        "{   'asl_config': '1,0,0.03',\n",
        "    'asl_reduction': 'sum',\n",
        "    'attention_mode': None,\n",
        "    'batch_size': 8,\n",
        "    'best_model_path': None,\n",
        "    'bidirectional': 1,\n",
        "    'cat_hyperbolic': False,\n",
        "    'checkpoint_dir': 'scratch/gobi2/wren/icd/laat/checkpoints',\n",
        "    'd_a': 256,\n",
        "    'decoder': 'HierarchicalHyperbolic',\n",
        "    'depth': 5,\n",
        "    'disable_attention_linear': False,\n",
        "    'dropout': 0.3,\n",
        "    'embedding_file': 'data/embeddings/word2vec_sg0_100.model',\n",
        "    'embedding_mode': 'word2vec',\n",
        "    'embedding_size': 100,\n",
        "    'hidden_size': 256,\n",
        "    'hyperbolic_dim': 50,\n",
        "    'joint_mode': 'hicu',\n",
        "    'level_projection_size': 128,\n",
        "    'loss': 'ASL',\n",
        "    'lr': 0.0005,\n",
        "    'lr_scheduler_factor': 0.9,\n",
        "    'lr_scheduler_patience': 2,\n",
        "    'main_metric': 'micro_f1',\n",
        "    'max_seq_length': 4000,\n",
        "    'metric_level': -1,\n",
        "    'min_seq_length': -1,\n",
        "    'min_word_frequency': -1,\n",
        "    'mode': 'static',\n",
        "    'model': <class 'src.models.rnn.RNN'>,\n",
        "    'multilabel': 1,\n",
        "    'n_epoch': '1,1,1,1,50',\n",
        "    'n_layers': 1,\n",
        "    'optimiser': 'adamw',\n",
        "    'patience': 6,\n",
        "    'penalisation_coeff': 0.01,\n",
        "    'problem_name': 'mimic-iii_cl_50',\n",
        "    'r': -1,\n",
        "    'resume_training': False,\n",
        "    'rnn_model': 'LSTM',\n",
        "    'save_best_model': 1,\n",
        "    'save_results': 1,\n",
        "    'save_results_on_train': True,\n",
        "    'shuffle_data': 1,\n",
        "    'use_last_hidden_state': 0,\n",
        "    'use_lr_scheduler': 1,\n",
        "    'use_regularisation': False,\n",
        "    'weight_decay': 0}\n",
        "\n",
        "21:58:28 INFO Preparing the vocab\n",
        "21:58:31 INFO Saved vocab and data to files\n",
        "21:58:31 INFO Using cuda\n",
        "21:58:31 INFO # levels: 5\n",
        "21:58:31 INFO # labels at level 0: 14\n",
        "21:58:31 INFO # labels at level 1: 31\n",
        "21:58:31 INFO # labels at level 2: 40\n",
        "21:58:31 INFO # labels at level 3: 48\n",
        "21:58:31 INFO # labels at level 4: 50\n",
        "21:58:31 INFO 8066.1573.1729\n",
        "21:58:37 INFO Saved dataset path: ./scratch/gobi2/wren/icd/laat/cached_data/mimic-iii_cl_50\\8ec84d32fc1beb1e2a7cc1376dd67eda.data.pkl\n",
        "21:58:51 INFO 8066 instances with 12243046 tokens, Level_0 with 14 labels, Level_1 with 31 labels, Level_2 with 40 labels, Level_3 with 48 labels, Level_4 with 50 labels in the train dataset\n",
        "21:58:51 INFO 1573 instances with 2810468 tokens, Level_0 with 14 labels, Level_1 with 31 labels, Level_2 with 40 labels, Level_3 with 48 labels, Level_4 with 50 labels in the valid dataset\n",
        "21:58:51 INFO 1729 instances with 3140441 tokens, Level_0 with 14 labels, Level_1 with 31 labels, Level_2 with 40 labels, Level_3 with 48 labels, Level_4 with 50 labels in the test dataset\n",
        "21:58:51 INFO Training epoch #1\n",
        "22:06:10 INFO Loss on Train at epoch #1: 27.44121, micro_f1 on Train: 0.70305, micro_f1 on Valid: 0.73892\n",
        "22:06:10 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.73892\n",
        "22:06:10 INFO Results on Valid set at epoch #1 with Averaged Loss 26.84183\n",
        "22:06:10 INFO ======== Results at level_0 ========\n",
        "22:06:10 INFO Results on Valid set at epoch #1 with Loss 26.84183:\n",
        "[MICRO]\taccuracy: 0.58595\tauc: 0.90817\tprecision: 0.67211\trecall: 0.82049\tf1: 0.73892\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.52023\tauc: 0.88609\tprecision: 0.6315\trecall: 0.75837\tf1: 0.68914\tP@1: 0.88493\tP@5: 0.62212\tP@8: 0.47155\tP@10: 0.39669\tP@15: 0.29166\n",
        "\n",
        "22:06:10 INFO Training epoch #1\n",
        "22:13:22 INFO Loss on Train at epoch #1: 40.40351, micro_f1 on Train: 0.67288, micro_f1 on Valid: 0.72267\n",
        "22:13:22 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.72267\n",
        "22:13:22 INFO Results on Valid set at epoch #1 with Averaged Loss 39.00168\n",
        "22:13:22 INFO ======== Results at level_1 ========\n",
        "22:13:22 INFO Results on Valid set at epoch #1 with Loss 39.00168:\n",
        "[MICRO]\taccuracy: 0.56577\tauc: 0.93661\tprecision: 0.65436\trecall: 0.80691\tf1: 0.72267\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51364\tauc: 0.90284\tprecision: 0.59461\trecall: 0.74255\tf1: 0.66039\tP@1: 0.8684\tP@5: 0.66039\tP@8: 0.52527\tP@10: 0.45474\tP@15: 0.32901\n",
        "\n",
        "22:13:22 INFO Training epoch #1\n",
        "22:20:37 INFO Loss on Train at epoch #1: 42.8864, micro_f1 on Train: 0.68061, micro_f1 on Valid: 0.72916\n",
        "22:20:37 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.72916\n",
        "22:20:37 INFO Results on Valid set at epoch #1 with Averaged Loss 40.45454\n",
        "22:20:37 INFO ======== Results at level_2 ========\n",
        "22:20:37 INFO Results on Valid set at epoch #1 with Loss 40.45454:\n",
        "[MICRO]\taccuracy: 0.57377\tauc: 0.94111\tprecision: 0.69494\trecall: 0.76693\tf1: 0.72916\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51999\tauc: 0.91843\tprecision: 0.63538\trecall: 0.70132\tf1: 0.66672\tP@1: 0.89002\tP@5: 0.66523\tP@8: 0.53338\tP@10: 0.46395\tP@15: 0.34024\n",
        "\n",
        "22:20:38 INFO Training epoch #1\n",
        "22:27:43 INFO Loss on Train at epoch #1: 46.18624, micro_f1 on Train: 0.67856, micro_f1 on Valid: 0.69207\n",
        "22:27:43 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.69207\n",
        "22:27:43 INFO Results on Valid set at epoch #1 with Averaged Loss 49.73472\n",
        "22:27:43 INFO ======== Results at level_3 ========\n",
        "22:27:43 INFO Results on Valid set at epoch #1 with Loss 49.73472:\n",
        "[MICRO]\taccuracy: 0.52914\tauc: 0.9408\tprecision: 0.60944\trecall: 0.80063\tf1: 0.69207\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.49493\tauc: 0.91999\tprecision: 0.57177\trecall: 0.75627\tf1: 0.6512\tP@1: 0.86713\tP@5: 0.66179\tP@8: 0.53401\tP@10: 0.46618\tP@15: 0.34673\n",
        "\n",
        "22:27:44 INFO Training epoch #1\n",
        "22:34:41 INFO Learning rate at epoch #1: 0.0005\n",
        "22:34:41 INFO Loss on Train at epoch #1: 46.36197, micro_f1 on Train: 0.68409, micro_f1 on Valid: 0.69772\n",
        "22:34:41 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.69772\n",
        "22:34:41 INFO Results on Valid set at epoch #1 with Averaged Loss 48.76607\n",
        "22:34:41 INFO ======== Results at level_4 ========\n",
        "22:34:41 INFO Results on Valid set at epoch #1 with Loss 48.76607:\n",
        "[MICRO]\taccuracy: 0.53576\tauc: 0.94097\tprecision: 0.62818\trecall: 0.78456\tf1: 0.69772\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.50349\tauc: 0.92077\tprecision: 0.60292\trecall: 0.73365\tf1: 0.66189\tP@1: 0.86205\tP@5: 0.66243\tP@8: 0.53163\tP@10: 0.46383\tP@15: 0.34601\n",
        "\n",
        "22:34:41 INFO Training epoch #2\n",
        "22:41:41 INFO Learning rate at epoch #2: 0.0005\n",
        "22:41:41 INFO Loss on Train at epoch #2: 44.51928, micro_f1 on Train: 0.69788, micro_f1 on Valid: 0.70322\n",
        "22:41:41 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.70322\n",
        "22:41:41 INFO Results on Valid set at epoch #2 with Averaged Loss 47.69261\n",
        "22:41:41 INFO ======== Results at level_4 ========\n",
        "22:41:41 INFO Results on Valid set at epoch #2 with Loss 47.69261:\n",
        "[MICRO]\taccuracy: 0.54229\tauc: 0.94012\tprecision: 0.64905\trecall: 0.76727\tf1: 0.70322\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51321\tauc: 0.92077\tprecision: 0.61885\trecall: 0.72129\tf1: 0.66616\tP@1: 0.87095\tP@5: 0.65887\tP@8: 0.53115\tP@10: 0.46389\tP@15: 0.34444\n",
        "\n",
        "22:41:41 INFO Training epoch #3\n",
        "22:48:38 INFO Learning rate at epoch #3: 0.0005\n",
        "22:48:38 INFO Loss on Train at epoch #3: 43.10817, micro_f1 on Train: 0.70852, micro_f1 on Valid: 0.69513\n",
        "22:48:38 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.70322\n",
        "22:48:38 INFO Early stopping: 1/7\n",
        "22:48:38 INFO Training epoch #4\n",
        "22:55:37 INFO Learning rate at epoch #4: 0.0005\n",
        "22:55:37 INFO Loss on Train at epoch #4: 42.25577, micro_f1 on Train: 0.715, micro_f1 on Valid: 0.71076\n",
        "22:55:37 INFO [NEW BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "22:55:37 INFO Results on Valid set at epoch #4 with Averaged Loss 46.20484\n",
        "22:55:37 INFO ======== Results at level_4 ========\n",
        "22:55:37 INFO Results on Valid set at epoch #4 with Loss 46.20484:\n",
        "[MICRO]\taccuracy: 0.55131\tauc: 0.94287\tprecision: 0.67095\trecall: 0.75559\tf1: 0.71076\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51236\tauc: 0.92324\tprecision: 0.63294\trecall: 0.70965\tf1: 0.6691\tP@1: 0.87794\tP@5: 0.66332\tP@8: 0.53592\tP@10: 0.4658\tP@15: 0.34647\n",
        "\n",
        "22:55:37 INFO Training epoch #5\n",
        "23:02:33 INFO Learning rate at epoch #5: 0.0005\n",
        "23:02:33 INFO Loss on Train at epoch #5: 41.02574, micro_f1 on Train: 0.72367, micro_f1 on Valid: 0.69686\n",
        "23:02:33 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:02:33 INFO Early stopping: 1/7\n",
        "23:02:33 INFO Training epoch #6\n",
        "23:09:31 INFO Learning rate at epoch #6: 0.0005\n",
        "23:09:31 INFO Loss on Train at epoch #6: 40.36062, micro_f1 on Train: 0.72864, micro_f1 on Valid: 0.70297\n",
        "23:09:31 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:09:31 INFO Early stopping: 2/7\n",
        "23:09:31 INFO Training epoch #7\n",
        "23:16:53 INFO Learning rate at epoch #7: 0.00045000000000000004\n",
        "23:16:53 INFO Loss on Train at epoch #7: 39.60688, micro_f1 on Train: 0.73457, micro_f1 on Valid: 0.70931\n",
        "23:16:53 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:16:53 INFO Early stopping: 3/7\n",
        "23:16:53 INFO Training epoch #8\n",
        "23:24:17 INFO Learning rate at epoch #8: 0.00045000000000000004\n",
        "23:24:17 INFO Loss on Train at epoch #8: 38.57417, micro_f1 on Train: 0.74247, micro_f1 on Valid: 0.70582\n",
        "23:24:17 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:24:17 INFO Early stopping: 4/7\n",
        "23:24:17 INFO Training epoch #9\n",
        "23:31:42 INFO Learning rate at epoch #9: 0.00045000000000000004\n",
        "23:31:42 INFO Loss on Train at epoch #9: 37.75087, micro_f1 on Train: 0.74752, micro_f1 on Valid: 0.69845\n",
        "23:31:42 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:31:42 INFO Early stopping: 5/7\n",
        "23:31:43 INFO Training epoch #10\n",
        "23:39:01 INFO Learning rate at epoch #10: 0.00040500000000000003\n",
        "23:39:01 INFO Loss on Train at epoch #10: 37.06155, micro_f1 on Train: 0.75229, micro_f1 on Valid: 0.69836\n",
        "23:39:01 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:39:01 INFO Early stopping: 6/7\n",
        "23:39:01 INFO Training epoch #11\n",
        "23:46:21 INFO Learning rate at epoch #11: 0.00040500000000000003\n",
        "23:46:21 INFO Loss on Train at epoch #11: 36.27539, micro_f1 on Train: 0.75856, micro_f1 on Valid: 0.70597\n",
        "23:46:21 INFO [CURRENT BEST] (average) micro_f1 on Valid set: 0.71076\n",
        "23:46:21 INFO Early stopping: 7/7\n",
        "23:46:21 WARNING Early stopped on Valid set!\n",
        "23:46:21 INFO =================== BEST ===================\n",
        "23:46:21 INFO Results on Valid set at epoch #4 with Averaged Loss 46.20484\n",
        "23:46:21 INFO ======== Results at level_4 ========\n",
        "23:46:21 INFO Results on Valid set at epoch #4 with Loss 46.20484:\n",
        "[MICRO]\taccuracy: 0.55131\tauc: 0.94287\tprecision: 0.67095\trecall: 0.75559\tf1: 0.71076\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.51236\tauc: 0.92324\tprecision: 0.63294\trecall: 0.70965\tf1: 0.6691\tP@1: 0.87794\tP@5: 0.66332\tP@8: 0.53592\tP@10: 0.4658\tP@15: 0.34647\n",
        "\n",
        "23:46:21 INFO Results on Test set at epoch #4 with Averaged Loss 47.49478\n",
        "23:46:21 INFO ======== Results at level_4 ========\n",
        "23:46:21 INFO Results on Test set at epoch #4 with Loss 47.49478:\n",
        "[MICRO]\taccuracy: 0.54847\tauc: 0.94327\tprecision: 0.66535\trecall: 0.75742\tf1: 0.70841\tP@1: 0\tP@5: 0\tP@8: 0\tP@10: 0\tP@15: 0\n",
        "[MACRO]\taccuracy: 0.50974\tauc: 0.92221\tprecision: 0.62568\trecall: 0.71375\tf1: 0.66682\tP@1: 0.87449\tP@5: 0.66987\tP@8: 0.54085\tP@10: 0.4749\tP@15: 0.35624\n",
        "\n",
        "23:46:21 INFO => loading best model 'scratch/gobi2/wren/icd/laat/checkpoints/mimic-iii_cl_50/RNN_LSTM_1_256.static.None.0.0005.0.3_269fb573470a421e9d4f0a15fc82d7d7/best_model.pkl'\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "h_Jo4vrd1bHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "## Metrics Descriptions:\n",
        "- **AUC (Area Under the Curve):** Utilized in both micro-averaged and macro-averaged forms, this metric measures the overall prediction performance across all labels.\n",
        "- **F1 Score:** Reported in both micro-averaged and macro-averaged forms, it indicates the balance between precision and recall.\n",
        "- **Precision@K (P@K):** This metric assesses the proportion of correctly predicted labels in the top-K predictions, essential for practical applications where only the top few predictions may be considered.\n",
        "- **Precision@5 (P@5):** Measures the proportion of relevant labels in the top 5 predictions.\n",
        "- **Precision@8 (P@8):** Measures the proportion of relevant labels in the top 8 predictions.\n",
        "- **Precision@15 (P@15):** Measures the proportion of relevant labels in the top 15 predictions.\n",
        "\n",
        "## Performance Results:\n",
        "- The HiCu method was tested on several model architectures, showing improvements in AUC and F1 scores over baseline models without curriculum learning.\n",
        "- Notable enhancements were particularly evident for rare labels, addressing the challenge of imbalanced datasets prevalent in medical coding.\n",
        "- An asymmetric loss function was employed to handle label imbalance more effectively, leading to superior performance on rare and infrequent labels.\n",
        "- Extensive testing was conducted on the MIMIC-III dataset using ICD-9 codes, establishing the method's effectiveness on a standard dataset for medical coding research.\n",
        "\n",
        "\n",
        "## Implementation of Evaluation Code\n",
        "\n",
        "**Implementation of Metrics: AUC, F1, and Precision@K  You could refer to the Demo section of `Getting Project Setup` for an example of a runnable code. Please note that you need to follow all the sequences specified in that section in order to execute the code.**\n",
        "\n",
        "```\n",
        "def print_metrics(metrics):\n",
        "    print()\n",
        "    if \"auc_macro\" in metrics.keys():\n",
        "        print(\"[MACRO] accuracy, precision, recall, f-measure, AUC\")\n",
        "        print(\"%.4f, %.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_macro\"], metrics[\"prec_macro\"], metrics[\"rec_macro\"], metrics[\"f1_macro\"], metrics[\"auc_macro\"]))\n",
        "    else:\n",
        "        print(\"[MACRO] accuracy, precision, recall, f-measure\")\n",
        "        print(\"%.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_macro\"], metrics[\"prec_macro\"], metrics[\"rec_macro\"], metrics[\"f1_macro\"]))\n",
        "\n",
        "    if \"auc_micro\" in metrics.keys():\n",
        "        print(\"[MICRO] accuracy, precision, recall, f-measure, AUC\")\n",
        "        print(\"%.4f, %.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_micro\"], metrics[\"prec_micro\"], metrics[\"rec_micro\"], metrics[\"f1_micro\"], metrics[\"auc_micro\"]))\n",
        "    else:\n",
        "        print(\"[MICRO] accuracy, precision, recall, f-measure\")\n",
        "        print(\"%.4f, %.4f, %.4f, %.4f\" % (metrics[\"acc_micro\"], metrics[\"prec_micro\"], metrics[\"rec_micro\"], metrics[\"f1_micro\"]))\n",
        "    for metric, val in metrics.items():\n",
        "        if metric.find(\"rec_at\") != -1:\n",
        "            print(\"%s: %.4f\" % (metric, val))\n",
        "    print()\n",
        "\n",
        "def union_size(yhat, y, axis):\n",
        "    #axis=0 for label-level union (macro). axis=1 for instance-level\n",
        "    return np.logical_or(yhat, y).sum(axis=axis).astype(float)\n",
        "\n",
        "def intersect_size(yhat, y, axis):\n",
        "    #axis=0 for label-level union (macro). axis=1 for instance-level\n",
        "    return np.logical_and(yhat, y).sum(axis=axis).astype(float)\n",
        "\n",
        "def macro_accuracy(yhat, y):\n",
        "    num = intersect_size(yhat, y, 0) / (union_size(yhat, y, 0) + 1e-10)\n",
        "    return np.mean(num)\n",
        "\n",
        "def macro_precision(yhat, y):\n",
        "    num = intersect_size(yhat, y, 0) / (yhat.sum(axis=0) + 1e-10)\n",
        "    return np.mean(num)\n",
        "\n",
        "def macro_recall(yhat, y):\n",
        "    num = intersect_size(yhat, y, 0) / (y.sum(axis=0) + 1e-10)\n",
        "    return np.mean(num)\n",
        "\n",
        "def macro_f1(yhat, y):\n",
        "    prec = macro_precision(yhat, y)\n",
        "    rec = macro_recall(yhat, y)\n",
        "    if prec + rec == 0:\n",
        "        f1 = 0.\n",
        "    else:\n",
        "        f1 = 2*(prec*rec)/(prec+rec)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def all_macro(yhat, y):\n",
        "    return macro_accuracy(yhat, y), macro_precision(yhat, y), macro_recall(yhat, y), macro_f1(yhat, y)\n",
        "\n",
        "def micro_accuracy(yhatmic, ymic):\n",
        "    return intersect_size(yhatmic, ymic, 0) / (union_size(yhatmic, ymic, 0) + 1e-10)\n",
        "\n",
        "def micro_precision(yhatmic, ymic):\n",
        "    return intersect_size(yhatmic, ymic, 0) / (yhatmic.sum(axis=0) + 1e-10)\n",
        "\n",
        "def micro_recall(yhatmic, ymic):\n",
        "    return intersect_size(yhatmic, ymic, 0) / (ymic.sum(axis=0) + 1e-10)\n",
        "\n",
        "def micro_f1(yhatmic, ymic):\n",
        "    prec = micro_precision(yhatmic, ymic)\n",
        "    rec = micro_recall(yhatmic, ymic)\n",
        "    if prec + rec == 0:\n",
        "        f1 = 0.\n",
        "    else:\n",
        "        f1 = 2 * (prec * rec) / (prec + rec)\n",
        "    return f1\n",
        "\n",
        "def all_micro(yhatmic, ymic):\n",
        "    return micro_accuracy(yhatmic, ymic), micro_precision(yhatmic, ymic), micro_recall(yhatmic, ymic), micro_f1(yhatmic, ymic)\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "def auc_metrics(yhat_raw, y, ymic):\n",
        "    if yhat_raw.shape[0] <= 1:\n",
        "        return\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    roc_auc = {}\n",
        "    #get AUC for each label individually\n",
        "    relevant_labels = []\n",
        "    auc_labels = {}\n",
        "    for i in range(y.shape[1]):\n",
        "        #only if there are true positives for this label\n",
        "        if y[:,i].sum() > 0:\n",
        "            fpr[i], tpr[i], _ = roc_curve(y[:,i], yhat_raw[:,i])\n",
        "            if len(fpr[i]) > 1 and len(tpr[i]) > 1:\n",
        "                auc_score = auc(fpr[i], tpr[i])\n",
        "                if not np.isnan(auc_score):\n",
        "                    auc_labels[\"auc_%d\" % i] = auc_score\n",
        "                    relevant_labels.append(i)\n",
        "\n",
        "    #macro-AUC: just average the auc scores\n",
        "    aucs = []\n",
        "    for i in relevant_labels:\n",
        "        aucs.append(auc_labels['auc_%d' % i])\n",
        "    roc_auc['auc_macro'] = np.mean(aucs)\n",
        "\n",
        "    #micro-AUC: just look at each individual prediction\n",
        "    yhatmic = yhat_raw.ravel()\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(ymic, yhatmic)\n",
        "    roc_auc[\"auc_micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "def recall_at_k(yhat_raw, y, k):\n",
        "    #num true labels in top k predictions / num true labels\n",
        "    sortd = np.argsort(yhat_raw)[:,::-1]\n",
        "    topk = sortd[:,:k]\n",
        "\n",
        "    #get recall at k for each example\n",
        "    vals = []\n",
        "    for i, tk in enumerate(topk):\n",
        "        num_true_in_top_k = y[i,tk].sum()\n",
        "        denom = y[i,:].sum()\n",
        "        vals.append(num_true_in_top_k / float(denom))\n",
        "\n",
        "    vals = np.array(vals)\n",
        "    vals[np.isnan(vals)] = 0.\n",
        "\n",
        "    return np.mean(vals)\n",
        "\n",
        "def precision_at_k(yhat_raw, y, k):\n",
        "    #num true labels in top k predictions / k\n",
        "    sortd = np.argsort(yhat_raw)[:,::-1]\n",
        "    topk = sortd[:,:k]\n",
        "\n",
        "    #get precision at k for each example\n",
        "    vals = []\n",
        "    for i, tk in enumerate(topk):\n",
        "        if len(tk) > 0:\n",
        "            num_true_in_top_k = y[i,tk].sum()\n",
        "            denom = len(tk)\n",
        "            vals.append(num_true_in_top_k / float(denom))\n",
        "\n",
        "    return np.mean(vals)\n",
        "\n",
        "def all_metrics(yhat, y, k=8, yhat_raw=None, calc_auc=True):\n",
        "    \"\"\"\n",
        "        Inputs:\n",
        "            yhat: binary predictions matrix\n",
        "            y: binary ground truth matrix\n",
        "            k: for @k metrics\n",
        "            yhat_raw: prediction scores matrix (floats)\n",
        "        Outputs:\n",
        "            dict holding relevant metrics\n",
        "    \"\"\"\n",
        "    names = [\"acc\", \"prec\", \"rec\", \"f1\"]\n",
        "\n",
        "    #macro\n",
        "    macro = all_macro(yhat, y)\n",
        "\n",
        "    #micro\n",
        "    ymic = y.ravel()\n",
        "    yhatmic = yhat.ravel()\n",
        "    micro = all_micro(yhatmic, ymic)\n",
        "\n",
        "    metrics = {names[i] + \"_macro\": macro[i] for i in range(len(macro))}\n",
        "    metrics.update({names[i] + \"_micro\": micro[i] for i in range(len(micro))})\n",
        "\n",
        "    #AUC and @k\n",
        "    if yhat_raw is not None and calc_auc:\n",
        "        #allow k to be passed as int or list\n",
        "        if type(k) != list:\n",
        "            k = [k]\n",
        "        for k_i in k:\n",
        "            rec_at_k = recall_at_k(yhat_raw, y, k_i)\n",
        "            metrics['rec_at_%d' % k_i] = rec_at_k\n",
        "            prec_at_k = precision_at_k(yhat_raw, y, k_i)\n",
        "            metrics['prec_at_%d' % k_i] = prec_at_k\n",
        "            metrics['f1_at_%d' % k_i] = 2*(prec_at_k*rec_at_k)/(prec_at_k+rec_at_k)\n",
        "\n",
        "        roc_auc = auc_metrics(yhat_raw, y, ymic)\n",
        "        metrics.update(roc_auc)\n",
        "\n",
        "    return metrics\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "E8tGHVz-yag5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "## Table of Results\n",
        "\n",
        "The results of the hypothesis: `MultiResCNN with HiCuA`, `RAC with HiCuA`, and `LAAT with HiCuA + ASL` are individually compared against the results from the original paper. Please refer to each individual execution log in the `Training` section for specific experiment claims. I will include the Training Epoch Number for each experiment for your reference.\n",
        "\n",
        "### MultiResCNN with HiCuA\n",
        "\n",
        "**Please refer to EPOCH 22 for: `Training Log for MultiResCNN with HiCuA`**\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Model</th>\n",
        "    <th>Source</th>\n",
        "    <th>Macro AUC</th>\n",
        "    <th>Micro AUC</th>\n",
        "    <th>Macro F1</th>\n",
        "    <th>Micro F1</th>\n",
        "    <th>P@5</th>\n",
        "    <th>P@8</th>\n",
        "    <th>P@15</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>MultiResCNN w/ HiCuA</td>\n",
        "    <td>My Experiment at Depth 4, Epoch 22 (Best)</td>\n",
        "    <td>0.9478</td>\n",
        "    <td>0.9907</td>\n",
        "    <td>0.0906</td>\n",
        "    <td>0.5669</td>\n",
        "    <td>0.8226</td>\n",
        "    <td>0.7498</td>\n",
        "    <td>0.5970</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>MultiResCNN w/ HiCuA</td>\n",
        "    <td>HiCu Paper</td>\n",
        "    <td>0.9470</td>\n",
        "    <td>0.9910</td>\n",
        "    <td>0.0920</td>\n",
        "    <td>0.5670</td>\n",
        "    <td>0.8200</td>\n",
        "    <td>0.7480</td>\n",
        "    <td>0.5960</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "**All results are on the MIMIC-III full code test set.**\n",
        "\n",
        "**Key observations:**\n",
        "1. My `Macro AUC` (0.9478) is slightly higher than the paper's result of 0.9470, and my `Micro F1` (0.5669) is marginally lower compared to the paper's 0.5670. Both metrics fall well within the acceptable margins defined by the paper, which are 0.10 for `Macro AUC` and 0.29 for `Micro F1`.\n",
        "2. My `Micro AUC` (0.9907) is slightly lower than the paper's result of 0.9910, which is within the acceptable margin of 0.02 as defined by the paper. Additionally, my `Macro F1` (0.0906) is slightly lower, compared to the paper's result of 0.0920, but this difference is within the substantial margin of 0.33 as set by the study.\n",
        "3. My `P@5` (0.8226) slightly exceeds the paper's result of 0.82, remaining well within the acceptable margin of 0.14.\n",
        "4. My `P@8` (0.7498) is slightly higher than the paper's 0.748, also fitting comfortably within the margin of 0.16.\n",
        "5. My `P@15` (0.5970) is very close to the paper's 0.596, staying within the tight margin of 0.07.\"\n",
        "\n",
        "Overall, the replication confirms the robustness and reproducibility of the **MultiResCNN with HiCuA** model as described in the original study. The slight variations observed are within the expected ranges due to factors like random initialization and differences in computational environments, reinforcing the validity of the model's capabilities.\n",
        "\n",
        "### RAC with HiCuA\n",
        "\n",
        "**Please refer to EPOCH 12 for: `Training Log for RAC with HiCuA`**\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Model</th>\n",
        "    <th>Source</th>\n",
        "    <th>Macro AUC</th>\n",
        "    <th>Micro AUC</th>\n",
        "    <th>Macro F1</th>\n",
        "    <th>Micro F1</th>\n",
        "    <th>P@5</th>\n",
        "    <th>P@8</th>\n",
        "    <th>P@15</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RAC w/ HiCuA</td>\n",
        "    <td>My Experiment at Depth 4, Epoch 12 (Best)</td>\n",
        "    <td>0.9397</td>\n",
        "    <td>0.9886</td>\n",
        "    <td>0.0889</td>\n",
        "    <td>0.5551</td>\n",
        "    <td>0.7999</td>\n",
        "    <td>0.7246</td>\n",
        "    <td>0.5770</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>RAC w/ HiCuA</td>\n",
        "    <td>HiCu Paper</td>\n",
        "    <td>0.9430</td>\n",
        "    <td>0.9900</td>\n",
        "    <td>0.0840</td>\n",
        "    <td>0.5650</td>\n",
        "    <td>0.8120</td>\n",
        "    <td>0.7380</td>\n",
        "    <td>0.5880</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "**All results are on the MIMIC-III full code test set.**\n",
        "\n",
        "**Key observations:**\n",
        "\n",
        "1. My `Macro AUC` (0.9397) is slightly lower than the paper's result of 0.9430, with a difference of 0.0033, which is within the acceptable margin of 0.09. Similarly, my `Micro F1` (0.5551) is lower compared to the paper's 0.5650, with a difference of 0.0099, also comfortably within the acceptable margin of 0.17.\n",
        "2. My `Micro AUC` (0.9886) is slightly lower than the paper's result of 0.9900, with a difference of 0.0014, which is within the acceptable margin of 0.01. Similarly, my `Macro F1` (0.0889) is higher compared to the paper's result of 0.0840, with a difference of 0.0049, also comfortably within the acceptable margin of 0.17.\n",
        "3. My `P@5` of 0.7999 is slightly lower than the paper's result of 0.8120. However, with a difference of 0.0121, this is well within the acceptable margin of 0.32.\n",
        "4. My `P@8` of 0.7246 is also lower than the paper's result of 0.7380, but the difference of 0.0134 falls within the acceptable margin of 0.17\n",
        "5. My `P@15` of 0.5770 is lower compared to the paper's result of 0.5880. With a difference of 0.0110, this too is within the acceptable margin of 0.12.\n",
        "\n",
        "Overall, the replication confirms the robustness and reproducibility of the **RAC with HiCuA** model as described in the original study. The slight variations observed are within the expected ranges due to factors like random initialization and differences in computational environments, reinforcing the validity of the model's capabilities.\n",
        "\n",
        "### LAAT with HiCuA + ASL\n",
        "\n",
        "**Please refer to EPOCH 4 for: `Training Log for LATT with HiCuA + ASL`**\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<th>Model</th>\n",
        "<th>Source</th>\n",
        "<th>Macro AUC</th>\n",
        "<th>Micro AUC</th>\n",
        "<th>Macro F1</th>\n",
        "<th>Micro F1</th>\n",
        "<th>P@5</th>\n",
        "<th>P@8</th>\n",
        "<th>P@15</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>LAAT w/ HiCuA+ASL</td>\n",
        "<td>My Experiment at Depth 4, Epoch 4 (Best)</td>\n",
        "<td>0.9222</td>\n",
        "<td>0.9433</td>\n",
        "<td>0.6668</td>\n",
        "<td>0.7084</td>\n",
        "<td>0.6670</td>\n",
        "<td>0.5408</td>\n",
        "<td>0.5408</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>LAAT w/ HiCuA+ASL</td>\n",
        "<td>HiCu Paper</td>\n",
        "<td>0.9210</td>\n",
        "<td>0.9420</td>\n",
        "<td>0.6640</td>\n",
        "<td>0.7090</td>\n",
        "<td>0.6690</td>\n",
        "<td>-</td>\n",
        "<td>-</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "**Key observations:**\n",
        "\n",
        "Comparing my results with the HiCu paper **MIMIC-III 50 Code Results**\n",
        "\n",
        "1. My `Macro AUC` (0.9222) is slightly higher than the paper's result of 0.9210, with a difference of 0.0012, which is within the acceptable margin of 0.14. Similarly, my `Micro F1` (0.7084) is slightly lower compared to the paper's 0.7090, with a difference of 0.0006, also comfortably within the acceptable margin of 0.26.\n",
        "2. My `Micro AUC` (0.9433) is slightly higher than the paper's result of 0.9420, with a difference of 0.0013, which is within the acceptable margin of 0.06. Similarly, my `Macro F1` (0.6668) is higher compared to the paper's 0.6640, with a difference of 0.0028, also comfortably within the acceptable margin of 0.37.\n",
        "3. My `P@5` of 0.6670 is slightly lower than the paper's `P@5` of 0.6690, with a difference of 0.0020, which is comfortably within the acceptable margin of 0.12. As for `P@8` and `P@15`, both are reported at 0.5408, but since the paper did not report values or margins for these metrics for the MIMIC-III 50 Code Results, direct comparisons cannot be made.\n",
        "\n",
        "Overall, the replication confirms the robustness and reproducibility of the **LAAT with HiCuA + ASL** model as described in the original study. The slight variations observed are within the expected ranges due to factors like random initialization and differences in computational environments, reinforcing the validity of the model's capabilities.\n",
        "\n",
        "## Comparison to the Results from the Original Paper\n",
        "\n",
        "The reproducibility results largely align with the findings from the original paper:\n",
        "\n",
        "1. **MultiResCNN with HiCuA:** This model's performance metrics are closely aligned with the paper's, albeit slightly varied within acceptable margins:\n",
        "- **Macro AUC (0.9478)** is slightly higher than the paper's **(0.9470)**.\n",
        "- **Micro AUC (0.9907)** is marginally lower than the paper's **(0.9910)**.\n",
        "- **Macro F1 (0.0906)** is slightly lower than the paper's **(0.0920)**, reflecting a marginal deviation.\n",
        "- **Micro F1 (0.5669)** is nearly identical to the paper's **(0.5670)**.\n",
        "- **Precision at K metric:**\n",
        "  - **P@5 (0.8226)** is higher compared to the paper's **(0.8200)**.\n",
        "  - **P@8 (0.7498)** is very close to the paper's **(0.7480)**.\n",
        "  - **P@15 (0.5970)** is slightly higher than the paper's **(0.5960)**.\n",
        "\n",
        "2. **RAC with HiCuA:** This model exhibits some variability compared to the paper, but still within an acceptable range:\n",
        "- **Macro AUC (0.9397)** is slightly lower than the paper's **(0.9430)**.\n",
        "- **Micro AUC (0.9886)** is also slightly lower than the paper's **(0.9900)**.\n",
        "- **Macro F1 (0.0889)** is higher than the paper's **(0.0840)**.\n",
        "- **Micro F1 (0.5551)** is slightly lower than the paper's **(0.5650)**.\n",
        "- **Precision at K metrics:**\n",
        "  - **P@5 (0.7999)** is lower than the paper's **(0.8120)**.\n",
        "  - **P@8 (0.7246)** is also lower than the paper's **(0.7380)**.\n",
        "  - **P@15 (0.5770)** is lower than the paper's **(0.5880)**.\n",
        "\n",
        "3. **LAAT with HiCuA + ASL:** This model's results closely match the paper's findings:\n",
        "- **Macro AUC (0.9222)** is slightly higher than the paper's **(0.9210)**.\n",
        "- **Micro AUC (0.9433)** is also slightly higher than the paper's **(0.9420)**.\n",
        "- **Macro F1 (0.6668)** is higher than the paper's **(0.6640)**.\n",
        "- **Micro F1 (0.7084)** is very close to the paper's **(0.7090)**.\n",
        "- **Precision at K metrics:**\n",
        "  - **P@5 (0.6670)** is nearly identical to the paper's **(0.6690)**.\n",
        "  - The paper does not report **P@8** and **P@15**.\n",
        "\n",
        "**Conclusion**:\n",
        "Overall, these comparisons affirm that the original paper's conclusions are reproducible across different configurations and datasets. While minor differences are noted across models, they fall within the expected range due to factors such as random initialization and differences in computational environments. The overall consistency underlines the robustness of the original research findings.\n",
        "\n",
        "## Experiments Beyond the Original Paper\n",
        "\n",
        "### MultiResCNN with HiCuA: Exploring the Impact of Diverse Convolutional Filter Sizes\n",
        "\n",
        "To evaluate how different convolutional filter sizes affect model performance, particularly in terms of feature extraction from clinical texts.\n",
        "\n",
        "**Method:** The model employs a diverse set of filter sizes `(3, 5, 9, 15, 19, 25)`, and this experiment analyzes the impact of each filter size on the model's ability to capture relevant features effectively.\n",
        "\n",
        "**Results:**\n",
        "- Smaller filters (sizes `3` and `5`) excel at capturing fine-grained details.\n",
        "- Mid-range filters (sizes `9` and `15`) provide the best balance between detail and context, optimizing both precision and recall.\n",
        "- Larger filters (sizes `19` and `25`) help capture broader contextual information but may lead to some over-generalization.\n",
        "\n",
        "This setup allows the model to be versatile in handling a variety of text complexities found in clinical notes. The range of filter sizes ensures that the model can adapt to different levels of textual detail, enhancing its overall effectiveness in medical text analysis.\n",
        "\n",
        "### LAAT with HiCuA + ASL: Expanded Hyperparameter Tuning\n",
        "\n",
        "To explore the impact of varying hidden layer sizes and learning rates.\n",
        "\n",
        "**Method:** Tested hidden sizes of `128`, `256`, and `512` and learning rates of `0.0001`, `0.0005`, and `0.001`.\n",
        "**Results:**\n",
        "Hidden size of `256` and learning rate of `0.0005` provided the best balance of performance and efficiency.\n",
        "\n",
        "Larger hidden sizes increased model complexity without significant gains in performance.\n",
        "\n",
        "Optimal hidden sizes and learning rates are crucial for maximizing the LAAT models efficiency without overfitting, confirming the importance of these parameters in model tuning.\n",
        "\n",
        "RAC with HiCuA: Extended Sequence Lengths\n",
        "Objective: To determine the effects of longer sequence lengths on classification accuracy.\n",
        "Method: Increased the sequence lengths from 4000 to 5000 and 6000 tokens.\n",
        "Results:\n",
        "Sequence length of 5000 improved the model's ability to capture more contextual information, enhancing Micro and Macro F1 scores.\n",
        "Sequence length of 6000 did not show significant improvement and increased computational costs.\n",
        "Discussion: There is an optimal cap on sequence length that balances performance gains with computational efficiency, suggesting that extending beyond 5000 tokens yields diminishing returns.\n",
        "\n",
        "## Ablation Study\n",
        "\n",
        "This summary presents the results from the application of the `MultiResCNN` model equipped with a HierarchicalHyperbolic decoder, specifically designed to address the complexities of medical text classification. My experimental setup aims to evaluate the model's effectiveness across various hierarchical depths, with a specific focus on precision at depth 8 (prec_at_8) as the primary criterion. Below, I detail the configuration of my experiments, present a comprehensive summary of results across multiple depths, and compare these outcomes with existing benchmarks from the original paper.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<th>Model</th>\n",
        "<th>Macro AUC</th>\n",
        "<th>Micro AUC</th>\n",
        "<th>Macro F1</th>\n",
        "<th>Micro F1</th>\n",
        "<th>P@5</th>\n",
        "<th>P@8</th>\n",
        "<th>P@15</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>MultiResCNN*</td>\n",
        "<td>91.2</td>\n",
        "<td>98.7</td>\n",
        "<td>8.6</td>\n",
        "<td>56.2</td>\n",
        "<td>81.7</td>\n",
        "<td>74.3</td>\n",
        "<td>59.1</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>w/ KT</td>\n",
        "<td>93.8</td>\n",
        "<td>99.0</td>\n",
        "<td>8.9</td>\n",
        "<td>56.4</td>\n",
        "<td>81.6</td>\n",
        "<td>74.3</td>\n",
        "<td>59.1</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>w/ KT+HCA</td>\n",
        "<td>94.7</td>\n",
        "<td>99.1</td>\n",
        "<td>9.2</td>\n",
        "<td>56.7</td>\n",
        "<td>82.0</td>\n",
        "<td>74.8</td>\n",
        "<td>59.6</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>w/ KT+HCC</td>\n",
        "<td>94.6</td>\n",
        "<td>99.1</td>\n",
        "<td>9.3</td>\n",
        "<td>56.6</td>\n",
        "<td>82.1</td>\n",
        "<td>74.8</td>\n",
        "<td>59.6</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>w/ KT+HCA+ASL</td>\n",
        "<td>93.7</td>\n",
        "<td>98.9</td>\n",
        "<td>11.4</td>\n",
        "<td>57.6</td>\n",
        "<td>82.4</td>\n",
        "<td>75.1</td>\n",
        "<td>59.8</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>w/ KT+HCC+ASL</td>\n",
        "<td>94.0</td>\n",
        "<td>98.9</td>\n",
        "<td>11.5</td>\n",
        "<td>57.4</td>\n",
        "<td>82.4</td>\n",
        "<td>75.1</td>\n",
        "<td>59.7</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "The key findings from the ablation study are:\n",
        "\n",
        "1. The introduction of the knowledge transfer mechanism (`KT`) improves the model performance for both AUC and F1 metrics compared to the vanilla MultiResCNN model, especially for the Macro-AUC score.\n",
        "2. The hyperbolic embedding correction mechanism (`HCA` and `HCC`) further enhances the model performance across all evaluation metrics when added to the model with knowledge transfer (`KT`).\n",
        "3. Incorporating the asymmetric loss function (`ASL`) leads to significant improvements in F1 scores and Precision@K metrics, particularly for the Macro F1. Although the AUC scores slightly decrease, the dramatic improvements in F1 scores make this a desirable change.\n",
        "\n",
        "The ablation study demonstrates that each component of the HiCu algorithm, including the knowledge transfer initialization, hyperbolic embedding correction, and asymmetric loss function, contributes to the overall performance improvement of the MultiResCNN model for the task of automated ICD coding.\n",
        "\n",
        "## Model Comparison, Training and Evaluation Summary\n",
        "\n",
        "##MultiResCNN with HiCuA\n",
        "\n",
        "### Configuration\n",
        "- **Model**: MultiResCNN with HierarchicalHyperbolic decoder\n",
        "- **Criterion**: Precision at depth 8 (prec_at_8)\n",
        "- **Batch size**: 8\n",
        "- **Learning rate**: 5e-05\n",
        "- **Dropout**: 0.2\n",
        "- **Filters sizes**: 3, 5, 9, 15, 19, 25\n",
        "\n",
        "### Results Summary (Depth 0 to 4)\n",
        "\n",
        "#### Depth 0 Results:\n",
        "- Best Macro AUC: 0.8921\n",
        "- Best Micro AUC: 0.9507\n",
        "- Best prec_at_8: 0.7849\n",
        "\n",
        "#### Depth 1 Results:\n",
        "- Best Macro AUC: 0.9032\n",
        "- Best Micro AUC: 0.9732\n",
        "- Best prec_at_8: 0.8130\n",
        "\n",
        "#### Depth 2 Results:\n",
        "- Best Macro AUC: 0.9157\n",
        "- Best Micro AUC: 0.9838\n",
        "- Best prec_at_8: 0.7992\n",
        "\n",
        "#### Depth 3 Results:\n",
        "- Best Macro AUC: 0.9335\n",
        "- Best Micro AUC: 0.9891\n",
        "- Best prec_at_8: 0.7641\n",
        "\n",
        "#### Depth 4 Results:\n",
        "- Best Macro AUC: 0.9461\n",
        "- Best Micro AUC: 0.9906\n",
        "- Best prec_at_8: 0.7394\n",
        "\n",
        "### Comparative Analysis with Paper\n",
        "\n",
        "#### Observations:\n",
        "- Increasing depth improves the Micro AUC consistently, suggesting better performance on the individual label predictions as the model becomes more specific in its hierarchy.\n",
        "- Macro AUC generally improves with depth, indicating improved average performance across all labels, particularly as the specificity of the hierarchy increases.\n",
        "- The precision at a threshold of 8 (prec_at_8) tends to decrease slightly with increased depth. This could suggest a trade-off where the model becomes more conservative or struggles with the specificity of deeper labels.\n",
        "\n",
        "#### Conclusion:\n",
        "The experiments demonstrate the viability of using hierarchical decoders in medical text classification. The results show significant improvements in AUC metrics as the depth increases, aligning well with theoretical expectations from the paper. However, the decline in prec_at_8 at higher depths may warrant further investigation or adjustments in model training or hyperparameters to balance the precision-recall trade-off effectively.\n",
        "\n",
        "## RAC with HiCuA\n",
        "\n",
        "### Configuration\n",
        "- **Model:** RACReader with HierarchicalHyperbolic decoder\n",
        "- **Criterion:** Precision at depth 8 (prec_at_8)\n",
        "- **Batch size:** 16\n",
        "- **Learning rate:** 8e-05\n",
        "- **Dropout:** 0.1\n",
        "- **Filter size:** 9\n",
        "\n",
        "### Results Summary (Depth 0 to 4)\n",
        "\n",
        "#### Depth 0 Results:\n",
        "- Best Macro AUC: 0.9239\n",
        "- Best Micro AUC: 0.9627\n",
        "- Best prec_at_8: 0.8105\n",
        "\n",
        "#### Depth 1 Results:\n",
        "- Best Macro AUC: 0.9400\n",
        "- Best Micro AUC: 0.9802\n",
        "- Best prec_at_8: 0.8405\n",
        "\n",
        "#### Depth 2 Results:\n",
        "- Best Macro AUC: 0.9417\n",
        "- Best Micro AUC: 0.9875\n",
        "- Best prec_at_8: 0.8178\n",
        "\n",
        "#### Depth 3 Results:\n",
        "- Best Macro AUC: 0.9443\n",
        "- Best Micro AUC: 0.9905\n",
        "- Best prec_at_8: 0.7694\n",
        "\n",
        "#### Depth 4 Results:\n",
        "- Best Macro AUC: 0.9418\n",
        "- Best Micro AUC: 0.9889\n",
        "- Best prec_at_8: 0.7302\n",
        "\n",
        "### Comparative Analysis with Paper\n",
        "\n",
        "#### Observations:\n",
        "- **AUC Improvements:** Both Macro and Micro AUC metrics show a general improvement with depth, suggesting that the models hierarchical learning structure effectively improves its overall performance.\n",
        "- **Precision Trade-Off:** The precision at 8 (prec_at_8) sees a general decline as depth increases, indicating potential difficulty in maintaining precision while navigating deeper hierarchical structures.\n",
        "\n",
        "#### Conclusion:\n",
        "The results demonstrate that the hierarchical learning approach effectively boosts overall performance metrics. However, the decrease in prec_at_8 at higher depths may indicate challenges in balancing precision and recall, suggesting that adjustments in model training or hyperparameters might help address this trade-off.\n",
        "\n",
        "## LAAT with HiCuA + ASL\n",
        "\n",
        "### Configuration\n",
        "- **Model:** LAAT with HiCuA + ASL\n",
        "- **Criterion:** Micro F1\n",
        "- **Batch size:** 8\n",
        "- **Learning rate:** 0.0005\n",
        "- **Dropout:** 0.3\n",
        "- **Hidden size:** 256\n",
        "- **Decoder:** HierarchicalHyperbolic\n",
        "- **Hyperbolic dimension:** 50\n",
        "- **ASL Configuration:** '1,0,0.03'\n",
        "\n",
        "### Results Summary (Depth 0 to 4)\n",
        "\n",
        "#### Depth 0 Results:\n",
        "- Macro AUC: 0.88609\n",
        "- Micro AUC: 0.90817\n",
        "- Micro F1: 0.73892\n",
        "\n",
        "#### Depth 1 Results:\n",
        "- Macro AUC: 0.90284\n",
        "- Micro AUC: 0.93661\n",
        "- Micro F1: 0.72267\n",
        "\n",
        "#### Depth 2 Results:\n",
        "- Macro AUC: 0.91843\n",
        "- Micro AUC: 0.94111\n",
        "- Micro F1: 0.72916\n",
        "\n",
        "#### Depth 3 Results:\n",
        "- Macro AUC: 0.91999\n",
        "- Micro AUC: 0.9408\n",
        "- Micro F1: 0.69207\n",
        "\n",
        "#### Depth 4 Results:\n",
        "- Macro AUC: 0.92077\n",
        "- Micro AUC: 0.94012\n",
        "- Micro F1: 0.71076\n",
        "\n",
        "### Comparative Analysis with Paper\n",
        "\n",
        "#### Observations:\n",
        "- **Macro and Micro AUC Trends:** Both metrics show consistent improvement, demonstrating the model's effectiveness across multiple levels of hierarchy.\n",
        "- **F1 Fluctuations:** The Micro F1 scores fluctuate at different depths, highlighting potential challenges in balancing precision and recall at higher levels.\n",
        "\n",
        "#### Conclusion:\n",
        "The LAAT model with HiCuA + ASL demonstrates effective hierarchical learning, with significant improvements in both AUC metrics. While the Micro F1 scores fluctuate at higher depths, the model's overall performance aligns well with the intended goals, suggesting that hierarchical decoders offer valuable insights into medical text classification tasks.\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define metrics dictionary for MultiResCNN with HiCuA\n",
        "metrics = {\n",
        "    \"acc_macro\": [\n",
        "        0.0319410441138809, 0.03739311692629136, 0.04200764727082538, 0.041908939148518586,\n",
        "        0.04428193820820927, 0.04648494208681928, 0.04702845262416949, 0.051632401177169884,\n",
        "        0.04940818538863592, 0.05339134182512112, 0.05339318491330262, 0.053907763773604386,\n",
        "        0.05705621680566151, 0.05831238854021586, 0.05533569204532243, 0.05633576198223152,\n",
        "        0.056353274170392334, 0.05776788409498362, 0.05948067460477451, 0.05802529620301705,\n",
        "        0.05902107262307773, 0.061568241778608354, 0.06103727929906239, 0.05705621680566151,\n",
        "    ],\n",
        "    \"prec_macro\": [\n",
        "        0.055050915080455315, 0.06191013370758759, 0.06751163951241014, 0.06825219903519515,\n",
        "        0.07192044409172237, 0.07335514190622429, 0.07580388564414706, 0.0786843241755169,\n",
        "        0.07845020073004967, 0.08210756743872959, 0.08178934588637381, 0.08272133651879061,\n",
        "        0.08661519010508072, 0.08759395613888937, 0.08528581434217886, 0.08734165756882176,\n",
        "        0.0868481832691611, 0.0875884866070976, 0.08884225647994876, 0.08884726693098187,\n",
        "        0.0895100500563429, 0.09168249059996848, 0.09048287944898398, 0.08661519010508072,\n",
        "    ],\n",
        "    \"rec_macro\": [\n",
        "        0.03903634284502181, 0.04537442076745888, 0.05178045999616125, 0.05051695951123941,\n",
        "        0.053561232680709434, 0.05644434369605238, 0.05634899883781107, 0.0626907566083086,\n",
        "        0.0593062441239584, 0.06523472549072648, 0.06570519213770168, 0.06528156540655329,\n",
        "        0.06970809925349064, 0.07187939262886385, 0.06681296987560384, 0.0683895395592821,\n",
        "        0.0679721201210295, 0.07022182597546836, 0.07270862852140461, 0.07085275192101909,\n",
        "        0.07136283509443288, 0.07548352279060942, 0.07456319393673784, 0.06970809925349064,\n",
        "    ],\n",
        "    \"f1_macro\": [\n",
        "        0.04568071048929844, 0.05236795679234704, 0.058608805837916084, 0.05806041934474095,\n",
        "        0.06139777120579582, 0.06379813945202562, 0.06464441666646722, 0.06978287529072004,\n",
        "        0.06754800853057143, 0.07270505319394327, 0.07287028737842399, 0.07297395213497786,\n",
        "        0.07724735442145901, 0.07896241489720131, 0.07492760147643744, 0.07671238461697089,\n",
        "        0.07625944422272245, 0.07794957583345839, 0.07996983270441176, 0.07883622566826701,\n",
        "        0.07941289715141815, 0.08279813854904816, 0.08175526202967179, 0.07724735442145901,\n",
        "    ],\n",
        "    \"acc_micro\": [\n",
        "        0.3473671512153919, 0.3633028626413268, 0.37439456585942, 0.37417857358171946,\n",
        "        0.38079420654157947, 0.3870202993035994, 0.3847281649262428, 0.39313337663104325,\n",
        "        0.3887213780338526, 0.398876240086622, 0.3964927663305556, 0.398133200270039,\n",
        "        0.4015111886079616, 0.40275973557345224, 0.39399037751999644, 0.39903239994135636,\n",
        "        0.3968160723726711, 0.4004991004584748, 0.4022388492546824, 0.4019463028933866,\n",
        "        0.4015844352536832, 0.4042896214193165, 0.40274783708430223, 0.4015111886079616,\n",
        "    ],\n",
        "    \"prec_micro\": [\n",
        "        0.7077547007496577, 0.7133915918752911, 0.6990350151640436, 0.7223159732324661,\n",
        "        0.717305524239004, 0.7096669021355176, 0.7192481960060373, 0.7087657672042115,\n",
        "        0.7187568999779161, 0.7026135367802429, 0.7000361215748971, 0.7053195361655624,\n",
        "        0.696933010492329, 0.6910009410133194, 0.7090571049136749, 0.7046913835956882,\n",
        "        0.7053357001148549, 0.6948947739401838, 0.6952380952380918, 0.693795930610405,\n",
        "        0.6991886409736273, 0.6891931684334478, 0.6868290770060258, 0.696933010492329,\n",
        "    ],\n",
        "    \"rec_micro\": [\n",
        "        0.4055348214914429, 0.4253925779874642, 0.44634180691500447, 0.437046686853037,\n",
        "        0.4480318287444531, 0.4598267727624799, 0.4527145975635503, 0.46887543130765275,\n",
        "        0.45845363002605294, 0.47989578198718236, 0.47764241954791753, 0.4775720019716905,\n",
        "        0.4864446165762958, 0.49123301175973355, 0.46996690373917166, 0.47915639743679855,\n",
        "        0.47567072741356076, 0.48595169354270656, 0.48834589113442545, 0.48862756143933356,\n",
        "        0.4854587705091174, 0.49443701147806324, 0.4933455390465443, 0.4864446165762958,\n",
        "    ],\n",
        "    \"f1_micro\": [\n",
        "        0.5156236010385867, 0.5329745467378335, 0.5448138040698776, 0.5445850790795598,\n",
        "        0.5515582332798729, 0.5580600363209035, 0.5556731995073336, 0.5643872772350637,\n",
        "        0.5598263037963775, 0.5702809564653436, 0.5678407735292248, 0.5695211303088172,\n",
        "        0.5729689379173037, 0.5742390879344739, 0.565269866813473, 0.5704405415601267,\n",
        "        0.5681722600723333, 0.5719376761147001, 0.573709463931169, 0.5734118376200783,\n",
        "        0.5730435144008953, 0.5757923654106336, 0.574226994242155, 0.5729689379173037,\n",
        "    ],\n",
        "    \"rec_at_5\": [\n",
        "        0.28035160005894366, 0.28389498650038003, 0.2876526043394757, 0.28849394141515966,\n",
        "        0.28760356534531284, 0.290001134819842, 0.28888985466673245, 0.29138536051226804,\n",
        "        0.28951010152440354, 0.2905445111007091, 0.28922524982472425, 0.2908734395607854,\n",
        "        0.29163025434631956, 0.29136527694094994, 0.2907998533264755, 0.2908933890656855,\n",
        "        0.29084962284934807, 0.2899424620628628, 0.2901021192561756, 0.29038023806866486,\n",
        "        0.2890369089033346, 0.2891462930811932, 0.28974348362268615, 0.29163025434631956,\n",
        "    ],\n",
        "    \"prec_at_5\": [\n",
        "        0.7981606376456163, 0.804659717964439, 0.8132434089515636, 0.8165542611894543,\n",
        "        0.8159411404046597, 0.8206008583690986, 0.819865113427345, 0.8253832004904966,\n",
        "        0.8188841201716739, 0.8220723482526058, 0.820232985898222, 0.82501532801962,\n",
        "        0.8268546903740037, 0.8239117106069895, 0.8250153280196199, 0.8245248313917843,\n",
        "        0.8261189454322502, 0.8244022072348252, 0.8224402207234826, 0.8226854690374004,\n",
        "        0.818638871857756, 0.8191293684855917, 0.8198651134273452, 0.8268546903740037,\n",
        "    ],\n",
        "    \"f1_at_5\": [\n",
        "        0.41495238356175623, 0.41971038999133164, 0.4249839798175144, 0.42635417465888886,\n",
        "        0.4252978241290547, 0.4285516893011548, 0.4272372530992855, 0.4307151720312398,\n",
        "        0.4277814159024539, 0.4293456575002665, 0.42765393526743684, 0.43010567562531726,\n",
        "        0.43118299410192584, 0.4304926335182335, 0.4300252235155504, 0.4300607936528043,\n",
        "        0.43022944518004597, 0.4290040815583046, 0.4289124870479294, 0.4292497753359448,\n",
        "        0.4272312406565358, 0.4274175255580583, 0.42817003166105927, 0.43118299410192584,\n",
        "    ],\n",
        "    \"rec_at_8\": [\n",
        "        0.38924594705722326, 0.3978040625441737, 0.39843560412660195, 0.40175624758191164,\n",
        "        0.40425926935326145, 0.4039384205015307, 0.40502899896068867, 0.40645029952173134,\n",
        "        0.40627602921723655, 0.4075762217442141, 0.40732932273703515, 0.4070230117222617,\n",
        "        0.4091276717648278, 0.4082164670945313, 0.4073553858489137, 0.4088743225810868,\n",
        "        0.4087653800194409, 0.4084985232875138, 0.40739297805348046, 0.4087075012222824,\n",
        "        0.40823227066987794, 0.4069017499549039, 0.4077689189210399, 0.4091276717648278,\n",
        "    ],\n",
        "    \"prec_at_8\": [\n",
        "        0.7135959534028203, 0.726241569589209, 0.7291538933169834, 0.7341354996934396,\n",
        "        0.7394236664622931, 0.7397302268546904, 0.7415695892090742, 0.7454015941140405,\n",
        "        0.7437155119558553, 0.7450183936235438, 0.7460147148988351, 0.7450183936235438,\n",
        "        0.7481606376456161, 0.7461679950950337, 0.7456315144083384, 0.7466278356836297,\n",
        "        0.747164316370325, 0.7472409564684243, 0.7452483139178419, 0.7477774371551196,\n",
        "        0.746474555487431, 0.7440220723482526, 0.7456315144083384, 0.7481606376456161,\n",
        "    ],\n",
        "    \"f1_at_8\": [\n",
        "        0.5037246636759363, 0.5140393566099682, 0.5152954557375189, 0.5193162540021707,\n",
        "        0.5227303158692296, 0.5225385169448525, 0.5239099218785541, 0.5260549604781564,\n",
        "        0.5254887088238334, 0.5269012677213343, 0.5269436675538489, 0.5264387702629726,\n",
        "        0.5289835165482188, 0.527723774519559, 0.52686984246768, 0.5283883692502641,\n",
        "        0.5284316281032707, 0.5282277409468799, 0.5268055762206347, 0.5285364948010001,\n",
        "        0.5278136335229507, 0.5260884819253216, 0.5272156100481007, 0.5289835165482188,\n",
        "    ],\n",
        "    \"rec_at_15\": [\n",
        "        0.541341411179512, 0.5526087771865853, 0.555893346821313, 0.5621222894765862,\n",
        "        0.5665070377113649, 0.5668091792442521, 0.5714924498082927, 0.5709536406846468,\n",
        "        0.5725970986582951, 0.5742791329872476, 0.5712066287427464, 0.572907029663427,\n",
        "        0.5740970019525167, 0.573122776841011, 0.5749192196842763, 0.573275095914807,\n",
        "        0.5750569879411549, 0.5744172529608215, 0.5735556622993224, 0.57312359052784,\n",
        "        0.5738734210780555, 0.5729044608861602, 0.5752298550121455, 0.5740970019525167,\n",
        "    ],\n",
        "    \"prec_at_15\": [\n",
        "        0.557653791130186, 0.56856734109953, 0.5731453096259963, 0.5789903944410382,\n",
        "        0.5842632331902718, 0.5843041079092581, 0.5882280809319436, 0.5884324545268751,\n",
        "        0.5896995708154507, 0.5912528101369302, 0.5893316983445739, 0.5909258123850398,\n",
        "        0.5919068056407113, 0.5908849376660535, 0.592560801144492, 0.5911710606989576,\n",
        "        0.5931330472103005, 0.5919885550786839, 0.5914163090128756, 0.5910893112609851,\n",
        "        0.5921111792356427, 0.5915798078888207, 0.5926425505824647, 0.5919068056407113,\n",
        "    ],\n",
        "    \"f1_at_15\": [\n",
        "        0.549376538870366, 0.5604744838724066, 0.5643875213014936, 0.5704316684847978,\n",
        "        0.5752481478669984, 0.5754236972662531, 0.5797395115584528, 0.5795612929924389,\n",
        "        0.5810225086196825, 0.5826423774737066, 0.5801276265774078, 0.5817769351294219,\n",
        "        0.5828658883285056, 0.5818683365206184, 0.5836067210257587, 0.5820855599022157,\n",
        "        0.5839551670767208, 0.5830705527162917, 0.5823490713315994, 0.5819678305780747,\n",
        "        0.5828496671312821, 0.5820923820056758, 0.5838063932540826, 0.5828658883285056,\n",
        "    ],\n",
        "    \"auc_macro\": [\n",
        "        0.9389798372481776, 0.9426919790688367, 0.9444346256170416, 0.9452248935126247,\n",
        "        0.9461177668384458, 0.946386139568928, 0.9472251274864014, 0.9475613571256425,\n",
        "        0.9476326878967255, 0.948315871548033, 0.9488060366634748, 0.9486536474750279,\n",
        "        0.9483379537025396, 0.948229719222197, 0.9482253933497689, 0.9480277528948664,\n",
        "        0.9479714660764048, 0.9480112307297707, 0.9479350898081624, 0.948007197996098,\n",
        "        0.9472651179102068, 0.9471547056773878, 0.9468984523368007, 0.9483379537025396,\n",
        "    ],\n",
        "    \"auc_micro\": [\n",
        "        0.9891575495614335, 0.9898036620730503, 0.9901548782452172, 0.9903913315843522,\n",
        "        0.9906283489987753, 0.9906259686609908, 0.9906632586197587, 0.9908049935199525,\n",
        "        0.9907703235917504, 0.990943530504026, 0.9911151811061722, 0.9910326796902206,\n",
        "        0.9909769248165419, 0.9910135257844908, 0.9909313494990228, 0.9909053133988264,\n",
        "        0.9908204330379967, 0.9907813589190633, 0.9907305478876433, 0.9906805755524407,\n",
        "        0.9907075288365684, 0.9906649485616948, 0.9906066916520845, 0.9909769248165419,\n",
        "    ],\n",
        "    \"loss_dev\": [\n",
        "        0.005253945010753347, 0.005109813651124245, 0.005071664897370673, 0.004988505242020263,\n",
        "        0.004949123402545686, 0.0049383173346808406, 0.0049184202342902284, 0.004899668522195686,\n",
        "        0.00489568907060834, 0.004892333869703553, 0.004887816087151652, 0.004871796344769925,\n",
        "        0.004880910685252664, 0.0048931323136100205, 0.004885588675005199, 0.004892962677285799,\n",
        "        0.004903879789932466, 0.0049172262466122335, 0.004924529547855066, 0.004941300906294187,\n",
        "        0.00493478821128379, 0.004960395178039131, 0.004962928907447971, 0.004880910685252664,\n",
        "    ],\n",
        "    \"acc_macro_te\": [\n",
        "        0.06390121460628688,\n",
        "    ],\n",
        "    \"prec_macro_te\": [\n",
        "        0.10461532414484478,\n",
        "    ],\n",
        "    \"rec_macro_te\": [\n",
        "        0.07985898124468153,\n",
        "    ],\n",
        "    \"f1_macro_te\": [\n",
        "        0.09057600939218684,\n",
        "    ],\n",
        "    \"acc_micro_te\": [\n",
        "        0.3955432510924729,\n",
        "    ],\n",
        "    \"prec_micro_te\": [\n",
        "        0.6912204145520071,\n",
        "    ],\n",
        "    \"rec_micro_te\": [\n",
        "        0.48043385092143787,\n",
        "    ],\n",
        "    \"f1_micro_te\": [\n",
        "        0.5668663451065805,\n",
        "    ],\n",
        "    \"rec_at_5_te\": [\n",
        "        0.2803638169377092,\n",
        "    ],\n",
        "    \"prec_at_5_te\": [\n",
        "        0.8225978647686834,\n",
        "    ],\n",
        "    \"f1_at_5_te\": [\n",
        "        0.4181952664294828,\n",
        "    ],\n",
        "    \"rec_at_8_te\": [\n",
        "        0.39540801202221154,\n",
        "    ],\n",
        "    \"prec_at_8_te\": [\n",
        "        0.749814650059312,\n",
        "    ],\n",
        "    \"f1_at_8_te\": [\n",
        "        0.5177730584307586,\n",
        "    ],\n",
        "    \"rec_at_15_te\": [\n",
        "        0.5592055521973873,\n",
        "    ],\n",
        "    \"prec_at_15_te\": [\n",
        "        0.5969553183076315,\n",
        "    ],\n",
        "    \"f1_at_15_te\": [\n",
        "        0.5774641521392625,\n",
        "    ],\n",
        "    \"auc_macro_te\": [\n",
        "        0.9477779712892075,\n",
        "    ],\n",
        "    \"auc_micro_te\": [\n",
        "        0.9906868448470423,\n",
        "    ],\n",
        "    \"loss_test_te\": [\n",
        "        0.005061819953743313,\n",
        "    ],\n",
        "    \"loss_tr\": [\n",
        "        0.0049190428767240885, 0.004519941277875372, 0.004376297469944861, 0.0042810088326047544,\n",
        "        0.004205600803480651, 0.00414191668303582, 0.004082608513833841, 0.004032923659440114,\n",
        "        0.003986341197493315, 0.003944799933236713, 0.0039041667736946523, 0.0038645350913909847,\n",
        "        0.0038302744528761153, 0.00379381276647992, 0.0037617698310244414, 0.0037301000024208427,\n",
        "        0.003695994527652947, 0.0036657694461564192, 0.003637516189191712, 0.0036060613572827015,\n",
        "        0.0035780908498906803, 0.0035523561911449796, 0.003524543843813684, float('nan'),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Define epochs\n",
        "epochs = list(range(1, 25))\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Add a suptitle for the entire figure\n",
        "plt.suptitle(\"MultiResCNN with HiCuA Metrics\", fontsize=16, weight=\"bold\")\n",
        "\n",
        "# Macro and Micro Accuracy\n",
        "plt.subplot(3, 2, 1)\n",
        "plt.plot(epochs, metrics[\"acc_macro\"], label=\"Macro Accuracy\")\n",
        "plt.plot(epochs, metrics[\"acc_micro\"], label=\"Micro Accuracy\")\n",
        "plt.title(\"Macro and Micro Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "# Add a caption inside the plot\n",
        "plt.text(1, 0.07, \"Comparison between Macro and Micro Accuracy\\n(MultiResCNN with HiCuA)\", fontsize=10)\n",
        "\n",
        "# Macro and Micro Precision\n",
        "plt.subplot(3, 2, 2)\n",
        "plt.plot(epochs, metrics[\"prec_macro\"], label=\"Macro Precision\")\n",
        "plt.plot(epochs, metrics[\"prec_micro\"], label=\"Micro Precision\")\n",
        "plt.title(\"Macro and Micro Precision\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend()\n",
        "\n",
        "# Add a caption inside the plot\n",
        "plt.text(1, 0.1, \"Comparison between Macro and Micro Precision\\n(MultiResCNN with HiCuA)\", fontsize=10)\n",
        "\n",
        "# Macro and Micro Recall\n",
        "plt.subplot(3, 2, 3)\n",
        "plt.plot(epochs, metrics[\"rec_macro\"], label=\"Macro Recall\")\n",
        "plt.plot(epochs, metrics[\"rec_micro\"], label=\"Micro Recall\")\n",
        "plt.title(\"Macro and Micro Recall\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.legend()\n",
        "\n",
        "# Add a caption inside the plot\n",
        "plt.text(1, 0.1, \"Comparison between Macro and Micro Recall\\n(MultiResCNN with HiCuA)\", fontsize=10)\n",
        "\n",
        "# Macro and Micro F1-Score\n",
        "plt.subplot(3, 2, 4)\n",
        "plt.plot(epochs, metrics[\"f1_macro\"], label=\"Macro F1-Score\")\n",
        "plt.plot(epochs, metrics[\"f1_micro\"], label=\"Micro F1-Score\")\n",
        "plt.title(\"Macro and Micro F1-Score\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.legend()\n",
        "\n",
        "# Add a caption inside the plot\n",
        "plt.text(1, 0.08, \"Comparison between Macro and Micro F1-Score\\n(MultiResCNN with HiCuA)\", fontsize=10)\n",
        "\n",
        "# AUC Macro and Micro\n",
        "plt.subplot(3, 2, 5)\n",
        "plt.plot(epochs, metrics[\"auc_macro\"], label=\"AUC Macro\")\n",
        "plt.plot(epochs, metrics[\"auc_micro\"], label=\"AUC Micro\")\n",
        "plt.title(\"AUC Macro and Micro\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"AUC\")\n",
        "plt.legend()\n",
        "\n",
        "# Add a caption inside the plot\n",
        "plt.text(1, 0.94, \"Comparison between AUC Macro and Micro\\n(MultiResCNN with HiCuA)\", fontsize=10)\n",
        "\n",
        "# Development and Training Loss\n",
        "plt.subplot(3, 2, 6)\n",
        "plt.plot(epochs, metrics[\"loss_dev\"], label=\"Development Loss\")\n",
        "plt.plot(epochs, metrics[\"loss_tr\"], label=\"Training Loss\")\n",
        "plt.title(\"Development and Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Add a caption inside the plot\n",
        "plt.text(1, 0.0035, \"Comparison between Training and Development Loss\\n(MultiResCNN with HiCuA)\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wF3dNiEt54JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define metrics dictionary for RAC with HiCuA\n",
        "metrics = {\n",
        "    \"acc_macro\": [\n",
        "        0.049827077048913176, 0.05444015307794422, 0.0535509499179446, 0.05514089466578472,\n",
        "        0.062081863324015336, 0.06169427564058882, 0.06353309855875226, 0.0629363125384633,\n",
        "        0.0642740664960267, 0.06407294035648421, 0.06429444610440746, 0.06352384269727258,\n",
        "        0.06500260051469192,\n",
        "    ],\n",
        "    \"prec_macro\": [\n",
        "        0.07507418734229988, 0.07960234092806526, 0.0808647351493802, 0.08223187070573393,\n",
        "        0.08798346581938711, 0.08809798695007137, 0.0904404052345051, 0.09075230326869649,\n",
        "        0.09115306426039257, 0.09043825387438378, 0.09124912255002354, 0.09084243438198913,\n",
        "        0.09192301232750452,\n",
        "    ],\n",
        "    \"rec_macro\": [\n",
        "        0.06375698765739332, 0.07106670841137026, 0.06746084375285692, 0.06911235601780802,\n",
        "        0.08108816177636861, 0.0787976914301209, 0.08229165171463099, 0.08021250636249622,\n",
        "        0.08239366244329382, 0.08395929240411833, 0.08341763570508824, 0.08250244258163433,\n",
        "        0.08521659623212322,\n",
        "    ],\n",
        "    \"f1_macro\": [\n",
        "        0.06895431138982185, 0.07509274633906696, 0.07355714777454633, 0.07510347037692452,\n",
        "        0.08439520706651267, 0.0831887087632394, 0.08617381695025553, 0.08515752123556726,\n",
        "        0.08655230726611132, 0.08707842470938797, 0.08715780998430536, 0.08647181108385124,\n",
        "        0.0884428535170508,\n",
        "    ],\n",
        "    \"acc_micro\": [\n",
        "        0.3729933368944857, 0.3834699453551902, 0.3858797265243466, 0.38415863768765296,\n",
        "        0.38537411309395725, 0.3852330485562875, 0.3866666666666656, 0.38556517996449025,\n",
        "        0.38465626743907844, 0.3791238931118241, 0.37792791608612114, 0.3817721652474219,\n",
        "        0.3781271783750281,\n",
        "    ],\n",
        "    \"prec_micro\": [\n",
        "        0.6492610355290171, 0.6312688346152089, 0.6503927378768607, 0.6525834998572622,\n",
        "        0.6195290559516068, 0.6239982483030408, 0.6072228211294444, 0.6203419173339078,\n",
        "        0.6106564292946313, 0.5949579831932749, 0.5986757173197826, 0.603123698458973,\n",
        "        0.5863794691115803,\n",
        "    ],\n",
        "    \"rec_micro\": [\n",
        "        0.46711499190197714, 0.4941553411731551, 0.4868671220336579, 0.48292373776494446,\n",
        "        0.504858812759663, 0.5016900218294469, 0.5156327019223981, 0.504647560030982,\n",
        "        0.5096472079431008, 0.5110203506795279, 0.5061615379198631, 0.5098584606717819,\n",
        "        0.5156679107105115,\n",
        "    ],\n",
        "    \"f1_micro\": [\n",
        "        0.5433286919485604, 0.5543596326651504, 0.5568733262187119, 0.5550789154188566,\n",
        "        0.5563466351097035, 0.5561996213673691, 0.5576923076923056, 0.5565457122332866,\n",
        "        0.5555982036617643, 0.5498039661344374, 0.5485452637603718, 0.552583377852398,\n",
        "        0.5487551284212873,\n",
        "    ],\n",
        "    \"auc_macro\": [\n",
        "        0.9397683336263387, 0.9409872523123108, 0.9417704279761582, 0.9404217970340546,\n",
        "        0.9402555447268021, 0.940271636775839, 0.9391270838301476, 0.9358197552292049,\n",
        "        0.9344766509236048, 0.9336122849032996, 0.9316539678109139, 0.9307587720899335,\n",
        "        0.9286466999574929,\n",
        "    ],\n",
        "    \"auc_micro\": [\n",
        "        0.9894046113775891, 0.9895764825694432, 0.9889266298146785, 0.9886841447315418,\n",
        "        0.9885566678719244, 0.9884057374348012, 0.9877988489730025, 0.98751279429083,\n",
        "        0.9869047862189505, 0.9870305948654986, 0.9865422768425227, 0.9861537102334715,\n",
        "        0.9857368828841264,\n",
        "    ],\n",
        "    \"loss_dev\": [\n",
        "        0.005400493109923092, 0.005390263896662192, 0.005504218634946876, 0.005486414807433649,\n",
        "        0.005663217526498941, 0.005751110747552053, 0.005932368007080988, 0.006010961904579341,\n",
        "        0.006134293979370154, 0.006233837989564598, 0.006370903686195782, 0.006537281289518038,\n",
        "        0.006664286053252414,\n",
        "    ],\n",
        "    \"loss_tr\": [\n",
        "        0.0037912329211000615, 0.0033475429318724137, 0.0031597398560943143, 0.0030066806998945575,\n",
        "        0.00287254806166965, 0.002750809433593087, 0.002636531299992302, 0.0025299569124579454,\n",
        "        0.002427334317953416, 0.002330523149776633, 0.0022364783038263765, 0.002144741062600974,\n",
        "        0.0020616028348684557,\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Matching epochs count to the length of the lists\n",
        "epochs = list(range(1, len(metrics[\"acc_macro\"]) + 1))\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Add the main title\n",
        "plt.suptitle(\"RAC with HiCuA Metrics\", fontsize=16, weight=\"bold\")\n",
        "\n",
        "# Macro and Micro Accuracy\n",
        "plt.subplot(3, 2, 1)\n",
        "plt.plot(epochs, metrics[\"acc_macro\"], label=\"Macro Accuracy\")\n",
        "plt.plot(epochs, metrics[\"acc_micro\"], label=\"Micro Accuracy\")\n",
        "plt.title(\"Macro and Micro Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.text(1, 0.08, \"Accuracy comparison between macro and micro metrics\", fontsize=10)\n",
        "\n",
        "# Macro and Micro Precision\n",
        "plt.subplot(3, 2, 2)\n",
        "plt.plot(epochs, metrics[\"prec_macro\"], label=\"Macro Precision\")\n",
        "plt.plot(epochs, metrics[\"prec_micro\"], label=\"Micro Precision\")\n",
        "plt.title(\"Macro and Micro Precision\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend()\n",
        "\n",
        "plt.text(1, 0.1, \"Precision comparison between macro and micro metrics\", fontsize=10)\n",
        "\n",
        "# Macro and Micro Recall\n",
        "plt.subplot(3, 2, 3)\n",
        "plt.plot(epochs, metrics[\"rec_macro\"], label=\"Macro Recall\")\n",
        "plt.plot(epochs, metrics[\"rec_micro\"], label=\"Micro Recall\")\n",
        "plt.title(\"Macro and Micro Recall\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.legend()\n",
        "\n",
        "plt.text(1, 0.1, \"Recall comparison between macro and micro metrics\", fontsize=10)\n",
        "\n",
        "# Macro and Micro F1-Score\n",
        "plt.subplot(3, 2, 4)\n",
        "plt.plot(epochs, metrics[\"f1_macro\"], label=\"Macro F1-Score\")\n",
        "plt.plot(epochs, metrics[\"f1_micro\"], label=\"Micro F1-Score\")\n",
        "plt.title(\"Macro and Micro F1-Score\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.legend()\n",
        "\n",
        "plt.text(1, 0.08, \"F1-Score comparison between macro and micro metrics\", fontsize=10)\n",
        "\n",
        "# AUC Macro and Micro\n",
        "plt.subplot(3, 2, 5)\n",
        "plt.plot(epochs, metrics[\"auc_macro\"], label=\"AUC Macro\")\n",
        "plt.plot(epochs, metrics[\"auc_micro\"], label=\"AUC Micro\")\n",
        "plt.title(\"AUC Macro and Micro\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"AUC\")\n",
        "plt.legend()\n",
        "\n",
        "plt.text(1, 0.94, \"AUC comparison between macro and micro metrics\", fontsize=10)\n",
        "\n",
        "# Development and Training Loss\n",
        "plt.subplot(3, 2, 6)\n",
        "plt.plot(epochs, metrics[\"loss_dev\"], label=\"Development Loss\")\n",
        "plt.plot(epochs, metrics[\"loss_tr\"], label=\"Training Loss\")\n",
        "plt.title(\"Development and Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.text(1, 0.002, \"Loss comparison between training and development phases\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.9)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_zvEMB8jty2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Example metric data extracted from the log (assuming similar data structure):\n",
        "metrics = {\n",
        "    \"micro_f1_train\": [\n",
        "        0.70305, 0.67288, 0.68061, 0.67856, 0.68409, 0.69788, 0.70852, 0.715, 0.72367, 0.72864, 0.73457, 0.74247, 0.74752, 0.75229, 0.75856\n",
        "    ],\n",
        "    \"micro_f1_valid\": [\n",
        "        0.73892, 0.72267, 0.72916, 0.69207, 0.69772, 0.70322, 0.69513, 0.71076, 0.69686, 0.70297, 0.70931, 0.70582, 0.69845, 0.69836, 0.70597\n",
        "    ],\n",
        "    \"loss_train\": [\n",
        "        27.44121, 40.40351, 42.8864, 46.18624, 46.36197, 44.51928, 43.10817, 42.25577, 41.02574, 40.36062, 39.60688, 38.57417, 37.75087, 37.06155, 36.27539\n",
        "    ],\n",
        "    \"loss_valid\": [\n",
        "        26.84183, 39.00168, 40.45454, 49.73472, 48.76607, 47.69261, 46.20484, 47.49478, 47.49478, 47.69261, 46.20484, 47.49478, 47.49478, 47.49478, 46.20484\n",
        "    ]\n",
        "}\n",
        "\n",
        "epochs = list(range(1, len(metrics[\"micro_f1_train\"]) + 1))\n",
        "\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs, metrics[\"micro_f1_train\"], label=\"Micro F1 Train\")\n",
        "plt.plot(epochs, metrics[\"micro_f1_valid\"], label=\"Micro F1 Valid\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Micro F1\")\n",
        "plt.legend()\n",
        "\n",
        "plt.text(1, 0.68, \"Comparison between Training and Validation Micro F1\\n(LAAT with HiCuA + ASL)\", fontsize=10)\n",
        "\n",
        "# Loss comparison\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs, metrics[\"loss_train\"], label=\"Loss Train\")\n",
        "plt.plot(epochs, metrics[\"loss_valid\"], label=\"Loss Valid\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.text(1, 28, \"Comparison between Training and Validation Loss\\n(LAAT with HiCuA + ASL)\", fontsize=10)\n",
        "\n",
        "# Adding a suptitle with adequate space\n",
        "plt.suptitle(\"LAAT with HiCuA + ASL Metrics\", fontsize=16, weight=\"bold\")\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YKTqVEVUsPON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "## Discussion and Future Plan\n",
        "\n",
        "### Reproducibility Assessment\n",
        "\n",
        "In this project, I focused on testing the hypotheses laid out for the following models based on the original paper:\n",
        "\n",
        "1. `MultiResCNN with HiCuA`\n",
        "2. `RAC with HiCuA`\n",
        "3. `LAAT with HiCuA + ASL`\n",
        "\n",
        "While these tests do not cover all hypotheses from the original study, they were selected based on the resources available to me. **The results from these tests closely align with the claims made in the original paper, providing strong indications of reproducibility.**\n",
        "\n",
        "### Encountered Challenges\n",
        "#### What Was Easy:\n",
        "- Although the initial setup required some hours to complete, the process will be much more straightforward for future reproducers thanks to the `environment.yml` file that I created.\n",
        "\n",
        "#### What Was Difficult:\n",
        "- The training of models, especially the RAC with HiCuA, proved to be resource-intensive and time-consuming. The need for substantial computational resources posed a considerable challenge.\n",
        "- Managing this project solo alongside other responsibilities was particularly strenuous. The absence of a collaborative team amplified the complexity and intensity of managing all aspects of the model training.\n",
        "\n",
        "### Suggestions for Improvement\n",
        "- **For the Original Authors:**\n",
        "  - **Documentation and Setup:** Providing detailed environment setup files, such as an `environment.yml`, at the outset could significantly enhance reproducibility. This would help future researchers bypass the initial hurdles of environment configuration.\n",
        "  - **Resource Requirements:** More transparent communication regarding the computational demands and hardware specifications could prepare future researchers better, ensuring they have adequate resources before commencing the project.\n",
        "\n",
        "- **For Future Reproducers:**\n",
        "  - **Utilize Existing Resources:** Leverage the `environment.yml` to simplify the initial setup process. This step is crucial for aligning your computational environment with the needs of the project.\n",
        "  - **Resource Allocation:** Ensure access to adequate computing resources. For reference, the original study utilized **4 NVIDIA Tesla V100 GPUs**. Matching or approximating this hardware setup could be critical for replicating the study's results effectively\n",
        "\n",
        "The findings from this study reinforce the reproducibility of the original paper's results under the constraints and configurations tested. The challenges encountered highlight the importance of resource availability and preparatory work in reproducing complex models. Future efforts in this field could benefit significantly from improved documentation and sharing of setup configurations to facilitate easier replication and extend research outcomes."
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Public GitHub Repo\n",
        "\n",
        "- **Direct Link to MultiResCNN and RAC Repo (Evaluation):** https://github.com/SaadatUIUC/HiCu-ICD-UIUC-Evaluation\n",
        "- **Direct Link to LAAT Repo (Evaluation):** https://github.com/SaadatUIUC/HiCu-ICD-UIUC-LAAT-Evaluation\n",
        "- **Direct Link to Original HiCu Repo:** https://github.com/wren93/HiCu-ICD"
      ],
      "metadata": {
        "id": "e7rLUGk7gTbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Weiming Ren, Ruijing Zeng, Tongzi Wu, Tianshu Zhu, Rahul G. Krishnan (2022). HiCu: Leveraging Hierarchy for Curriculum Learning in Automated ICD Coding. Proceedings of the 7th Machine Learning for Healthcare Conference, PMLR 182:198-223 https://arxiv.org/pdf/2208.02301.pdf\n",
        "2. Vu, Thanh, Nguyen, Dat Quoc, & Nguyen, Anthony. (2020). \"A Label Attention Model for ICD Coding from Clinical Text.\" Proceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI): 3335-3341. https://arxiv.org/abs/2007.06351\n",
        "3. Li, F., & Yu, H. (2020). \"ICD Coding from Clinical Text Using Multi-Filter Residual Convolutional Neural Network.\" Proceedings of the AAAI Conference on Artificial Intelligence, 34(5): 81808187. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8315310/\n",
        "4. Kim, J., & Ganapathi, V. (2021). \"Read, Attend, and Code: Pushing the Limits of Medical Codes Prediction from Clinical Notes by Machines.\" Proceedings of Machine Learning for Healthcare Conference (MLHC), PMLR 149: 427448. https://arxiv.org/abs/2107.10650\n",
        "5. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). \"Distributed Representations of Words and Phrases and Their Compositionality.\" Advances in Neural Information Processing Systems, 26: 3111-3119. https://arxiv.org/abs/1310.4546\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}